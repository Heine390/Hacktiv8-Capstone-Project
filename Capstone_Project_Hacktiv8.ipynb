{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community\n",
        "!pip install replicate\n",
        "from langchain_community.llms import Replicate\n",
        "import os\n",
        "from google.colab import userdata\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set the API token\n",
        "api_token = userdata.get('REPLICATE_API_TOKEN')\n",
        "os.environ[\"REPLICATE_API_TOKEN\"] = api_token\n",
        "# Model setup\n",
        "model = \"ibm-granite/granite-3.3-8b-instruct\"\n",
        "output = Replicate(\n",
        "model=model,\n",
        "replicate_api_token=api_token,\n",
        ")"
      ],
      "metadata": {
        "id": "HhRl9vmchEMm",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfc53da9-ab00-408a-96f0-2e0e25788deb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.74)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.14)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain_community) (0.3.9)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain_community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain_community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.3.1)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain_community-0.3.27 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0\n",
            "Collecting replicate\n",
            "  Downloading replicate-1.0.7-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from replicate) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from replicate) (25.0)\n",
            "Requirement already satisfied: pydantic>1.10.7 in /usr/local/lib/python3.11/dist-packages (from replicate) (2.11.7)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from replicate) (4.14.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.21.0->replicate) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.21.0->replicate) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.21.0->replicate) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.21.0->replicate) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.21.0->replicate) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>1.10.7->replicate) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>1.10.7->replicate) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>1.10.7->replicate) (0.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.21.0->replicate) (1.3.1)\n",
            "Downloading replicate-1.0.7-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.6/48.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: replicate\n",
            "Successfully installed replicate-1.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NCBwwvp2Lur"
      },
      "source": [
        "#PREPROCESSING DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6_Gj6ftxH-j"
      },
      "outputs": [],
      "source": [
        "folder_path = \"/content/drive/MyDrive/DATASET/GAMMAFEST/Paper Database/Paper Database\"\n",
        "files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
        "data = []\n",
        "for file_name in files:\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        content = f.read().strip()\n",
        "    data.append({\n",
        "        \"judul\": os.path.splitext(file_name)[0],\n",
        "        \"isi\": content\n",
        "    })\n",
        "df = pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIhZouTw3yPn"
      },
      "outputs": [],
      "source": [
        "df1 = pd.read_csv('/content/drive/MyDrive/DATASET/GAMMAFEST/train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "PEvQhVJl5yfv",
        "outputId": "1d666858-8b6c-4457-c6ea-2d901f5db199"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               paper paper_id  \\\n",
              "0  Findings of the Association for Computational ...    p2128   \n",
              "1  Residual Algorithms:\\nReinforcement Learning w...    p0389   \n",
              "2  SegNet: A Deep Convolutional Encoder-Decoder\\n...    p1298   \n",
              "3  Segmentation-Aware Convolutional Networks Usin...    p0211   \n",
              "4  Vol.:(0123456789)\\nArchives of Computational M...    p0843   \n",
              "\n",
              "                                    referenced_paper referenced_paper_id  \\\n",
              "0  Optimization Methods for Large-Scale Machine L...               p3728   \n",
              "1  “© 2019 IEEE. Personal use of this material is...               p3811   \n",
              "2  Integrative Methods for Analysing Big Data in\\...               p3760   \n",
              "3  he goals of IBM Research are to advance comput...               p1808   \n",
              "4  Machine Learning, 22, 33-57 \\nO 1996 Kluwer Ac...               p2964   \n",
              "\n",
              "   is_referenced  \n",
              "0              0  \n",
              "1              0  \n",
              "2              0  \n",
              "3              0  \n",
              "4              0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-acb623ed-7c0b-4f61-bd4c-f951e7e9af55\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper</th>\n",
              "      <th>paper_id</th>\n",
              "      <th>referenced_paper</th>\n",
              "      <th>referenced_paper_id</th>\n",
              "      <th>is_referenced</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Findings of the Association for Computational ...</td>\n",
              "      <td>p2128</td>\n",
              "      <td>Optimization Methods for Large-Scale Machine L...</td>\n",
              "      <td>p3728</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Residual Algorithms:\\nReinforcement Learning w...</td>\n",
              "      <td>p0389</td>\n",
              "      <td>“© 2019 IEEE. Personal use of this material is...</td>\n",
              "      <td>p3811</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SegNet: A Deep Convolutional Encoder-Decoder\\n...</td>\n",
              "      <td>p1298</td>\n",
              "      <td>Integrative Methods for Analysing Big Data in\\...</td>\n",
              "      <td>p3760</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Segmentation-Aware Convolutional Networks Usin...</td>\n",
              "      <td>p0211</td>\n",
              "      <td>he goals of IBM Research are to advance comput...</td>\n",
              "      <td>p1808</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Vol.:(0123456789)\\nArchives of Computational M...</td>\n",
              "      <td>p0843</td>\n",
              "      <td>Machine Learning, 22, 33-57 \\nO 1996 Kluwer Ac...</td>\n",
              "      <td>p2964</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-acb623ed-7c0b-4f61-bd4c-f951e7e9af55')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-acb623ed-7c0b-4f61-bd4c-f951e7e9af55 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-acb623ed-7c0b-4f61-bd4c-f951e7e9af55');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-d854d49b-07c2-4991-9a46-ab43c8a1fdc3\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d854d49b-07c2-4991-9a46-ab43c8a1fdc3')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-d854d49b-07c2-4991-9a46-ab43c8a1fdc3 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_precln"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "judul_to_isi = dict(zip(df['judul'], df['isi']))\n",
        "df_precln = pd.DataFrame({\n",
        "    'paper': df1['paper'].map(judul_to_isi),\n",
        "    'paper_id': df1['paper'],\n",
        "    'referenced_paper': df1['referenced_paper'].map(judul_to_isi),\n",
        "    'referenced_paper_id': df1['referenced_paper'],\n",
        "    'is_referenced': df1['is_referenced']\n",
        "})\n",
        "df_precln.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcHzip2L2F4z"
      },
      "source": [
        "#MODELING USING IBM GRANITE 3.3 8B INSTRUCT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_ex = (\n",
        "    df_precln\n",
        "    .groupby(\"is_referenced\", group_keys=False)\n",
        "    .apply(lambda x: x.sample(n=1, random_state=42))\n",
        ")\n",
        "df_ex.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "OVaHFmweBdJQ",
        "outputId": "ff1c28dc-d0fe-4afb-d0a9-1126cd809efe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2661511708.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda x: x.sample(n=1, random_state=42))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    paper paper_id  \\\n",
              "339300  Accelerating the Super-Resolution\\nConvolution...    p2246   \n",
              "339528  ViViT: A Video Vision Transformer\\nAnurag Arna...    p4225   \n",
              "\n",
              "                                         referenced_paper referenced_paper_id  \\\n",
              "339300  Web-Scale Training for Face Identiﬁcation\\nYan...               p0808   \n",
              "339528  Published as a conference paper at ICLR 2018\\n...               p0160   \n",
              "\n",
              "        is_referenced  \n",
              "339300              0  \n",
              "339528              1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1c83f424-9da5-4276-9bb0-ac692c0a69b6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper</th>\n",
              "      <th>paper_id</th>\n",
              "      <th>referenced_paper</th>\n",
              "      <th>referenced_paper_id</th>\n",
              "      <th>is_referenced</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>339300</th>\n",
              "      <td>Accelerating the Super-Resolution\\nConvolution...</td>\n",
              "      <td>p2246</td>\n",
              "      <td>Web-Scale Training for Face Identiﬁcation\\nYan...</td>\n",
              "      <td>p0808</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>339528</th>\n",
              "      <td>ViViT: A Video Vision Transformer\\nAnurag Arna...</td>\n",
              "      <td>p4225</td>\n",
              "      <td>Published as a conference paper at ICLR 2018\\n...</td>\n",
              "      <td>p0160</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1c83f424-9da5-4276-9bb0-ac692c0a69b6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1c83f424-9da5-4276-9bb0-ac692c0a69b6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1c83f424-9da5-4276-9bb0-ac692c0a69b6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-a279b745-43d6-4a3c-9b55-cfeec6e499b7\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a279b745-43d6-4a3c-9b55-cfeec6e499b7')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-a279b745-43d6-4a3c-9b55-cfeec6e499b7 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_ex",
              "summary": "{\n  \"name\": \"df_ex\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"paper\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"ViViT: A Video Vision Transformer\\nAnurag Arnab*\\nMostafa Dehghani*\\nGeorg Heigold\\nMario Lu\\u02c7ci\\u00b4c\\u2020\\nCordelia Schmid\\u2020\\nGoogle Research\\n{aarnab, dehghani, heigold, chensun, lucic, cordelias}@google.com\\nWe present pure-transformer based models for video\\nclassi\\ufb01cation, drawing upon the recent success of such models in image classi\\ufb01cation.\\nOur model extracts spatiotemporal tokens from the input video, which are then encoded by a series of transformer layers. In order to handle the long sequences of tokens encountered in video, we\\npropose several, ef\\ufb01cient variants of our model which factorise the spatial- and temporal-dimensions of the input. Although transformer-based models are known to only be effective when large training datasets are available, we show\\nhow we can effectively regularise the model during training\\nand leverage pretrained image models to be able to train on\\ncomparatively small datasets. We conduct thorough ablation studies, and achieve state-of-the-art results on multiple\\nvideo classi\\ufb01cation benchmarks including Kinetics 400 and\\n600, Epic Kitchens, Something-Something v2 and Moments\\nin Time, outperforming prior methods based on deep 3D\\nconvolutional networks. To facilitate further research, we\\nrelease code at \\n1. Introduction\\nApproaches based on deep convolutional neural networks have advanced the state-of-the-art across many standard datasets for vision problems since AlexNet . At\\nthe same time, the most prominent architecture of choice in\\nsequence-to-sequence modelling (e.g. in natural language\\nprocessing) is the transformer , which does not use convolutions, but is based on multi-headed self-attention. This\\noperation is particularly effective at modelling long-range\\ndependencies and allows the model to attend over all elements in the input sequence. This is in stark contrast to\\nconvolutions where the corresponding \\u201creceptive \\ufb01eld\\u201d is\\nlimited, and grows linearly with the depth of the network.\\nThe success of attention-based models in NLP has recently inspired approaches in computer vision to integrate\\ntransformers into CNNs , as well as some attempts to\\nreplace convolutions completely . However, it is\\n*Equal contribution\\n\\u2020Equal advising\\nonly very recently with the Vision Transformer (ViT) ,\\nthat a pure-transformer based architecture has outperformed\\nits convolutional counterparts in image classi\\ufb01cation. Dosovitskiy et al. closely followed the original transformer\\narchitecture of , and noticed that its main bene\\ufb01ts\\nwere observed at large scale \\u2013 as transformers lack some\\nof the inductive biases of convolutions (such as translational equivariance), they seem to require more data \\nor stronger regularisation .\\nInspired by ViT, and the fact that attention-based architectures are an intuitive choice for modelling longrange contextual relationships in video, we develop several transformer-based models for video classi\\ufb01cation. Currently, the most performant models are based on deep 3D\\nconvolutional architectures which were a natural extension of image classi\\ufb01cation CNNs . Recently, these models were augmented by incorporating selfattention into their later layers to better capture long-range\\ndependencies .\\nAs shown in Fig. 1, we propose pure-transformer models for video classi\\ufb01cation. The main operation performed\\nin this architecture is self-attention, and it is computed on\\na sequence of spatio-temporal tokens that we extract from\\nthe input video. To effectively process the large number of\\nspatio-temporal tokens that may be encountered in video,\\nwe present several methods of factorising our model along\\nspatial and temporal dimensions to increase ef\\ufb01ciency and\\nscalability. Furthermore, to train our model effectively on\\nsmaller datasets, we show how to reguliarise our model during training and leverage pretrained image models.\\nWe also note that convolutional models have been developed by the community for several years, and there are\\nthus many \\u201cbest practices\\u201d associated with such models.\\nAs pure-transformer models present different characteristics, we need to determine the best design choices for such\\narchitectures. We conduct a thorough ablation analysis of\\ntokenisation strategies, model architecture and regularisation methods. Informed by this analysis, we achieve stateof-the-art results on multiple standard video classi\\ufb01cation\\nbenchmarks, including Kinetics 400 and 600 , Epic\\nKitchens 100 , Something-Something v2 and Moments in Time .\\n \\nPosition + Token\\nFactorised\\nSelf-Attention\\nTransformer Encoder\\nLayer Norm\\nLayer Norm\\nMulti-Head\\nDot-Product\\nFactorised\\nSelf-Attention\\nFactorised\\nDot-Product\\nFigure 1: We propose a pure-transformer architecture for video classi\\ufb01cation, inspired by the recent success of such models for images .\\nTo effectively process a large number of spatio-temporal tokens, we develop several model variants which factorise different components\\nof the transformer encoder over the spatial- and temporal-dimensions. As shown on the right, these factorisations correspond to different\\nattention patterns over space and time.\\n2. Related Work\\nArchitectures for video understanding have mirrored advances in image recognition.\\nEarly video research used\\nhand-crafted features to encode appearance and motion\\ninformation .\\nThe success of AlexNet on ImageNet initially led to the repurposing of 2D image convolutional networks (CNNs) for video as \\u201ctwostream\\u201d networks .\\nThese models processed\\nRGB frames and optical \\ufb02ow images independently before\\nfusing them at the end. Availability of larger video classi-\\n\\ufb01cation datasets such as Kinetics subsequently facilitated the training of spatio-temporal 3D CNNs \\nwhich have signi\\ufb01cantly more parameters and thus require\\nlarger training datasets. As 3D convolutional networks require signi\\ufb01cantly more computation than their image counterparts, many architectures factorise convolutions across\\nspatial and temporal dimensions and/or use grouped convolutions . We also leverage factorisation\\nof the spatial and temporal dimensions of videos to increase\\nef\\ufb01ciency, but in the context of transformer-based models.\\nConcurrently, in natural language processing (NLP),\\nVaswani et al. achieved state-of-the-art results by replacing convolutions and recurrent networks with the transformer network that consisted only of self-attention, layer\\nnormalisation and multilayer perceptron (MLP) operations.\\nCurrent state-of-the-art architectures in NLP remain transformer-based, and have been scaled to web-scale\\ndatasets . Many variants of the transformer have also\\nbeen proposed to reduce the computational cost of selfattention when processing longer sequences and to improve parameter ef\\ufb01ciency .\\nAlthough self-attention has been employed extensively in\\ncomputer vision, it has, in contrast, been typically incorporated as a layer at the end or in the later stages of\\nthe network or to augment residual\\nblocks within a ResNet architecture .\\nAlthough previous works attempted to replace convolutions in vision architectures , it is only very recently that Dosovitisky et al. showed with their ViT architecture that pure-transformer networks, similar to those\\nemployed in NLP, can achieve state-of-the-art results for\\nimage classi\\ufb01cation too.\\nThe authors showed that such\\nmodels are only effective at large scale, as transformers lack\\nsome of inductive biases of convolutional networks (such\\nas translational equivariance), and thus require datasets\\nlarger than the common ImageNet ILSRVC dataset to\\ntrain. ViT has inspired a large amount of follow-up work\\nin the community, and we note that there are a number\\nof concurrent approaches on extending it to other tasks in\\ncomputer vision and improving its dataef\\ufb01ciency . In particular, have also proposed\\ntransformer-based models for video.\\nIn this paper, we develop pure-transformer architectures\\nfor video classi\\ufb01cation. We propose several variants of our\\nmodel, including those that are more ef\\ufb01cient by factorising the spatial and temporal dimensions of the input video.\\nWe also show how additional regularisation and pretrained\\nmodels can be used to combat the fact that video datasets\\nare not as large as their image counterparts that ViT was\\noriginally trained on. Furthermore, we outperform the stateof-the-art across \\ufb01ve popular datasets.\\n3. Video Vision Transformers\\nWe start by summarising the recently proposed Vision\\nTransformer in Sec. 3.1, and then discuss two approaches for extracting tokens from video in Sec. 3.2. Finally, we develop several transformer-based architectures\\nfor video classi\\ufb01cation in Sec. 3.3 and 3.4.\\n3.1. Overview of Vision Transformers (ViT)\\nVision Transformer (ViT) adapts the transformer\\narchitecture of to process 2D images with minimal\\nchanges. In particular, ViT extracts N non-overlapping image patches, xi \\u2208Rh\\u00d7w, performs a linear projection and\\nthen rasterises them into 1D tokens zi \\u2208Rd. The sequence\\nof tokens input to the following transformer encoder is\\nz = [zcls, Ex1, Ex2, . . . , ExN] + p,\\nwhere the projection by E is equivalent to a 2D convolution.\\nAs shown in Fig. 1, an optional learned classi\\ufb01cation token\\nzcls is prepended to this sequence, and its representation at\\nthe \\ufb01nal layer of the encoder serves as the \\ufb01nal representation used by the classi\\ufb01cation layer . In addition, a\\nlearned positional embedding, p \\u2208RN\\u00d7d, is added to the\\ntokens to retain positional information, as the subsequent\\nself-attention operations in the transformer are permutation\\ninvariant. The tokens are then passed through an encoder\\nconsisting of a sequence of L transformer layers. Each layer\\n\\u2113comprises of Multi-Headed Self-Attention , layer normalisation (LN) , and MLP blocks as follows:\\ny\\u2113= MSA(LN(z\\u2113)) + z\\u2113\\nz\\u2113+1 = MLP(LN(y\\u2113)) + y\\u2113.\\nThe MLP consists of two linear projections separated by a\\nGELU non-linearity and the token-dimensionality, d,\\nremains \\ufb01xed throughout all layers. Finally, a linear classi-\\n\\ufb01er is used to classify the encoded input based on zL\\nif it was prepended to the input, or a global average pooling\\nof all the tokens, zL, otherwise.\\nAs the transformer , which forms the basis of\\nViT , is a \\ufb02exible architecture that can operate on any\\nsequence of input tokens z \\u2208RN\\u00d7d, we describe strategies\\nfor tokenising videos next.\\n3.2. Embedding video clips\\nWe consider two simple methods for mapping a video\\nRT \\u00d7H\\u00d7W \\u00d7C to a sequence of tokens \\u02dcz\\nRnt\\u00d7nh\\u00d7nw\\u00d7d. We then add the positional embedding and\\nreshape into RN\\u00d7d to obtain z, the input to the transformer.\\nUniform frame sampling\\nAs illustrated in Fig. 2, a\\nstraightforward method of tokenising the input video is to\\nuniformly sample nt frames from the input video clip, embed each 2D frame independently using the same method\\nas ViT , and concatenate all these tokens together. Concretely, if nh \\u00b7 nw non-overlapping image patches are extracted from each frame, as in , then a total of nt\\u00b7nh\\u00b7nw\\ntokens will be forwarded through the transformer encoder.\\nIntuitively, this process may be seen as simply constructing\\na large 2D image to be tokenised following ViT. We note\\nthat this is the input embedding method employed by the\\nconcurrent work of .\\nFigure 2: Uniform frame sampling: We simply sample nt frames,\\nand embed each 2D frame independently following ViT .\\nFigure 3: Tubelet embedding. We extract and linearly embed nonoverlapping tubelets that span the spatio-temporal input volume.\\nTubelet embedding\\nAn alternate method, as shown in\\nFig. 3, is to extract non-overlapping, spatio-temporal\\n\\u201ctubes\\u201d from the input volume, and to linearly project this to\\nRd. This method is an extension of ViT\\u2019s embedding to 3D,\\nand corresponds to a 3D convolution. For a tubelet of dimension t \\u00d7 h \\u00d7 w, nt = \\u230aT\\nt \\u230b, nh = \\u230aH\\nh \\u230band nw = \\u230aW\\ntokens are extracted from the temporal, height, and width\\ndimensions respectively. Smaller tubelet dimensions thus\\nresult in more tokens which increases the computation.\\nIntuitively, this method fuses spatio-temporal information\\nduring tokenisation, in contrast to \\u201cUniform frame sampling\\u201d where temporal information from different frames is\\nfused by the transformer.\\n3.3. Transformer Models for Video\\nAs illustrated in Fig. 1, we propose multiple transformerbased architectures. We begin with a straightforward extension of ViT that models pairwise interactions between all spatio-temporal tokens, and then develop more\\nef\\ufb01cient variants which factorise the spatial and temporal\\ndimensions of the input video at various levels of the transformer architecture.\\nModel 1: Spatio-temporal attention\\nThis model simply forwards all spatio-temporal tokens extracted from the\\nvideo, z0, through the transformer encoder. We note that\\nthis has also been explored concurrently by in their\\n\\u201cJoint Space-Time\\u201d model. In contrast to CNN architectures, where the receptive \\ufb01eld grows linearly with the\\nnumber of layers, each transformer layer models all pair-\\nPositional + Token\\nTemporal + Token\\nEmbed to tokens\\nTemporal Transformer Encoder\\nSpatial Transformer\\nSpatial Transformer\\nSpatial Transformer\\nFigure 4: Factorised encoder (Model 2). This model consists of\\ntwo transformer encoders in series: the \\ufb01rst models interactions\\nbetween tokens extracted from the same temporal index to produce\\na latent representation per time-index. The second transformer\\nmodels interactions between time steps. It thus corresponds to a\\n\\u201clate fusion\\u201d of spatial- and temporal information.\\nwise interactions between all spatio-temporal tokens, and it\\nthus models long-range interactions across the video from\\nthe \\ufb01rst layer.\\nHowever, as it models all pairwise interactions, Multi-Headed Self Attention (MSA) has\\nquadratic complexity with respect to the number of tokens.\\nThis complexity is pertinent for video, as the number of tokens increases linearly with the number of input frames, and\\nmotivates the development of more ef\\ufb01cient architectures\\nModel 2: Factorised encoder\\nAs shown in Fig. 4, this\\nmodel consists of two separate transformer encoders. The\\n\\ufb01rst, spatial encoder, only models interactions between tokens extracted from the same temporal index. A representation for each temporal index, hi \\u2208Rd, is obtained after Ls\\nlayers: This is the encoded classi\\ufb01cation token, zLs\\ncls if it was\\nprepended to the input (Eq. 1), or a global average pooling\\nfrom the tokens output by the spatial encoder, zLs, otherwise. The frame-level representations, hi, are concatenated\\ninto H \\u2208Rnt\\u00d7d, and then forwarded through a temporal\\nencoder consisting of Lt transformer layers to model interactions between tokens from different temporal indices.\\nThe output token of this encoder is then \\ufb01nally classi\\ufb01ed.\\nThis architecture corresponds to a \\u201clate fusion\\u201d of temporal information, and the initial spatial encoder is identical to the one used for image classi-\\n\\ufb01cation. It is thus analogous to CNN architectures such\\n which \\ufb01rst extract per-frame features, and then aggregate them into a \\ufb01nal representation\\nbefore classifying them.\\nAlthough this model has more\\ntransformer layers than Model 1 (and thus more parameters), it requires fewer \\ufb02oating point operations (FLOPs),\\nas the two separate transformer blocks have a complexity\\nof O((nh \\u00b7 nw)2 + n2\\nt) compared to O((nt \\u00b7 nh \\u00b7 nw)2) of\\nTransformer Block x L\\nLayer Norm\\nLayer Norm\\nMulti-Head\\nLayer Norm\\nMulti-Head\\nTemporal Self-Attention Block\\nSpatial Self-Attention Block\\nToken embedding\\nPositional embedding\\nFigure 5: Factorised self-attention (Model 3). Within each transformer block, the multi-headed self-attention operation is factorised into two operations (indicated by striped boxes) that \\ufb01rst\\nonly compute self-attention spatially, and then temporally.\\nModel 3: Factorised self-attention\\nThis model, in contrast, contains the same number of transformer layers as\\nHowever, instead of computing multi-headed\\nself-attention across all pairs of tokens, z\\u2113, at layer l, we\\nfactorise the operation to \\ufb01rst only compute self-attention\\nspatially (among all tokens extracted from the same temporal index), and then temporally (among all tokens extracted from the same spatial index) as shown in Fig. 5.\\nEach self-attention block in the transformer thus models\\nspatio-temporal interactions, but does so more ef\\ufb01ciently\\nthan Model 1 by factorising the operation over two smaller\\nsets of elements, thus achieving the same computational\\ncomplexity as Model 2. We note that factorising attention\\nover input dimensions has also been explored in ,\\nand concurrently in the context of video by in their \\u201cDivided Space-Time\\u201d model.\\nThis operation can be performed ef\\ufb01ciently by reshaping\\nthe tokens z from R1\\u00d7nt\\u00b7nh\\u00b7nw\\u00b7d to Rnt\\u00d7nh\\u00b7nw\\u00b7d (denoted\\nby zs) to compute spatial self-attention. Similarly, the input\\nto temporal self-attention, zt is reshaped to Rnh\\u00b7nw\\u00d7nt\\u00b7d.\\nHere we assume the leading dimension is the \\u201cbatch dimension\\u201d. Our factorised self-attention is de\\ufb01ned as\\ns = MSA(LN(z\\u2113\\nt = MSA(LN(y\\u2113\\nz\\u2113+1 = MLP(LN(y\\u2113\\nWe observed that the order of spatial-then-temporal selfattention or temporal-then-spatial self-attention does not\\nmake a difference, provided that the model parameters are\\ninitialised as described in Sec. 3.4. Note that the number\\nof parameters, however, increases compared to Model 1, as\\nthere is an additional self-attention layer (cf. Eq. 7). We do\\nnot use a classi\\ufb01cation token in this model, to avoid ambiguities when reshaping the input tokens between spatial and\\ntemporal dimensions.\\nModel 4: Factorised dot-product attention\\nFinally, we\\ndevelop a model which has the same computational complexity as Models 2 and 3, while retaining the same number\\nof parameters as the unfactorised Model 1. The factorisation of spatial- and temporal dimensions is similar in spirit\\nSelf-Attention Block\\nLayer Norm\\nMulti-Head\\nDot-product Attention\\nConcatenate\\nScaled Dot-Product Attention\\nSpatial Heads\\nScaled Dot-Product Attention\\nTemporal Heads\\nFigure 6: Factorised dot-product attention (Model 4). For half of\\nthe heads, we compute dot-product attention over only the spatial\\naxes, and for the other half, over only the temporal axis.\\nto Model 3, but we factorise the multi-head dot-product attention operation instead (Fig. 6). Concretely, we compute\\nattention weights for each token separately over the spatialand temporal-dimensions using different heads. First, we\\nnote that the attention operation for each head is de\\ufb01ned as\\nAttention(Q, K, V) = Softmax\\nIn self-attention, the queries Q = XWq, keys K = XWk,\\nand values V = XWv are linear projections of the input X\\nwith X, Q, K, V \\u2208RN\\u00d7d. Note that in the unfactorised\\ncase (Model 1), the spatial and temporal dimensions are\\nmerged as N = nt \\u00b7 nh \\u00b7 nw.\\nThe main idea here is to modify the keys and values for\\neach query to only attend over tokens from the same spatialand temporal index by constructing Ks, Vs \\u2208Rnh\\u00b7nw\\u00d7d\\nand Kt, Vt \\u2208Rnt\\u00d7d, namely the keys and values corresponding to these dimensions. Then, for half of the attention heads, we attend over tokens from the spatial dimension by computing Ys = Attention(Q, Ks, Vs), and for\\nthe rest we attend over the temporal dimension by computing Yt = Attention(Q, Kt, Vt). Given that we are only\\nchanging the attention neighbourhood for each query, the\\nattention operation has the same dimension as in the unfactorised case, namely Ys, Yt \\u2208RN\\u00d7d. We then combine\\nthe outputs of multiple heads by concatenating them and\\nusing a linear projection , Y = Concat(Ys, Yt)WO.\\n3.4. Initialisation by leveraging pretrained models\\nViT has been shown to only be effective when\\ntrained on large-scale datasets, as transformers lack some of\\nthe inductive biases of convolutional networks . However, even the largest video datasets such as Kinetics ,\\nhave several orders of magnitude less labelled examples\\nwhen compared to their image counterparts . As\\na result, training large models from scratch to high accuracy\\nis extremely challenging. To sidestep this issue, and enable\\nmore ef\\ufb01cient training we initialise our video models from\\npretrained image models. However, this raises several practical questions, speci\\ufb01cally on how to initialise parameters\\nnot present or incompatible with image models. We now\\ndiscuss several effective strategies to initialise these largescale video classi\\ufb01cation models.\\nPositional embeddings\\nA positional embedding p is\\nadded to each input token (Eq. 1).\\nHowever, our video\\nmodels have nt times more tokens than the pretrained image model. As a result, we initialise the positional embeddings by \\u201crepeating\\u201d them temporally from Rnw\\u00b7nh\\u00d7d to\\nRnt\\u00b7nh\\u00b7nw\\u00d7d. Therefore, at initialisation, all tokens with\\nthe same spatial index have the same embedding which is\\nthen \\ufb01ne-tuned.\\nEmbedding weights, E\\nWhen using the \\u201ctubelet embedding\\u201d tokenisation method (Sec. 3.2), the embedding \\ufb01lter\\nE is a 3D tensor, compared to the 2D tensor in the pretrained model, Eimage. A common approach for initialising\\n3D convolutional \\ufb01lters from 2D \\ufb01lters for video classi\\ufb01cation is to \\u201cin\\ufb02ate\\u201d them by replicating the \\ufb01lters along the\\ntemporal dimension and averaging them as\\nt [Eimage, . . . , Eimage, . . . , Eimage].\\nWe consider an additional strategy, which we denote as\\n\\u201ccentral frame initialisation\\u201d, where E is initialised with zeroes along all temporal positions, except at the centre \\u230at\\nE = [0, . . . , Eimage, . . . , 0].\\nTherefore, the 3D convolutional \\ufb01lter effectively behaves\\nlike \\u201cUniform frame sampling\\u201d (Sec. 3.2) at initialisation,\\nwhile also enabling the model to learn to aggregate temporal\\ninformation from multiple frames as training progresses.\\nTransformer weights for Model 3\\nThe transformer\\nblock in Model 3 (Fig. 5) differs from the pretrained ViT\\nmodel , in that it contains two multi-headed self attention (MSA) modules. In this case, we initialise the spatial\\nMSA module from the pretrained module, and initialise all\\nweights of the temporal MSA with zeroes, such that Eq. 5\\nbehaves as a residual connection at initialisation.\\n4. Empirical evaluation\\nWe \\ufb01rst present our experimental setup and implementation details in Sec. 4.1, before ablating various components\\nof our model in Sec. 4.2. We then present state-of-the-art\\nresults on \\ufb01ve datasets in Sec. 4.3.\\n4.1. Experimental Setup\\nNetwork architecture and training\\nOur backbone architecture follows that of ViT and BERT . We consider ViT-Base (ViT-B, L=12, NH=12, d=768), ViT-Large\\n(ViT-L, L=24, NH=16, d=1024), and ViT-Huge (ViT-H,\\nL=32, NH=16, d=1280), where L is the number of transformer layers, each with a self-attention block of NH heads\\nTable 1: Comparison of input encoding methods using ViViT-B\\nand spatio-temporal attention on Kinetics. Further details in text.\\nTop-1 accuracy\\nUniform frame sampling\\nTubelet embedding\\nRandom initialisation \\nFilter in\\ufb02ation \\nCentral frame\\nand hidden dimension d. We also apply the same naming\\nscheme to our models (e.g., ViViT-B/16x2 denotes a ViT-\\nBase backbone with a tubelet size of h\\u00d7w\\u00d7t = 16\\u00d716\\u00d72).\\nIn all experiments, the tubelet height and width are equal.\\nNote that smaller tubelet sizes correspond to more tokens at\\nthe input, and thus more computation.\\nWe train our models using synchronous SGD and momentum, a cosine learning rate schedule and TPU-v3 accelerators.\\nWe initialise our models from a ViT image\\nmodel trained either on ImageNet-21K (unless otherwise speci\\ufb01ed) or the larger JFT dataset. We implement our method using the Scenic library and have released our code and models.\\nWe evaluate the performance of our proposed\\nmodels on a diverse set of video classi\\ufb01cation datasets:\\nKinetics consists of 10-second videos sampled at\\n25fps from YouTube. We evaluate on both Kinetics 400\\nand 600, containing 400 and 600 classes respectively. As\\nthese are dynamic datasets (videos may be removed from\\nYouTube), we note our dataset sizes are approximately 267\\n000 and 446 000 respectively.\\nEpic Kitchens-100 consists of egocentric videos capturing daily kitchen activities spanning 100 hours and 90 000\\nclips . We report results following the standard \\u201caction\\nrecognition\\u201d protocol. Here, each video is labelled with a\\n\\u201cverb\\u201d and a \\u201cnoun\\u201d and we therefore predict both categories using a single network with two \\u201cheads\\u201d. The topscoring verb and action pair predicted by the network form\\nan \\u201caction\\u201d, and action accuracy is the primary metric.\\nMoments in Time consists of 800 000, 3-second\\nYouTube clips that capture the gist of a dynamic scene involving animals, objects, people, or natural phenomena.\\nSomething-Something v2 (SSv2) contains 220 000\\nvideos, with durations ranging from 2 to 6 seconds. In contrast to the other datasets, the objects and backgrounds in\\nthe videos are consistent across different action classes, and\\nthis dataset thus places more emphasis on a model\\u2019s ability\\nto recognise \\ufb01ne-grained motion cues.\\nThe input to our network is a video clip of 32\\nframes using a stride of 2, unless otherwise mentioned, similar to . Following common practice, at inference\\ntime, we process multiple views of a longer video and aver-\\nTable 2: Comparison of model architectures using ViViT-B as the\\nbackbone, and tubelet size of 16\\u00d72. We report Top-1 accuracy on\\nKinetics 400 (K400) and action accuracy on Epic Kitchens (EK).\\nRuntime is during inference on a TPU-v3.\\nModel 1: Spatio-temporal\\nModel 2: Fact. encoder\\nModel 3: Fact. self-attention\\nModel 4: Fact. dot product\\nModel 2: Ave. pool baseline\\nTable 3: The effect of varying the number of temporal transformers, Lt, in the Factorised encoder model (Model 2). We report the\\nTop-1 accuracy on Kinetics 400. Note that Lt = 0 corresponds to\\nthe \\u201caverage pooling baseline\\u201d.\\nage per-view logits to obtain the \\ufb01nal result. Unless otherwise speci\\ufb01ed, we use a total of 4 views per video (as this\\nis suf\\ufb01cient to \\u201csee\\u201d the entire video clip across the various\\ndatasets), and ablate these and other design choices next.\\n4.2. Ablation study\\nInput encoding\\nWe \\ufb01rst consider the effect of different\\ninput encoding methods (Sec. 3.2) using our unfactorised\\nmodel (Model 1) and ViViT-B on Kinetics 400. As we pass\\n32-frame inputs to the network, sampling 8 frames and extracting tubelets of length t = 4 correspond to the same\\nnumber of tokens in both cases. Table 1 shows that tubelet\\nembedding initialised using the \\u201ccentral frame\\u201d method\\n(Eq. 9) performs well, outperforming the commonly-used\\n\\u201c\\ufb01lter in\\ufb02ation\\u201d initialisation method by 1.6%, and\\n\\u201cuniform frame sampling\\u201d by 0.7%. We therefore use this\\nencoding method for all subsequent experiments.\\nModel variants\\nWe compare our proposed model variants (Sec. 3.3) across the Kinetics 400 and Epic Kitchens\\ndatasets, both in terms of accuracy and ef\\ufb01ciency, in Tab. 2.\\nIn all cases, we use the \\u201cBase\\u201d backbone and tubelet size of\\n16 \\u00d7 2. Model 2 (\\u201cFactorised Encoder\\u201d) has an additional\\nhyperparameter, the number of temporal transformers, Lt.\\nWe set Lt = 4 for all experiments and show in Tab. 3 that\\nthe model is not sensitive to this choice.\\nThe unfactorised model (Model 1) performs the best\\non Kinetics 400. However, it can also over\\ufb01t on smaller\\ndatasets such as Epic Kitchens, where we \\ufb01nd our \\u201cFactorised Encoder\\u201d (Model 2) to perform the best. We also\\nconsider an additional baseline (last row), based on Model\\n2, where we do not use any temporal transformer, and simply average pool the frame-level representations from the\\nspatial encoder before classifying.\\nThis average pooling\\nbaseline performs the worst, and has a larger accuracy drop\\nTable 4: The effect of progressively adding regularisation (each\\nrow includes all methods above it) on Top-1 action accuracy on\\nEpic Kitchens. We use a Factorised encoder model with tubelet\\nsize 16 \\u00d7 2.\\nTop-1 accuracy\\nRandom crop, \\ufb02ip, colour jitter\\n+ Kinetics 400 initialisation\\n+ Stochastic depth \\n+ Random augment \\n+ Label smoothing \\n+ Mixup \\nInput tubelet size\\nTop-1 Accuracy\\nInput tubelet size\\n(a) Accuracy\\n(b) Compute\\nFigure 7: The effect of the backbone architecture on (a) accuracy\\nand (b) computation on Kinetics 400, for the spatio-temporal attention model (Model 1).\\nInput tubelet size\\nTop-1 Accuracy\\nInput tubelet size\\nSpatio-temporal\\nFactorised encoder\\nFactorised self-attention\\nFactorised dot-product\\n(a) Accuracy\\n(b) Compute\\nFigure 8: The effect of varying the number of temporal tokens on\\n(a) accuracy and (b) computation on Kinetics 400, for different\\nvariants of our model with a ViViT-B backbone.\\non Epic Kitchens, suggesting that this dataset requires more\\ndetailed modelling of temporal relations.\\nAs described in Sec. 3.3, all factorised variants of our\\nmodel use signi\\ufb01cantly fewer FLOPs than the unfactorised\\nModel 1, as the attention is computed separately over\\nspatial- and temporal-dimensions. Model 4 adds no additional parameters to the unfactorised Model 1, and uses the\\nleast compute. The temporal transformer encoder in Model\\n2 operates on only nt tokens, which is why there is a barely\\na change in compute and runtime over the average pooling baseline, even though it improves the accuracy substantially (3% on Kinetics and 4.9% on Epic Kitchens). Finally, Model 3 requires more compute and parameters than\\nthe other factorised models, as its additional self-attention\\nblock means that it performs another query-, key-, valueand output-projection in each transformer layer .\\nregularisation\\nPure-transformer\\narchitectures\\nsuch as ViT are known to require large training\\ndatasets, and we observed over\\ufb01tting on smaller datasets\\nlike Epic Kitchens and SSv2, even when using an ImageNet\\npretrained model. In order to effectively train our models\\nTable 5: The effect of spatial resolution on the performance of\\nViViT-L/16x2 and spatio-temporal attention on Kinetics 400.\\non such datasets, we employed several regularisation strategies that we ablate using our \\u201cFactorised encoder\\u201d model\\nin Tab. 4. We note that these regularisers were originally\\nproposed for training CNNs, and that have recently\\nexplored them for training ViT for image classi\\ufb01cation.\\nEach row of Tab. 4 includes all the methods from the\\nrows above it, and we observe progressive improvements\\nfrom adding each regulariser. Overall, we obtain a substantial overall improvement of 5.3% on Epic Kitchens. We\\nalso achieve a similar improvement of 5% on SSv2 by using all the regularisation in Tab. 4. Note that the Kineticspretrained models that we initialise from are from Tab. 2,\\nand that all Epic Kitchens models in Tab. 2 were trained\\nwith all the regularisers in Tab. 4. For larger datasets like\\nKinetics and Moments in Time, we do not use these additional regularisers (we use only the \\ufb01rst row of Tab. 4),\\nas we obtain state-of-the-art results without them. The appendix contains hyperparameter values and additional details for all regularisers.\\nVarying the backbone\\nFigure 7 compares the ViViT-\\nB and ViViT-L backbones for the unfactorised spatiotemporal model. We observe consistent improvements in\\naccuracy as the backbone capacity increases. As expected,\\nthe compute also grows as a function of the backbone size.\\nVarying the number of tokens\\nWe \\ufb01rst analyse the performance as a function of the number of tokens along the\\ntemporal dimension in Fig. 8. We observe that using smaller\\ninput tubelet sizes (and therefore more tokens) leads to consistent accuracy improvements across all of our model architectures.\\nAt the same time, computation in terms of\\nFLOPs increases accordingly, and the unfactorised model\\n(Model 1) is impacted the most.\\nWe then vary the number of tokens fed into the model\\nby increasing the spatial crop-size from the default of 224\\nto 320 in Tab. 5. As expected, there is a consistent increase\\nin both accuracy and computation. We note that when comparing to prior work we consistently obtain state-of-the-art\\nresults (Sec. 4.3) using a spatial resolution of 224, but we\\nalso highlight that further improvements can be obtained at\\nhigher spatial resolutions.\\nVarying the number of input frames\\nIn our experiments\\nso far, we have kept the number of input frames \\ufb01xed at 32.\\nWe now increase the number of frames input to the model,\\nthereby increasing the number of tokens proportionally.\\nTable 6: Comparisons to state-of-the-art across multiple datasets. For \\u201cviews\\u201d, x \\u00d7 y denotes x temporal crops and y spatial crops. We\\nreport the TFLOPs to process all spatio-temporal views. \\u201cFE\\u201d denotes our Factorised Encoder model.\\n(a) Kinetics 400\\nblVNet \\nTSM-ResNeXt-101 \\nI3D NL \\nCorrNet-101 \\nip-CSN-152 \\nLGD-3D R101 \\nSlowFast R101-NL \\nX3D-XXL \\nTimeSformer-L \\nViViT-L/16x2 FE\\nViViT-L/16x2 FE\\nMethods with large-scale pretraining\\nip-CSN-152 (IG )\\nViViT-L/16x2 FE (JFT)\\nViViT-H/14x2 (JFT)\\n(b) Kinetics 600\\nAttentionNAS \\nLGD-3D R101 \\nSlowFast R101-NL \\nX3D-XL \\nTimeSformer-L \\nViViT-L/16x2 FE\\nViViT-L/16x2 FE (JFT)\\nViViT-H/14x2 (JFT)\\n(c) Moments in Time\\nblVNet \\nAssembleNet-101 \\nViViT-L/16x2 FE\\n(d) Epic Kitchens 100 Top 1 accuracy\\nSlowFast \\nViViT-L/16x2 FE\\n(e) Something-Something v2\\nSlowFast \\nTimeSformer-HR \\nblVNet \\nViVIT-L/16x2 FE\\nNumber of views\\nTop-1 Accuracy\\n32 stride 2\\n64 stride 2\\n128 stride 2\\nFigure 9: The effect of varying the number of frames input to the\\nnetwork and increasing the number of tokens proportionally. We\\nuse ViViT-L/16x2 Factorised Encoder on Kinetics 400. A Kinetics\\nvideo contains 250 frames (10 seconds sampled at 25 fps) and the\\naccuracy for each model saturates once the number of equidistant\\ntemporal views is suf\\ufb01cient to \\u201csee\\u201d the whole video clip. Observe how models processing more frames (and thus more tokens)\\nachieve higher single- and multi-view accuracy.\\nFigure 9 shows that as we increase the number of frames\\ninput to the network, the accuracy from processing a single view increases, since the network incorporates longer\\ntemporal context. However, common practice on datasets\\nsuch as Kinetics is to average results over multiple, shorter \\u201cviews\\u201d of the same video clip. Figure 9 also\\nshows that the accuracy saturates once the number of views\\nis suf\\ufb01cient to cover the whole video. As a Kinetics video\\nconsists of 250 frames, and we sample frames with a stride\\nof 2, our model which processes 128 frames requires just a\\nsingle view to \\u201csee\\u201d the whole video and achieve its maximum accuarcy.\\nNote that we used ViViT-L/16x2 Factorised Encoder\\n(Model 2) here. As this model is more ef\\ufb01cient it can process more tokens, compared to the unfactorised Model 1\\nwhich runs out of memory after 48 frames using tubelet\\nlength t = 2 and a \\u201cLarge\\u201d backbone. Models processing\\nmore frames (and thus more tokens) consistently achieve\\nhigher single- and multi-view accuracy, in line with our observations in previous experiments (Tab. 5, Fig. 8). Moroever, observe that by processing more frames (and thus\\nmore tokens) with Model 2, we are able to achieve higher\\naccuracy than Model 1 (with fewer total FLOPs as well).\\nFinally, we observed that for Model 2, the number of\\nFLOPs effectively increases linearly with the number of input frames as the overall computation is dominated by the\\ninitial Spatial Transformer. As a result, the total number\\nof FLOPs for the number of temporal views required to\\nachieve maximum accuracy is constant across the models.\\nIn other words, ViViT-L/16x2 FE with 32 frames requires\\n995.3 GFLOPs per view, and 4 views to saturate multi-view\\naccuracy. The 128-frame model requires 3980.4 GFLOPs\\nbut only a single view. As shown by Fig. 9, the latter model\\nachieves the highest accuracy.\\n4.3. Comparison to state-of-the-art\\nBased on our ablation studies in the previous section,\\nwe compare to the current state-of-the-art using two of our\\nmodel variants. We primarily use our Factorised Encoder\\nmodel (Model 2), as it can process more tokens than Model\\n1 to achieve higher accuracy.\\nTables 6a and 6b show that our spatio-temporal\\nattention models outperform the state-of-the-art on Kinetics\\n400 and 600 respectively. Following standard practice, we\\ntake 3 spatial crops (left, centre and right) \\nfor each temporal view, and notably, we require signi\\ufb01cantly fewer views than previous CNN-based methods.\\nWe surpass the previous CNN-based state-of-the-art using ViViT-L/16x2 Factorised Encoder (FE) pretrained on\\nImageNet, and also outperform who concurrently proposed a pure-transformer architecture. Moreover, by initialising our backbones from models pretrained on the larger\\nJFT dataset , we obtain further improvements.\\nAlthough these models are not directly comparable to previous work, we do also outperform who pretrained on\\nthe large-scale, Instagram dataset . Our best model uses\\na ViViT-H backbone pretrained on JFT and signi\\ufb01cantly advances the best reported results on Kinetics 400 and 600 to\\n84.9% and 85.8%, respectively.\\nMoments in Time\\nWe surpass the state-of-the-art by a\\nsigni\\ufb01cant margin as shown in Tab. 6c. We note that the\\nvideos in this dataset are diverse and contain signi\\ufb01cant label noise, making this task challenging and leading to lower\\naccuracies than on other datasets.\\nEpic Kitchens 100\\nTable 6d shows that our Factorised\\nEncoder model outperforms previous methods by a signi\\ufb01cant margin. In addition, our model obtains substantial improvements for Top-1 accuracy of \\u201cnoun\\u201d classes, and the\\nonly method which achieves higher \\u201cverb\\u201d accuracy used\\noptical \\ufb02ow as an additional input modality . Furthermore, all variants of our model presented in Tab. 2 outperformed the existing state-of-the-art on action accuracy.\\nWe note that we use the same model to predict verbs and\\nnouns using two separate \\u201cheads\\u201d, and for simplicity, we do\\nnot use separate loss weights for each head.\\nSomething-Something v2 (SSv2)\\nFinally, Tab. 6e shows\\nthat we achieve state-of-the-art Top-1 accuracy with our\\nFactorised encoder model (Model 2), albeit with a smaller\\nmargin compared to previous methods. Notably, our Factorised encoder model signi\\ufb01cantly outperforms the concurrent TimeSformer method by 2.9%, which also proposes\\na pure-transformer model, but does not consider our Factorised encoder variant or our additional regularisation.\\nSSv2 differs from other datasets in that the backgrounds\\nand objects are quite similar across different classes, meaning that recognising \\ufb01ne-grained motion patterns is necessary to distinguish classes from each other. Our results suggest that capturing these \\ufb01ne-grained motions is an area of\\nimprovement and future work for our model. We also note\\nan inverse correlation between the relative performance of\\nprevious methods on SSv2 (Tab. 6e) and Kinetics (Tab. 6a)\\nsuggesting that these two datasets evaluate complementary\\ncharacteristics of a model.\\n5. Conclusion and Future Work\\nWe have presented four pure-transformer models for\\nvideo classi\\ufb01cation, with different accuracy and ef\\ufb01ciency\\npro\\ufb01les, achieving state-of-the-art results across \\ufb01ve popular datasets.\\nFurthermore, we have shown how to effectively regularise such high-capacity models for training\\non smaller datasets and thoroughly ablated our main design choices. Future work is to remove our dependence on\\nimage-pretrained models. Finally, going beyond video classi\\ufb01cation towards more complex tasks is a clear next step.\",\n          \"Accelerating the Super-Resolution\\nConvolutional Neural Network\\nChao Dong, Chen Change Loy, and Xiaoou Tang\\nDepartment of Information Engineering, The Chinese University of Hong Kong\\n{dc012,ccloy,xtang}@ie.cuhk.edu.hk\\nAbstract. As a successful deep model applied in image super-resolution (SR),\\nthe Super-Resolution Convolutional Neural Network (SRCNN) has demonstrated superior performance to the previous hand-crafted models either in speed\\nand restoration quality. However, the high computational cost still hinders it from\\npractical usage that demands real-time performance (24 fps). In this paper, we aim\\nat accelerating the current SRCNN, and propose a compact hourglass-shape CNN\\nstructure for faster and better SR. We re-design the SRCNN structure mainly in\\nthree aspects. First, we introduce a deconvolution layer at the end of the network, then the mapping is learned directly from the original low-resolution image (without interpolation) to the high-resolution one. Second, we reformulate\\nthe mapping layer by shrinking the input feature dimension before mapping and\\nexpanding back afterwards. Third, we adopt smaller \\ufb01lter sizes but more mapping layers. The proposed model achieves a speed up of more than 40 times with\\neven superior restoration quality. Further, we present the parameter settings that\\ncan achieve real-time performance on a generic CPU while still maintaining good\\nperformance. A corresponding transfer strategy is also proposed for fast training\\nand testing across different upscaling factors.\\nIntroduction\\nSingle image super-resolution (SR) aims at recovering a high-resolution (HR) image\\nfrom a given low-resolution (LR) one. Recent SR algorithms are mostly learning-based\\n(or patch-based) methods that learn a mapping between the LR and\\nHR image spaces. Among them, the Super-Resolution Convolutional Neural Network\\n(SRCNN) has drawn considerable attention due to its simple network structure\\nand excellent restoration quality. Though SRCNN is already faster than most previous\\nlearning-based methods, the processing speed on large images is still unsatisfactory. For\\nexample, to upsample an 240\\u00d7240 image by a factor of 3, the speed of the original SR-\\nCNN is about 1.32 fps, which is far from real-time (24 fps). To approach real-time,\\nwe should accelerate SRCNN for at least 17 times while keeping the previous performance. This sounds implausible at the \\ufb01rst glance, as accelerating by simply reducing\\nthe parameters will severely impact the performance. However, when we delve into the\\nnetwork structure, we \\ufb01nd two inherent limitations that restrict its running speed.\\nFirst, as a pre-processing step, the original LR image needs to be upsampled to the\\ndesired size using bicubic interpolation to form the input. Thus the computation complexity of SRCNN grows quadratically with the spatial size of the HR image (not the\\n \\nChao Dong et al.\\nRunning time (sec)\\n\\u2014\\u2014> Slower\\nFaster <\\u2014\\u2014\\nPSNR: > SCN\\nSpeed: 16.4 fps\\nPSNR: > SRCNN\\nSpeed: 43.5 fps\\nFig. 1. The proposed FSRCNN networks achieve better super-resolution quality than existing\\nmethods, and are tens of times faster. Especially, the FSRCNN-s can run in real-time (> 24 fps)\\non a generic CPU. The chart is based on the Set14 results summarized in Tables 3 and 4.\\noriginal LR image). For the upscaling factor n, the computational cost of convolution\\nwith the interpolated LR image will be n2 times of that for the original LR one. This is\\nalso the restriction for most learning-based SR methods . If the network\\nwas learned directly from the original LR image, the acceleration would be signi\\ufb01cant,\\ni.e., about n2 times faster.\\nThe second restriction lies on the costly non-linear mapping step. In SRCNN, input\\nimage patches are projected on a high-dimensional LR feature space, then followed by\\na complex mapping to another high-dimensional HR feature space. Dong et al. show\\nthat the mapping accuracy can be substantially improved by adopting a wider mapping\\nlayer, but at the cost of the running time. For example, the large SRCNN (SRCNN-\\nEx) has 57,184 parameters, which are six times larger than that for SRCNN (8,032\\nparameters). Then the question is how to shrink the network scale while still keeping\\nthe previous accuracy.\\nAccording to the above observations, we investigate a more concise and ef\\ufb01cient\\nnetwork structure for fast and accurate image SR. To solve the \\ufb01rst problem, we adopt\\na deconvolution layer to replace the bicubic interpolation. To further ease the computational burden, we place the deconvolution layer1 at the end of the network, then the\\ncomputational complexity is only proportional to the spatial size of the original LR image. It is worth noting that the deconvolution layer is not equal to a simple substitute\\nof the conventional interpolation kernel like in FCN , or \\u2018unpooling+convolution\\u2019\\nlike . Instead, it consists of diverse automatically learned upsampling kernels (see\\nFigure 3) that work jointly to generate the \\ufb01nal HR output, and replacing these deconvolution \\ufb01lters with uniform interpolation kernels will result in a drastic PSNR drop\\n(e.g., at least 0.9 dB on the Set5 dataset for \\u00d73).\\nFor the second problem, we add a shrinking and an expanding layer at the beginning\\nand the end of the mapping layer separately to restrict mapping in a low-dimensional\\nfeature space. Furthermore, we decompose a single wide mapping layer into several\\nlayers with a \\ufb01xed \\ufb01lter size 3 \\u00d7 3. The overall shape of the new structure looks like an\\n1 We follow to adopt the terminology \\u2018deconvolution\\u2019. We note that it carries very different\\nmeaning in classic image processing, see .\\nAccelerating the Super-Resolution Convolutional Neural Network\\nhourglass, which is symmetrical on the whole, thick at the ends and thin in the middle.\\nExperiments show that the proposed model, named as Fast Super-Resolution Convolutional Neural Networks (FSRCNN) 2, achieves a speed-up of more than 40\\u00d7 with even\\nsuperior performance than the SRCNN-Ex. In this work, we also present a small FS-\\nRCNN network (FSRCNN-s) that achieves similar restoration quality as SRCNN, but\\nis 17.36 times faster and can run in real time (24 fps) with a generic CPU. As shown\\nin Figure 1, the FSRCNN networks are much faster than contemporary SR models yet\\nachieving superior performance.\\nApart from the notable improvement in speed, the FSRCNN also has another appealing property that could facilitate fast training and testing across different upscaling factors. Speci\\ufb01cally, in FSRCNN, all convolution layers (except the deconvolution\\nlayer) can be shared by networks of different upscaling factors. During training, with\\na well-trained network, we only need to \\ufb01ne-tune the deconvolution layer for another\\nupscaling factor with almost no loss of mapping accuracy. During testing, we only need\\nto do convolution operations once, and upsample an image to different scales using the\\ncorresponding deconvolution layer.\\nOur contributions are three-fold: 1) We formulate a compact hourglass-shape CNN\\nstructure for fast image super-resolution. With the collaboration of a set of deconvolution \\ufb01lters, the network can learn an end-to-end mapping between the original LR and\\nHR images with no pre-processing. 2) The proposed model achieves a speed up of at\\nleast 40\\u00d7 than the SRCNN-Ex while still keeping its exceptional performance. One\\nof its small-size version can run in real-time (>24 fps) on a generic CPU with better\\nrestoration quality than SRCNN . 3) We transfer the convolution layers of the proposed networks for fast training and testing across different upscaling factors, with no\\nloss of restoration quality.\\nRelated Work\\nDeep learning for SR: Recently, the deep learning techniques have been successfully applied on SR. The pioneer work is termed as the Super-Resolution Convolutional Neural Network (SRCNN) proposed by Dong et al. . Motivated by SRCNN,\\nsome problems such as face hallucination and depth map super-resolution \\nhave achieved state-of-the-art results. Deeper structures have also been explored in \\nand . Different from the conventional learning-based methods, SRCNN directly\\nlearns an end-to-end mapping between LR and HR images, leading to a fast and accurate inference. The inherent relationship between SRCNN and the sparse-codingbased methods ensures its good performance. Based on the same assumption, Wang et\\nal. further replace the mapping layer by a set of sparse coding sub-networks and\\npropose a sparse coding based network (SCN). With the domain expertise of the conventional sparse-coding-based method, it outperforms SRCNN with a smaller model\\nsize. However, as it strictly mimics the sparse-coding solver, it is very hard to shrink\\nthe sparse coding sub-network with no loss of mapping accuracy. Furthermore, all these\\n2 The implementation is available on the project page \\nprojects/FSRCNN.html.\\nChao Dong et al.\\nnetworks need to process the bicubic-upscaled LR images. The proposed FS-\\nRCNN does not only perform on the original LR image, but also contains a simpler but\\nmore ef\\ufb01cient mapping layer. Furthermore, the previous methods have to train a totally\\ndifferent network for a speci\\ufb01c upscaling factor, while the FSRCNN only requires a\\ndifferent deconvolution layer. This also provides us a faster way to upscale an image to\\nseveral different sizes.\\nCNNs acceleration: A number of studies have investigated the acceleration of CNN.\\nDenton et al. \\ufb01rst investigate the redundancy within the CNNs designed for object detection. Then Zhang et al. make attempts to accelerate very deep CNNs\\nfor image class\\ufb01cation. They also take the non-linear units into account and reduce\\nthe accumulated error by asymmetric reconstruction. Our model also aims at accelerating CNNs but in a different manner. First, they focus on approximating the existing\\nwell-trained models, while we reformulate the previous model and achieves better performance. Second, the above methods are all designed for high-level vision problems\\n(e.g., image classi\\ufb01cation and object detection), while ours are for the low-level vision\\ntask. As the deep models for SR contain no fully-connected layers, the approximation\\nof convolution \\ufb01lters will severely impact the performance.\\nFast Super-Resolution by CNN\\nWe \\ufb01rst brie\\ufb02y describe the network structure of SRCNN , and then we detail\\nhow we reformulate the network layer by layer. The differences between FSRCNN and\\nSRCNN are presented at the end of this section.\\nSRCNN aims at learning an end-to-end mapping function F between the bicubicinterpolated LR image Y and the HR image X. The network contains all convolution\\nlayers, thus the size of the output is the same as that of the input image. As depicted in\\nFigure 2, the overall structure consists of three parts that are analogous to the main steps\\nof the sparse-coding-based methods . The patch extraction and representation part\\nrefers to the \\ufb01rst layer, which extracts patches from the input and represents each patch\\nas a high-dimensional feature vector. The non-linear mapping part refers to the middle\\nlayer, which maps the feature vectors non-linearly to another set of feature vectors, or\\nnamely HR features. Then the last reconstruction part aggregates these features to form\\nthe \\ufb01nal output image.\\nThe computation complexity of the network can be calculated as follows,\\n1 n1 + n1f 2\\n2 n2 + n2f 2\\nwhere {fi}3\\ni=1 and {ni}3\\ni=1 are the \\ufb01lter size and \\ufb01lter number of the three layers, respectively. SHR is the size of the HR image. We observe that the complexity is proportional to the size of the HR image, and the middle layer contributes most to the network\\nparameters. In the next section, we present the FSRCNN by giving special attention to\\nthese two facets.\\nAccelerating the Super-Resolution Convolutional Neural Network\\nPatch extraction and\\nrepresentation\\nNon-linear\\nReconstruction\\nlow-resolution\\nFeature extraction\\nDeconvolution\\ninterpolation\\nNo pre-processing\\nHigh-resolution\\nFig. 2. This \\ufb01gure shows the network structures of the SRCNN and FSRCNN. The proposed\\nFSRCNN is different from SRCNN mainly in three aspects. First, FSRCNN adopts the original\\nlow-resolution image as input without bicubic interpolation. A deconvolution layer is introduced\\nat the end of the network to perform upsampling. Second, The non-linear mapping step in SRCNN\\nis replaced by three steps in FSRCNN, namely the shrinking, mapping, and expanding step. Third,\\nFSRCNN adopts smaller \\ufb01lter sizes and a deeper network structure. These improvements provide\\nFSRCNN with better performance but lower computational cost than SRCNN.\\nAs shown in Figure 2, FSRCNN can be decomposed into \\ufb01ve parts \\u2013 feature extraction,\\nshrinking, mapping, expanding and deconvolution. The \\ufb01rst four parts are convolution\\nlayers, while the last one is a deconvolution layer. For better understanding, we denote a\\nconvolution layer as Conv(fi, ni, ci), and a deconvolution layer as DeConv(fi, ni, ci),\\nwhere the variables fi, ni, ci represent the \\ufb01lter size, the number of \\ufb01lters and the number of channels, respectively.\\nAs the whole network contains tens of variables (i.e., {fi, ni, ci}6\\ni=1), it is impossible for us to investigate each of them. Thus we assign a reasonable value to the insensitive variables in advance, and leave the sensitive variables unset. We call a variable\\nsensitive when a slight change of the variable could signi\\ufb01cantly in\\ufb02uence the performance. These sensitive variables always represent some important in\\ufb02uential factors in\\nSR, which will be shown in the following descriptions.\\nFeature extraction: This part is similar to the \\ufb01rst part of SRCNN, but different on the\\ninput image. FSRCNN performs feature extraction on the original LR image without\\ninterpolation. To distinguish from SRCNN, we denote the small LR input as Ys. By doing convolution with the \\ufb01rst set of \\ufb01lters, each patch of the input (1-pixel overlapping)\\nis represented as a high-dimensional feature vector.\\nWe refer to SRCNN on the choice of parameters \\u2013 f1, n1, c1. In SRCNN, the \\ufb01lter\\nsize of the \\ufb01rst layer is set to be 9. Note that these \\ufb01lters are performed on the upscaled\\nimage Y . As most pixels in Y are interpolated from Ys, a 5 \\u00d7 5 patch in Ys could cover\\nalmost all information of a 9 \\u00d7 9 patch in Y . Therefore, we can adopt a smaller \\ufb01lter\\nsize f1 = 5 with little information loss. For the number of channels, we follow SRCNN\\nto set c1 = 1. Then we only need to determine the \\ufb01lter number n1. From another\\nChao Dong et al.\\nperspective, n1 can be regarded as the number of LR feature dimension, denoted as d \\u2013\\nthe \\ufb01rst sensitive variable. Finally, the \\ufb01rst layer can be represented as Conv(5, d, 1).\\nShrinking: In SRCNN, the mapping step generally follows the feature extraction step,\\nthen the high-dimensional LR features are mapped directly to the HR feature space.\\nHowever, as the LR feature dimension d is usually very large, the computation complexity of the mapping step is pretty high. This phenomenon is also observed in some\\ndeep models for high-level vision tasks. Authors in apply 1 \\u00d7 1 layers to save the\\ncomputational cost.\\nWith the same consideration, we add a shrinking layer after the feature extraction\\nlayer to reduce the LR feature dimension d. We \\ufb01x the \\ufb01lter size to be f2 = 1, then the\\n\\ufb01lters perform like a linear combination within the LR features. By adopting a smaller\\n\\ufb01lter number n2 = s << d, the LR feature dimension is reduced from d to s. Here s\\nis the second sensitive variable that determines the level of shrinking, and the second\\nlayer can be represented as Conv(1, s, d). This strategy greatly reduces the number of\\nparameters (detailed computation in Section 3.3).\\nNon-linear mapping: The non-linear mapping step is the most important part that affects the SR performance, and the most in\\ufb02uencing factors are the width (i.e., the number of \\ufb01lters in a layer) and depth (i.e., the number of layers) of the mapping layer.\\nAs indicated in SRCNN , a 5 \\u00d7 5 layer achieves much better results than a 1 \\u00d7 1\\nlayer. But they are lack of experiments on very deep networks. The above experiences\\nhelp us to formulate a more ef\\ufb01cient mapping layer for FSRCNN. First, as a trade-off\\nbetween the performance and network scale, we adopt a medium \\ufb01lter size f3 = 3.\\nThen, to maintain the same good performance as SRCNN, we use multiple 3 \\u00d7 3 layers\\nto replace a single wide one. The number of mapping layers is another sensitive variable (denoted as m), which determines both the mapping accuracy and complexity. To\\nbe consistent, all mapping layers contain the same number of \\ufb01lters n3 = s. Then the\\nnon-linear mapping part can be represented as m \\u00d7 Conv(3, s, s).\\nExpanding: The expanding layer acts like an inverse process of the shrinking layer.\\nThe shrinking operation reduces the number of LR feature dimension for the sake of\\nthe computational ef\\ufb01ciency. However, if we generate the HR image directly from these\\nlow-dimensional features, the \\ufb01nal restoration quality will be poor. Therefore, we add\\nan expanding layer after the mapping part to expand the HR feature dimension. To\\nmaintain consistency with the shrinking layer, we also adopt 1 \\u00d7 1 \\ufb01lters, the number\\nof which is the same as that for the LR feature extraction layer. As opposed to the\\nshrinking layer Conv(1, s, d), the expanding layer is Conv(1, d, s). Experiments show\\nthat without the expanding layer, the performance decreases up to 0.3 dB on the Set5\\ntest set .\\nDeconvolution: The last part is a deconvolution layer, which upsamples and aggregates the previous features with a set of deconvolution \\ufb01lters. The deconvolution can\\nbe regarded as an inverse operation of the convolution. For convolution, the \\ufb01lter is\\nconvolved with the image with a stride k, and the output is 1/k times of the input.\\nContrarily, if we exchange the position of the input and output, the output will be k\\ntimes of the input, as depicted in Figure 4. We take advantage of this property to set\\nthe stride k = n, which is the desired upscaling factor. Then the output is directly the\\nreconstructed HR image.\\nAccelerating the Super-Resolution Convolutional Neural Network\\nFig. 3. The learned deconvolution layer (56 channels) for the upscaling factor 3.\\nWhen we determine the \\ufb01lter size of the deconvolution \\ufb01lters, we can look at the network from another perspective. Interestingly, the reversed network is like a downscaling operator that accepts an HR image and outputs the LR one. Then the deconvolution\\nlayer becomes a convolution layer with a stride n. As it extracts features from the HR\\nimage, we should adopt 9 \\u00d7 9 \\ufb01lters that are consistent with the \\ufb01rst layer of SRCNN.\\nSimilarly, if we reverse back, the deconvolution \\ufb01lters should also have a spatial size\\nf5 = 9. Experiments also demonstrate this assumption. Figure 3 shows the learned deconvolution \\ufb01lters, the patterns of which are very similar to that of the \\ufb01rst-layer \\ufb01lters\\nin SRCNN. Lastly, we can represent the deconvolution layer as DeConv(9, 1, d).\\nDifferent from inserting traditional interpolation kernels (e.g., bicubic or bilinear)\\nin-network or having \\u2018unpooling+convolution\\u2019 , the deconvolution layer learns\\na set of upsampling kernel for the input feature maps. As shown in Figure 3, these kernels are diverse and meaningful. If we force these kernels to be identical, the parameters\\nwill be used inef\\ufb01ciently (equal to sum up the input feature maps as one), and the performance will drop at least 0.9 dB on the Set5.\\nPReLU: For the activation function after each convolution layer, we suggest the use\\nof the Parametric Recti\\ufb01ed Linear Unit (PReLU) instead of the commonly-used\\nRecti\\ufb01ed Linear Unit (ReLU). They are different on the coef\\ufb01cient of the negative\\npart. For ReLU and PReLU, we can de\\ufb01ne a general activation function as f(xi) =\\nmax(xi, 0) + aimin(0, xi), where xi is the input signal of the activation f on the i-th\\nchannel, and ai is the coef\\ufb01cient of the negative part. The parameter ai is \\ufb01xed to be\\nzero for ReLU, but is learnable for PReLU. We choose PReLU mainly to avoid the\\n\\u201cdead features\\u201d caused by zero gradients in ReLU. Then we can make full use of\\nall parameters to test the maximum capacity of different network designs. Experiments\\nshow that the performance of the PReLU-activated networks is more stable, and can be\\nseen as the up-bound of that for the ReLU-activated networks.\\nOverall structure: We can connect the above \\ufb01ve parts to form a complete FSRCNN\\nnetwork as Conv(5, d, 1)\\u2212PReLU \\u2212Conv(1, s, d)\\u2212PReLU \\u2212m\\u00d7Conv(3, s, s)\\u2212\\nPReLU \\u2212Conv(1, d, s) \\u2212PReLU \\u2212DeConv(9, 1, d). On the whole, there are three\\nsensitive variables (i.e., the LR feature dimension d, the number of shrinking \\ufb01lters s,\\nand the mapping depth m) governing the performance and speed. For simplicity, we\\nrepresent a FSRCNN network as FSRCNN(d, s, m). The computational complexity\\ncan be calculated as\\nO{(25d + sd + 9ms2 + ds + 81d)SLR} = O{(9ms2 + 2sd + 106d)SLR}.\\nChao Dong et al.\\nTable 1. The transitions from SRCNN to FSRCNN.\\nTransition State 1 Transition State 2 FSRCNN (56,12,4)\\nFirst part\\nConv(9,64,1)\\nConv(9,64,1)\\nConv(9,64,1)\\nConv(5,56,1)\\nConv(1,12,64)-\\nConv(1,12,56)-\\nConv(5,32,64)\\nConv(5,32,64)\\n4Conv(3,12,12)\\n4Conv(3,12,12)\\n-Conv(1,64,12)\\n-Conv(1,56,12)\\nConv(5,1,32)\\nDeConv(9,1,32)\\nDeConv(9,1,64)\\nDeConv(9,1,56)\\nInput size\\nParameters\\nPSNR (Set5)\\nWe exclude the parameters of PReLU , which introduce negligible computational cost.\\nInterestingly, the new structure looks like an hourglass, which is symmetrical on the\\nwhole, thick at the ends, and thin in the middle. The three sensitive variables are just\\nthe controlling parameters for the appearance of the hourglass. Experiments show that\\nthis hourglass design is very effective for image super-resolution.\\nCost function: Following SRCNN, we adopt the mean square error (MSE) as the cost\\nfunction. The optimization objective is represented as\\ni=1 ||F(Y i\\ns ; \\u03b8) \\u2212Xi||2\\ns and Xi are the i-th LR and HR sub-image pair in the training data, and\\ns ; \\u03b8) is the network output for Y i\\ns with parameters \\u03b8. All parameters are optimized\\nusing stochastic gradient descent with the standard backpropagation.\\nDifferences against SRCNN: From SRCNN to FSRCNN\\nTo better understand how we accelerate SRCNN, we transform the SRCNN-Ex to another FSRCNN (56,12,4) within three steps, and show how much acceleration and\\nPSNR gain are obtained by each step. We use a representative upscaling factor n = 3.\\nThe network con\\ufb01gurations of SRCNN, FSRCNN and the two transition states are\\nshown in Table 1. We also show their performance (average PSNR on Set5) trained\\non the 91-image dataset .\\nFirst, we replace the last convolution layer of SRCNN-Ex with a deconvolution\\nlayer, then the whole network will perform on the original LR image and the computation complexity is proportional to SLR instead of SHR. This step will enlarge the\\nnetwork scale but achieve a speedup of 8.7\\u00d7 (i.e., 57184/58976 \\u00d7 32). As the learned\\ndeconvolution kernels are better than a single bicubic kernel, the performance increases\\nroughly by 0.12 dB. Second, the single mapping layer is replaced with the combination\\nof a shrinking layer, 4 mapping layers and an expanding layer. Overall, there are 5 more\\nlayers, but the parameters are decreased from 58,976 to 17,088. Also, the acceleration\\nafter this step is the most prominent \\u2013 30.1\\u00d7. It is widely observed that depth is the\\nkey factor that affects the performance. Here, we use four \\u201cnarrow\\u201d layers to replace\\na single \\u201cwide\\u201d layer, thus achieving better results (33.01 dB) with much less parameters. Lastly, we adopt smaller \\ufb01lter sizes and less \\ufb01lters (e.g., from Conv(9, 64, 1)\\nAccelerating the Super-Resolution Convolutional Neural Network\\nConvolution layers\\nStride = 3\\nStride = 2\\nFor factor 2\\nFor factor 3\\nDeconvolution\\nThe convolution filters can be shared for\\ndifferent upscaling factors\\nFig. 4. The FSRCNN consists of convolution layers and a deconvolution layer. The convolution\\nlayers can be shared for different upscaling factors. A speci\\ufb01c deconvolution layer is trained for\\ndifferent upscaling factors.\\nto Conv(5, 56, 1)), and obtain a \\ufb01nal speedup of 41.3\\u00d7. As we remove some redundant parameters, the network is trained more ef\\ufb01ciently and achieves another 0.05 dB\\nimprovement.\\nIt is worth noting that this acceleration is NOT at the cost of performance degradation. Contrarily, the FSRCNN (56,12,4) outperforms SRCNN-Ex by a large margin\\n(e.g., 0.23dB on the Set5 dataset). The main reasons of high performance have been\\npresented in the above analysis. This is the main difference between our method and\\nother CNN acceleration works . Nevertheless, with the guarantee of good performance, it is easier to cooperate with other acceleration methods to get a faster model.\\nSR for Different Upscaling Factors\\nAnother advantage of FSRCNN over the previous learning-based methods is that FSR-\\nCNN could achieve fast training and testing across different upscaling factors. Specifically, we \\ufb01nd that all convolution layers on the whole act like a complex feature extractor of the LR image, and only the last deconvolution layer contains the information\\nof the upscaling factor. This is also proved by experiments, of which the convolution\\n\\ufb01lters are almost the same for different upscaling factors3. With this property, we can\\ntransfer the convolution \\ufb01lters for fast training and testing.\\nIn practice, we train a model for an upscaling factor in advance. Then during training, we only \\ufb01ne-tune the deconvolution layer for another upscaling factor and leave\\nthe convolution layers unchanged. The \\ufb01ne-tuning is fast, and the performance is as\\ngood as training from scratch (see Section 4.4). During testing, we perform the convolution operations once, and upsample an image to different sizes with the corresponding\\ndeconvolution layer. If we need to apply several upscaling factors simultaneously, this\\nproperty can lead to much faster testing (as illustrated in Figure 4).\\n3 Note that in SRCNN and SCN, the convolution \\ufb01lters differ a lot for different upscaling factors.\\nChao Dong et al.\\nExperiments\\nImplementation Details\\nTraining dataset. The 91-image dataset is widely used as the training set in learningbased SR methods . As deep models generally bene\\ufb01t from big data, studies\\nhave found that 91 images are not enough to push a deep model to the best performance.\\nYang et al. and Schulter et al. use the BSD500 dataset . However, images\\nin the BSD500 are in JPEG format, which are not optimal for the SR task. Therefore,\\nwe contribute a new General-100 dataset that contains 100 bmp-format images (with\\nno compression)4. The size of the newly introduced 100 images ranges from 710 \\u00d7 704\\n(large) to 131 \\u00d7 112 (small). They are all of good quality with clear edges but fewer\\nsmooth regions (e.g., sky and ocean), thus are very suitable for the SR training. In\\nthe following experiments, apart from using the 91-image dataset for training, we will\\nalso evaluate the applicability of the joint set of the General-100 dataset and the 91image dataset to train our networks. To make full use of the dataset, we also adopt data\\naugmentation as in . We augment the data in two ways. 1) Scaling: each image is\\ndownscaled with the factor 0.9, 0,8, 0.7 and 0.6. 2) Rotation: each image is rotated with\\nthe degree of 90, 180 and 270. Then we will have 5 \\u00d7 4 \\u22121 = 19 times more images\\nfor training.\\nTest and validation dataset. Following SRCNN and SCN, we use the Set5 , Set14\\n and BSD200 dataset for testing. Another 20 images from the validation set of\\nthe BSD500 dataset are selected for validation.\\nTraining samples. To prepare the training data, we \\ufb01rst downsample the original training images by the desired scaling factor n to form the LR images. Then we crop the LR\\ntraining images into a set of fsub \\u00d7 fsub-pixel sub-images with a stride k. The corresponding HR sub-images (with size (nfsub)2) are also cropped from the ground truth\\nimages. These LR/HR sub-image pairs are the primary training data.\\nFor the issue of padding, we empirically \\ufb01nd that padding the input or output maps\\ndoes little effect on the \\ufb01nal performance. Thus we adopt zero padding in all layers\\naccording to the \\ufb01lter size. In this way, there is no need to change the sub-image size\\nfor different network designs. Another issue affecting the sub-image size is the deconvolution layer. As we train our models with the Caffe package , its deconvolution\\n\\ufb01lters will generate the output with size (nfsub \\u2212n + 1)2 instead of (nfsub)2. So we\\nalso crop (n \\u22121)-pixel borders on the HR sub-images. Finally, for \\u00d72, \\u00d73 and \\u00d74, we\\nset the size of LR/HR sub-images to be 102/192, 72/192 and 62/212, respectively.\\nTraining strategy. For fair comparison with the state-of-the-arts (Sec. 4.5), we adopt\\nthe 91-image dataset for training. In addition, we also explore a two-step training strategy. First, we train a network from scratch with the 91-image dataset. Then, when the\\ntraining is saturated, we add the General-100 dataset for \\ufb01ne-tuning. With this strategy, the training converges much earlier than training with the two datasets from the\\nbeginning.\\nWhen training with the 91-image dataset, the learning rate of the convolution layers\\nis set to be 10\\u22123 and that of the deconvolution layer is 10\\u22124. Then during \\ufb01ne-tuning,\\n4 We follow to introduce only 100 images in a new super-resolution dataset. A larger dataset\\nwith more training images will be released on the project page.\\nAccelerating the Super-Resolution Convolutional Neural Network\\nTable 2. The comparison of PSNR (Set5) and parameters of different settings.\\nd = 48, s = 12 32.87 (8832) 32.88 (10128) 33.08 (11424)\\nd = 56, s = 12 33.00 (9872) 32.97 (11168) 33.16 (12464)\\nd = 48, s = 16 32.95 (11232) 33.10 (13536) 33.18 (15840)\\nd = 56, s = 16 33.01 (12336) 33.12 (14640) 33.17 (16944)\\nthe learning rate of all layers is reduced by half. For initialization, the weights of the\\nconvolution \\ufb01lters are initialized with the method designed for PReLU in . As we\\ndo not have activation functions at the end, the deconvolution \\ufb01lters are initialized by\\nthe same way as in SRCNN (i.e., drawing randomly from a Gaussian distribution with\\nzero mean and standard deviation 0.001).\\nInvestigation of Different Settings\\nTo test the property of the FSRCNN structure, we design a set of controlling experiments with different values of the three sensitive variables \\u2013 the LR feature dimension\\nd, the number of shrinking \\ufb01lters s, and the mapping depth m. Speci\\ufb01cally, we choose\\nd = 48, 56, s = 12, 16 and m = 2, 3, 4, thus we conduct a total of 2 \\u00d7 2 \\u00d7 3 = 12\\nexperiments with different combinations.\\nThe average PSNR values on the Set5 dataset of these experiments are shown in\\nTable 2. We analyze the results in two directions, i.e., horizontally and vertically in\\nthe table. First, we \\ufb01x d, s and examine the in\\ufb02uence of m. Obviously, m = 4 leads\\nto better results than m = 2 and m = 3. This trend can also be observed from the\\nconvergence curves shown in Figure 5(a). Second, we \\ufb01x m and examine the in\\ufb02uence\\nof d and s. In general, a better result usually requires more parameters (e.g., a larger d\\nor s), but more parameters do not always guarantee a better result. This trend is also\\nre\\ufb02ected in Figure 5(b), where we see the three largest networks converge together.\\nFrom all the results, we \\ufb01nd the best trade-off between performance and parameters \\u2013\\nFSRCNN (56,12,4), which achieves one of the highest results with a moderate number\\nof parameters.\\nIt is worth noticing that the smallest network FSRCNN (48,12,2) achieves an average PSNR of 32.87 dB, which is already higher than that of SRCNN-Ex (32.75 dB)\\nreported in . The FSRCNN (48,12,2) contains only 8,832 parameters, then the acceleration compared with SRCNN-Ex is 57184/8832 \\u00d7 9 = 58.3 times.\\nTowards Real-Time SR with FSRCNN\\nNow we want to \\ufb01nd a more concise FSRCNN network that could realize real-time SR\\nwhile still keep good performance. First, we calculate how many parameters can meet\\nthe minimum requirement of real-time implementation (24 fps). As mentioned in the\\nintroduction, the speed of SRCNN to upsample an image to the size 760 \\u00d7 760 is 1.32\\nfps. The upscaling factor is 3, and SRCNN has 8032 parameters. Then according to\\nEquation 1 and 2, the desired FSRCNN network should have at most 8032\\u00d71.32/24\\u00d7\\nChao Dong et al.\\nNumber)of)backprops\\nAverage)test)PSNR)(dB)\\nFSRCNN)(56,16,4)\\nFSRCNN)(56,16,3)\\nFSRCNN)(56,16,2)\\nNumber)of)backprops\\nAverage)test)PSNR)(dB)\\nFSRCNN)(56,16,4)\\nFSRCNN)(48,16,4)\\nFSRCNN)(56,12,4)\\nFSRCNN)(48,12,4)\\n(a) We fix d=56, s=16 and change m\\n(b) We fix m=4 and change d, s\\nFig. 5. Convergence curves of different network designs.\\nNumberdofdbackprops\\nAveragedtestdPSNRdldBy\\nfine-tuningdthedlastdlayer\\ntrainingdfromdscratch\\nFig. 6. Convergence curves for different training strategies.\\n32 \\u22483976 parameters. To achieve this goal, we \\ufb01nd an appropriate con\\ufb01guration \\u2013\\nFSRCNN (32,5,1) that contains 3937 parameters. With our C++ test code, the speed of\\nFSRCNN (32,5,1) reaches 24.7 fps, satisfying the real-time requirement. Furthermore,\\nthe FSRCNN (32,5,1) even outperforms SRCNN (9-1-5) (see Table 3 and 4).\\nExperiments for Different Upscaling Factors\\nUnlike existing methods that need to train a network from scratch for a different scaling factor, the proposed FSRCNN enjoys the \\ufb02exibility of learning and testing across upscaling factors through transferring the convolution \\ufb01lters (Sec. 3.4). We\\ndemonstrate this \\ufb02exibility in this section. We choose the FSRCNN (56,12,4) as the\\ndefault network. As we have obtained a well-trained model under the upscaling factor 3\\n(in Section 4.2), we then train the network for \\u00d72 on the basis of that for \\u00d73. To be speci\\ufb01c, the parameters of all convolution \\ufb01lters in the well-trained model are transferred\\nto the network of \\u00d72. During training, we only \\ufb01ne-tune the deconvolution layer on the\\n91-image and General-100 datasets of \\u00d72. For comparison, we train another network\\nalso for \\u00d72 but from scratch. The convergence curves of these two networks are shown\\nin Figure 6. Obviously, with the transferred parameters, the network converges very fast\\n(only a few hours) with the same good performance as that training form scratch. In the\\nfollowing experiments, we only train the networks from scratch for \\u00d73, and \\ufb01ne-tune\\nthe corresponding deconvolution layers for \\u00d72 and \\u00d74.\\nComparison with State-of-the-Arts\\nCompare using the same training set. First, we compare our method with four stateof-the-art learning-based SR algorithms that rely on external databases, namely the\\nAccelerating the Super-Resolution Convolutional Neural Network\\nsuper-resolution forest (SRF) , SRCNN , SRCNN-Ex and the sparse coding\\nbased network (SCN) . The implementations of these methods are all based on their\\nreleased source code. As they are written in different programming languages, the comparison of their test time may not be fair, but still re\\ufb02ects the main trend. To have a fair\\ncomparison on restoration quality, all models are trained on the augmented 91-image\\ndataset, so the results are slightly different from that in the corresponding paper. We select two representative FSRCNN networks \\u2013 FSRCNN (short for FSRCNN (56,12,4)),\\nand FSRCNN-s (short for FSRCNN (32,5,1)). The inference time is tested with the\\nC++ implementation on an Intel i7 CPU 4.0 GHz. The quantitative results (PSNR and\\ntest time) for different upscaling factors are listed in Table 3. We \\ufb01rst look at the test\\ntime, which is the main focus of our work. The proposed FSRCNN is undoubtedly the\\nfastest method that is at least 40 times faster than SRCNN-Ex, SRF and SCN (with\\nthe upscaling factor 3), while the fastest FSRCNN-s can achieve real-time performance\\n(> 24 fps) on almost all the test images. Moreover, the FSRCNN still outperforms the\\nprevious methods on the PSNR values especially for \\u00d72 and \\u00d73. We also notice that the\\nFSRCNN achieves slightly lower PSNR than SCN on factor 4. This is mainly because\\nthat the SCN adopts two models of \\u00d72 to upsample an image by \\u00d74. We have also tried\\nthis strategy and achieved comparable results. However, as we pay more attention to\\nspeed, we still present the results of a single network.\\nCompare using different training sets (following the literature). To follow the literature, we also compare the best PSNR results that are reported in the corresponding\\npaper, as shown in Table 4. We also add another two competitive methods \\u2013 KK \\nand A+ for comparison. Note that these results are obtained using different datasets,\\nand our models are trained on the 91-image and General-100 datasets. From Table 4, we\\ncan see that the proposed FSRCNN still outperforms other methods on most upscaling\\nfactors and datasets. We have also done comprehensive comparisons in terms of SSIM\\nand IFC in Table 5 and 6, where we observe the same trend. The reconstructed\\nimages of FSRCNN (shown in Figure 7 and 8), more examples can be found on the\\nproject page) are sharper and clearer than other results. In another aspect, the restoration\\nquality of small models (FSRCNN-s and SRCNN) is slightly worse than large models\\n(SRCNN-Ex, SCN and FSRCNN). In Figure 7, we could observe some \\u201djaggies\\u201d or\\nringing effects in the results of FSRCNN-s and SRCNN.\\nConclusion\\nWhile observing the limitations of current deep learning based SR models, we explore\\na more ef\\ufb01cient network structure to achieve high running speed without the loss of\\nrestoration quality. We approach this goal by re-designing the SRCNN structure, and\\nachieves a \\ufb01nal acceleration of more than 40 times. Extensive experiments suggest that\\nthe proposed method yields satisfactory SR performance, while superior in terms of\\nrun time. The proposed model can be adapted for real-time video SR, and motivate fast\\ndeep models for other low-level vision tasks.\\nAcknowledgment. This work is partially supported by SenseTime Group Limited.\\nChao Dong et al.\\nTable 3. The results of PSNR (dB) and test time (sec) on three test datasets. All models are trained\\non the 91-image dataset.\\nSRCNN SRCNN-Ex \\nPSNR Time PSNR Time PSNR Time PSNR\\nPSNR Time PSNR Time PSNR Time\\n36.33 0.18 36.67\\n36.76 0.94 36.53 0.024 36.94 0.068\\n32.15 0.39 32.35\\n32.22 0.061 32.54 0.16\\n31.34 0.23 31.53\\n31.44 0.033 31.73 0.098\\n32.45 0.18 32.83\\n32.55 0.010 33.06 0.027\\n29.01 0.39 29.26\\n29.08 0.023 29.37 0.061\\n28.27 0.23 28.47\\n28.32 0.013 28.55 0.035\\n30.15 0.18 30.45\\n30.04 0.0052 30.55 0.015\\n27.21 0.39 27.44\\n27.12 0.0099 27.50 0.029\\n26.72 0.23 26.88\\n26.73 0.0072 26.92 0.019\\nTable 4. The results of PSNR (dB) on three test datasets. We present the best results reported\\nin the corresponding paper. The proposed FSCNN and FSRCNN-s are trained on both 91-image\\nand General-100 dataset. More comparisons with other methods on PSNR, SSIM and IFC \\ncan be found in the supplementary \\ufb01le.\\nupscaling Bicubic KK A+ SRF SRCNN SRCNN-Ex SCN FSRCNN-s FSRCNN\\nBicubicx/x31.68xdB\\nOriginalx/xPSNR\\nSCNx/x33.61xdB\\nFSRCNNx/x33.85xdB\\nSRFx/x33.53xdB\\nSRCNNx/x33.39xdB\\nFSRCNN-sx/x33.43xdB\\nSRCNN-Exx/x33.67xdB\\nFig. 7. The \\u201clenna\\u201d image from the Set14 dataset with an upscaling factor 3.\\nAccelerating the Super-Resolution Convolutional Neural Network\\nTable 5. The results of PSNR (dB), SSIM and IFC on the Set5 , Set14 and\\nBSD200 datasets.\\nPSNR/SSIM/IFC PSNR/SSIM/IFC PSNR/SSIM/IFC PSNR/SSIM/IFC PSNR/SSIM/IFC\\n33.66/0.9299/6.10 36.20/0.9511/6.87 35.83/0.9499/8.09 36.55/0.9544/8.48 36.87/0.9556/8.63\\n30.23/0.8687/6.09 32.11/0.9026/6.83 31.80/0.9004/7.81 32.28/0.9056/8.11 32.51/0.9074/8.22\\n29.70/0.8625/5.70 31.30/0.9000/6.26 31.02/0.8968/7.27 31.44/0.9031/7.49 31.65/0.9053/7.60\\n30.39/0.9299/6.10 32.28/0.9033/4.14 31.92/0.8968/4.52 32.59/0.9088/4.84 32.71/0.9098/4.90\\n27.54/0.7736/3.41 28.94/0.8132/3.83 28.65/0.8093/4.23 29.13/0.8188/4.45 29.23/0.8206/4.49\\n27.26/0.7638/3.19 28.19/0.8016/3.49 28.02/0.7981/3.91 28.36/0.8078/4.07 28.45/0.8095/4.11\\n28.42/0.8104/2.35 30.03/0.8541/2.81 29.69/0.8419/3.02 30.28/0.8603/3.26 30.35/0.8600/3.26\\n26.00/0.7019/2.23 27.14/0.7419/2.57 26.85/0.7353/2.78 27.32/0.7471/2.74 27.41/0.7497/2.94\\n25.97/0.6949/2.04 26.68/0.7282/2.22 26.56/0.7253/2.51 26.83/0.7359/2.62 26.89/0.7368/2.62\\nTable 6. The results of PSNR (dB), SSIM and IFC on the Set5 , Set14 and\\nBSD200 datasets.\\nSRCNN-Ex \\nPSNR/SSIM/IFC PSNR/SSIM/IFC PSNR/SSIM/IFC PSNR/SSIM/IFC PSNR/SSIM/IFC\\n36.34/0.9521/7.54 36.66/0.9542/8.05 36.76/0.9545/7.32 36.58/0.9532/7.75 37.00/0.9558/8.06\\n32.18/0.9039/7.22 32.45/0.9067/7.76 32.48/0.9067/7.00 32.28/0.9052/7.47 32.63/0.9088/7.71\\n31.38/0.9287/6.80 31.63/0.9044/7.26 31.63/0.9048/6.45 31.48/0.9027/7.01 31.80/0.9074/7.25\\n32.39/0.9033/4.25 32.75/0.9090/4.58 33.04/0.9136/4.37 32.54/0.9055/4.56 33.16/0.9140/4.88\\n29.00/0.8145/3.96 29.30/0.8215/4.26 29.37/0.8226/3.99 29.08/0.8167/4.24 29.43/0.8242/4.47\\n28.28/0.8038/3.67 28.48/0.8102/3.92 28.54/0.8119/3.59 28.32/0.8058/3.96 28.60/0.8137/4.11\\n30.09/0.8530/2.86 30.49/0.8628/3.01 30.82/0.8728/3.07 30.11/0.8499/2.76 30.71/0.8657/3.01\\n27.20/0.7413/2.60 27.50/0.7513/2.74 27.62/0.7571/2.71 27.19/0.7423/2.55 27.59/0.7535/2.70\\n26.73/0.7291/2.37 26.92/0.7376/2.46 27.02/0.7434/2.38 26.75/0.7312/2.32 26.98/0.7398/2.41\\nBicubic3/324.043dB\\nOriginal3/3PSNR\\nSCN3/328.573dB\\nFSRCNN3/328.683dB\\nSRF3/327.963dB\\nSRCNN3/327.583dB\\nFSRCNN-s3/327.733dB\\nSRCNN-Ex3/327.953dB\\nFig. 8. The \\u201cbutter\\ufb02y\\u201d image from the Set5 dataset with an upscaling factor 3.\\nChao Dong et al.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"p4225\",\n          \"p2246\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"referenced_paper\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Published as a conference paper at ICLR 2018\\nmixup: BEYOND EMPIRICAL RISK MINIMIZATION\\nHongyi Zhang\\nMoustapha Cisse, Yann N. Dauphin, David Lopez-Paz\\u2217\\nLarge deep neural networks are powerful, but exhibit undesirable behaviors such\\nas memorization and sensitivity to adversarial examples. In this work, we propose\\nmixup, a simple learning principle to alleviate these issues. In essence, mixup trains\\na neural network on convex combinations of pairs of examples and their labels.\\nBy doing so, mixup regularizes the neural network to favor simple linear behavior\\nin-between training examples. Our experiments on the ImageNet-2012, CIFAR-10,\\nCIFAR-100, Google commands and UCI datasets show that mixup improves the\\ngeneralization of state-of-the-art neural network architectures. We also \\ufb01nd that\\nmixup reduces the memorization of corrupt labels, increases the robustness to\\nadversarial examples, and stabilizes the training of generative adversarial networks.\\nINTRODUCTION\\nLarge deep neural networks have enabled breakthroughs in \\ufb01elds such as computer vision , speech recognition , and reinforcement learning .\\nIn most successful applications, these neural networks share two commonalities. First, they are\\ntrained as to minimize their average error over the training data, a learning rule also known as the\\nEmpirical Risk Minimization (ERM) principle . Second, the size of these state-of-theart neural networks scales linearly with the number of training examples. For instance, the network of\\nSpringenberg et al. used 106 parameters to model the 5 \\u00b7 104 images in the CIFAR-10 dataset,\\nthe network of used 108 parameters to model the 106 images in the\\nImageNet-2012 dataset, and the network of Chelba et al. used 2 \\u00b7 1010 parameters to model\\nthe 109 words in the One Billion Word dataset.\\nStrikingly, a classical result in learning theory tells us that the\\nconvergence of ERM is guaranteed as long as the size of the learning machine (e.g., the neural\\nnetwork) does not increase with the number of training data. Here, the size of a learning machine is\\nmeasured in terms of its number of parameters or, relatedly, its VC-complexity .\\nThis contradiction challenges the suitability of ERM to train our current neural network models, as\\nhighlighted in recent research. On the one hand, ERM allows large neural networks to memorize\\n(instead of generalize from) the training data even in the presence of strong regularization, or in\\nclassi\\ufb01cation problems where the labels are assigned at random . On the other\\nhand, neural networks trained with ERM change their predictions drastically when evaluated on\\nexamples just outside the training distribution , also known as adversarial\\nexamples. This evidence suggests that ERM is unable to explain or provide generalization on testing\\ndistributions that differ only slightly from the training data. However, what is the alternative to ERM?\\nThe method of choice to train on similar but different examples to the training data is known as data\\naugmentation , formalized by the Vicinal Risk Minimization (VRM) principle\\n . In VRM, human knowledge is required to describe a vicinity or neighborhood\\naround each example in the training data. Then, additional virtual examples can be drawn from the\\nvicinity distribution of the training examples to enlarge the support of the training distribution. For\\ninstance, when performing image classi\\ufb01cation, it is common to de\\ufb01ne the vicinity of one image\\nas the set of its horizontal re\\ufb02ections, slight rotations, and mild scalings. While data augmentation\\nconsistently leads to improved generalization , the procedure is dataset-dependent,\\nand thus requires the use of expert knowledge. Furthermore, data augmentation assumes that the\\n\\u2217Alphabetical order.\\n \\nPublished as a conference paper at ICLR 2018\\nexamples in the vicinity share the same class, and does not model the vicinity relation across examples\\nof different classes.\\nContribution\\nMotivated by these issues, we introduce a simple and data-agnostic data augmentation routine, termed mixup (Section 2). In a nutshell, mixup constructs virtual training examples\\n\\u02dcx = \\u03bbxi + (1 \\u2212\\u03bb)xj,\\nwhere xi, xj are raw input vectors\\n\\u02dcy = \\u03bbyi + (1 \\u2212\\u03bb)yj,\\nwhere yi, yj are one-hot label encodings\\n(xi, yi) and (xj, yj) are two examples drawn at random from our training data, and \\u03bb \\u2208 .\\nTherefore, mixup extends the training distribution by incorporating the prior knowledge that linear\\ninterpolations of feature vectors should lead to linear interpolations of the associated targets. mixup\\ncan be implemented in a few lines of code, and introduces minimal computation overhead.\\nDespite its simplicity, mixup allows a new state-of-the-art performance in the CIFAR-10, CIFAR-\\n100, and ImageNet-2012 image classi\\ufb01cation datasets (Sections 3.1 and 3.2). Furthermore, mixup\\nincreases the robustness of neural networks when learning from corrupt labels (Section 3.4), or facing\\nadversarial examples (Section 3.5). Finally, mixup improves generalization on speech (Sections 3.3)\\nand tabular (Section 3.6) data, and can be used to stabilize the training of GANs (Section 3.7). The\\nsource-code necessary to replicate our CIFAR-10 experiments is available at:\\n \\nTo understand the effects of various design choices in mixup, we conduct a thorough set of ablation\\nstudy experiments (Section 3.8). The results suggest that mixup performs signi\\ufb01cantly better than\\nrelated methods in previous work, and each of the design choices contributes to the \\ufb01nal performance.\\nWe conclude by exploring the connections to prior work (Section 4), as well as offering some points\\nfor discussion (Section 5).\\nFROM EMPIRICAL RISK MINIMIZATION TO mixup\\nIn supervised learning, we are interested in \\ufb01nding a function f \\u2208F that describes the relationship\\nbetween a random feature vector X and a random target vector Y , which follow the joint distribution\\nP(X, Y ). To this end, we \\ufb01rst de\\ufb01ne a loss function \\u2113that penalizes the differences between\\npredictions f(x) and actual targets y, for examples (x, y) \\u223cP. Then, we minimize the average of\\nthe loss function \\u2113over the data distribution P, also known as the expected risk:\\n\\u2113(f(x), y)dP(x, y).\\nUnfortunately, the distribution P is unknown in most practical situations. Instead, we usually have\\naccess to a set of training data D = {(xi, yi)}n\\ni=1, where (xi, yi) \\u223cP for all i = 1, . . . , n. Using\\nthe training data D, we may approximate P by the empirical distribution\\nP\\u03b4(x, y) = 1\\n\\u03b4(x = xi, y = yi),\\nwhere \\u03b4(x = xi, y = yi) is a Dirac mass centered at (xi, yi). Using the empirical distribution P\\u03b4, we\\ncan now approximate the expected risk by the empirical risk:\\n\\u2113(f(x), y)dP\\u03b4(x, y) = 1\\n\\u2113(f(xi), yi).\\nLearning the function f by minimizing (1) is known as the Empirical Risk Minimization (ERM)\\nprinciple . While ef\\ufb01cient to compute, the empirical risk (1) monitors the behaviour\\nof f only at a \\ufb01nite set of n examples. When considering functions with a number parameters\\ncomparable to n (such as large neural networks), one trivial way to minimize (1) is to memorize the\\ntraining data . Memorization, in turn, leads to the undesirable behaviour of f\\noutside the training data .\\nPublished as a conference paper at ICLR 2018\\n# y1, y2 should be one-hot vectors\\nfor (x1, y1), (x2, y2) in zip(loader1, loader2):\\nlam = numpy.random.beta(alpha, alpha)\\nx = Variable(lam * x1 + (1. - lam) * x2)\\ny = Variable(lam * y1 + (1. - lam) * y2)\\noptimizer.zero_grad()\\nloss(net(x), y).backward()\\noptimizer.step()\\n(a) One epoch of mixup training in PyTorch.\\n(b) Effect of mixup (\\u03b1 = 1) on a\\ntoy problem.\\nGreen: Class 0.\\nOrange: Class 1. Blue shading indicates\\np(y = 1|x).\\nFigure 1: Illustration of mixup, which converges to ERM as \\u03b1 \\u21920.\\nHowever, the na\\u00a8\\u0131ve estimate P\\u03b4 is one out of many possible choices to approximate the true distribution P. For instance, in the Vicinal Risk Minimization (VRM) principle , the\\ndistribution P is approximated by\\nP\\u03bd(\\u02dcx, \\u02dcy) = 1\\n\\u03bd(\\u02dcx, \\u02dcy|xi, yi),\\nwhere \\u03bd is a vicinity distribution that measures the probability of \\ufb01nding the virtual feature-target\\npair (\\u02dcx, \\u02dcy) in the vicinity of the training feature-target pair (xi, yi). In particular, Chapelle et al.\\n considered Gaussian vicinities \\u03bd(\\u02dcx, \\u02dcy|xi, yi) = N(\\u02dcx \\u2212xi, \\u03c32)\\u03b4(\\u02dcy = yi), which is equivalent\\nto augmenting the training data with additive Gaussian noise. To learn using VRM, we sample the\\nvicinal distribution to construct a dataset D\\u03bd := {(\\u02dcxi, \\u02dcyi)}m\\ni=1, and minimize the empirical vicinal\\n\\u2113(f(\\u02dcxi), \\u02dcyi).\\nThe contribution of this paper is to propose a generic vicinal distribution, called mixup:\\n\\u00b5(\\u02dcx, \\u02dcy|xi, yi) = 1\\n\\u03bb [\\u03b4(\\u02dcx = \\u03bb \\u00b7 xi + (1 \\u2212\\u03bb) \\u00b7 xj, \\u02dcy = \\u03bb \\u00b7 yi + (1 \\u2212\\u03bb) \\u00b7 yj)] ,\\nwhere \\u03bb \\u223cBeta(\\u03b1, \\u03b1), for \\u03b1 \\u2208(0, \\u221e). In a nutshell, sampling from the mixup vicinal distribution\\nproduces virtual feature-target vectors\\n\\u02dcx = \\u03bbxi + (1 \\u2212\\u03bb)xj,\\n\\u02dcy = \\u03bbyi + (1 \\u2212\\u03bb)yj,\\nwhere (xi, yi) and (xj, yj) are two feature-target vectors drawn at random from the training data, and\\n\\u03bb \\u2208 . The mixup hyper-parameter \\u03b1 controls the strength of interpolation between feature-target\\npairs, recovering the ERM principle as \\u03b1 \\u21920.\\nThe implementation of mixup training is straightforward, and introduces a minimal computation\\noverhead. Figure 1a shows the few lines of code necessary to implement mixup training in PyTorch.\\nFinally, we mention alternative design choices. First, in preliminary experiments we \\ufb01nd that convex\\ncombinations of three or more examples with weights sampled from a Dirichlet distribution does not\\nprovide further gain, but increases the computation cost of mixup. Second, our current implementation\\nuses a single data loader to obtain one minibatch, and then mixup is applied to the same minibatch\\nafter random shuf\\ufb02ing. We found this strategy works equally well, while reducing I/O requirements.\\nThird, interpolating only between inputs with equal label did not lead to the performance gains of\\nmixup discussed in the sequel. More empirical comparison can be found in Section 3.8.\\nWhat is mixup doing?\\nThe mixup vicinal distribution can be understood as a form of data augmentation that encourages the model f to behave linearly in-between training examples. We argue\\nthat this linear behaviour reduces the amount of undesirable oscillations when predicting outside the\\ntraining examples. Also, linearity is a good inductive bias from the perspective of Occam\\u2019s razor,\\nPublished as a conference paper at ICLR 2018\\n(a) Prediction errors in-between training data. Evaluated at x = \\u03bbxi+(1\\u2212\\u03bb)xj, a prediction is counted as\\na \\u201cmiss\\u201d if it does not belong to {yi, yj}. The model\\ntrained with mixup has fewer misses.\\n(b) Norm of the gradients of the model w.r.t. input\\nin-between training data, evaluated at x = \\u03bbxi +\\n(1 \\u2212\\u03bb)xj. The model trained with mixup has smaller\\ngradient norms.\\nFigure 2: mixup leads to more robust model behaviors in-between the training data.\\nTop-1 Error\\nTop-5 Error\\nERM \\nmixup \\u03b1 = 0.2\\nResNet-101\\nERM \\nmixup \\u03b1 = 0.2\\nResNeXt-101 32*4d\\nERM \\nmixup \\u03b1 = 0.4\\nResNeXt-101 64*4d\\nERM \\nmixup \\u03b1 = 0.4\\nmixup \\u03b1 = 0.2\\nResNet-101\\nmixup \\u03b1 = 0.2\\nResNeXt-101 32*4d\\nmixup \\u03b1 = 0.4\\nTable 1: Validation errors for ERM and mixup on the development set of ImageNet-2012.\\nsince it is one of the simplest possible behaviors. Figure 1b shows that mixup leads to decision\\nboundaries that transition linearly from class to class, providing a smoother estimate of uncertainty.\\nFigure 2 illustrate the average behaviors of two neural network models trained on the CIFAR-10\\ndataset using ERM and mixup. Both models have the same architecture, are trained with the same\\nprocedure, and are evaluated at the same points in-between randomly sampled training data. The\\nmodel trained with mixup is more stable in terms of model predictions and gradient norms in-between\\ntraining samples.\\nEXPERIMENTS\\nIMAGENET CLASSIFICATION\\nWe evaluate mixup on the ImageNet-2012 classi\\ufb01cation dataset . This\\ndataset contains 1.3 million training images and 50,000 validation images, from a total of 1,000 classes.\\nFor training, we follow standard data augmentation practices: scale and aspect ratio distortions,\\nrandom crops, and horizontal \\ufb02ips . During evaluation, only the 224 \\u00d7 224 central\\ncrop of each image is tested. We use mixup and ERM to train several state-of-the-art ImageNet-2012\\nclassi\\ufb01cation models, and report both top-1 and top-5 error rates in Table 1.\\nPublished as a conference paper at ICLR 2018\\nPreAct ResNet-18\\nWideResNet-28-10\\nDenseNet-BC-190\\nPreAct ResNet-18\\nWideResNet-28-10\\nDenseNet-BC-190\\n(a) Test errors for the CIFAR experiments.\\nCIFAR-10 Test Error\\nDenseNet-190 baseline\\nDenseNet-190 mixup\\n(b) Test error evolution for the best\\nERM and mixup models.\\nFigure 3: Test errors for ERM and mixup on the CIFAR experiments.\\nFor all the experiments in this section, we use data-parallel distributed training in Caffe21 with\\na minibatch size of 1,024. We use the learning rate schedule described in .\\nSpeci\\ufb01cally, the learning rate is increased linearly from 0.1 to 0.4 during the \\ufb01rst 5 epochs, and it is\\nthen divided by 10 after 30, 60 and 80 epochs when training for 90 epochs; or after 60, 120 and 180\\nepochs when training for 200 epochs.\\nFor mixup, we \\ufb01nd that \\u03b1 \\u2208[0.1, 0.4] leads to improved performance over ERM, whereas for large \\u03b1,\\nmixup leads to under\\ufb01tting. We also \\ufb01nd that models with higher capacities and/or longer training\\nruns are the ones to bene\\ufb01t the most from mixup. For example, when trained for 90 epochs, the mixup\\nvariants of ResNet-101 and ResNeXt-101 obtain a greater improvement (0.5% to 0.6%) over their\\nERM analogues than the gain of smaller models such as ResNet-50 (0.2%). When trained for 200\\nepochs, the top-1 error of the mixup variant of ResNet-50 is further reduced by 1.2% compared to the\\n90 epoch run, whereas its ERM analogue stays the same.\\nCIFAR-10 AND CIFAR-100\\nWe conduct additional image classi\\ufb01cation experiments on the CIFAR-10 and CIFAR-100 datasets\\nto further evaluate the generalization performance of mixup. In particular, we compare ERM and\\nmixup training for: PreAct ResNet-18 as implemented in , WideResNet-\\n28-10 as implemented in , and\\nDenseNet as implemented in . For DenseNet, we change the growth\\nrate to 40 to follow the DenseNet-BC-190 speci\\ufb01cation from . For mixup, we\\n\\ufb01x \\u03b1 = 1, which results in interpolations \\u03bb uniformly distributed between zero and one. All models\\nare trained on a single Nvidia Tesla P100 GPU using PyTorch2 for 200 epochs on the training set\\nwith 128 examples per minibatch, and evaluated on the test set. Learning rates start at 0.1 and are\\ndivided by 10 after 100 and 150 epochs for all models except WideResNet. For WideResNet, we\\nfollow and divide the learning rate by 10 after 60, 120 and 180\\nepochs. Weight decay is set to 10\\u22124. We do not use dropout in these experiments.\\nWe summarize our results in Figure 3a. In both CIFAR-10 and CIFAR-100 classi\\ufb01cation problems,\\nthe models trained using mixup signi\\ufb01cantly outperform their analogues trained with ERM. As seen\\nin Figure 3b, mixup and ERM converge at a similar speed to their best test errors. Note that the\\nDenseNet models in were trained for 300 epochs with further learning rate\\ndecays scheduled at the 150 and 225 epochs, which may explain the discrepancy the performance of\\nDenseNet reported in Figure 3a and the original result of Huang et al. .\\nSPEECH DATA\\nNext, we perform speech recognition experiments using the Google commands dataset . The dataset contains 65,000 utterances, where each utterance is about one-second long and\\nbelongs to one out of 30 classes. The classes correspond to voice commands such as yes, no, down,\\nleft, as pronounced by a few thousand different speakers. To preprocess the utterances, we \\ufb01rst\\n1 \\n2 \\nPublished as a conference paper at ICLR 2018\\nValidation set\\nmixup (\\u03b1 = 0.1)\\nmixup (\\u03b1 = 0.2)\\nmixup (\\u03b1 = 0.1)\\nmixup (\\u03b1 = 0.2)\\nFigure 4: Classi\\ufb01cation errors of ERM and mixup on the Google commands dataset.\\nextract normalized spectrograms from the original waveforms at a sampling rate of 16 kHz. Next, we\\nzero-pad the spectrograms to equalize their sizes at 160 \\u00d7 101. For speech data, it is reasonable to\\napply mixup both at the waveform and spectrogram levels. Here, we apply mixup at the spectrogram\\nlevel just before feeding the data to the network.\\nFor this experiment, we compare a LeNet and a VGG-11 architecture, each of them composed by two convolutional and two fully-connected layers.\\nWe train each model for 30 epochs with minibatches of 100 examples, using Adam as the optimizer\\n . Training starts with a learning rate equal to 3 \\u00d7 10\\u22123 and is divided by 10\\nevery 10 epochs. For mixup, we use a warm-up period of \\ufb01ve epochs where we train the network on\\noriginal training examples, since we \\ufb01nd it speeds up initial convergence. Table 4 shows that mixup\\noutperforms ERM on this task, specially when using VGG-11, the model with larger capacity.\\nMEMORIZATION OF CORRUPTED LABELS\\nFollowing Zhang et al. , we evaluate the robustness of ERM and mixup models against randomly\\ncorrupted labels. We hypothesize that increasing the strength of mixup interpolation \\u03b1 should generate\\nvirtual examples further from the training examples, making memorization more dif\\ufb01cult to achieve.\\nIn particular, it should be easier to learn interpolations between real examples compared to memorizing\\ninterpolations involving random labels. We adapt an open-source implementation \\nto generate three CIFAR-10 training sets, where 20%, 50%, or 80% of the labels are replaced by\\nrandom noise, respectively. All the test labels are kept intact for evaluation. Dropout is considered the state-of-the-art method for learning with corrupted labels . Thus, we compare in these experiments mixup, dropout, mixup + dropout, and ERM. For\\nmixup, we choose \\u03b1 \\u2208{1, 2, 8, 32}; for dropout, we add one dropout layer in each PreAct block after\\nthe ReLU activation layer between two convolution layers, as suggested in . We choose the dropout probability p \\u2208{0.5, 0.7, 0.8, 0.9}. For the combination of mixup\\nand dropout, we choose \\u03b1 \\u2208{1, 2, 4, 8} and p \\u2208{0.3, 0.5, 0.7}. These experiments use the PreAct\\nResNet-18 model implemented in . All the other settings are the same as\\nin Section 3.2.\\nWe summarize our results in Table 2, where we note the best test error achieved during the training\\nsession, as well as the \\ufb01nal test error after 200 epochs. To quantify the amount of memorization, we\\nalso evaluate the training errors at the last epoch on real labels and corrupted labels. As the training\\nprogresses with a smaller learning rate (e.g. less than 0.01), the ERM model starts to over\\ufb01t the\\ncorrupted labels. When using a large probability (e.g. 0.7 or 0.8), dropout can effectively reduce\\nover\\ufb01tting. mixup with a large \\u03b1 (e.g. 8 or 32) outperforms dropout on both the best and last epoch\\ntest errors, and achieves lower training error on real labels while remaining resistant to noisy labels.\\nInterestingly, mixup + dropout performs the best of all, showing that the two methods are compatible.\\nROBUSTNESS TO ADVERSARIAL EXAMPLES\\nOne undesirable consequence of models trained using ERM is their fragility to adversarial examples . Adversarial examples are obtained by adding tiny (visually imperceptible)\\nperturbations to legitimate examples in order to deteriorate the performance of the model. The adversarial noise is generated by ascending the gradient of the loss surface with respect to the legitimate\\nexample. Improving the robustness to adversarial examples is a topic of active research.\\nPublished as a conference paper at ICLR 2018\\nLabel corruption\\nTest error\\nTraining error\\nERM + dropout (p = 0.7)\\nmixup (\\u03b1 = 8)\\nmixup + dropout (\\u03b1 = 4, p = 0.1)\\nERM + dropout (p = 0.8)\\nmixup (\\u03b1 = 32)\\nmixup + dropout (\\u03b1 = 8, p = 0.3)\\nERM + dropout (p = 0.8)\\nmixup (\\u03b1 = 32)\\nmixup + dropout (\\u03b1 = 8, p = 0.3)\\nTable 2: Results on the corrupted label experiments for the best models.\\n(a) White box attacks.\\n(b) Black box attacks.\\nTable 3: Classi\\ufb01cation errors of ERM and mixup models when tested on adversarial examples.\\nAmong the several methods aiming to solve this problem, some have proposed to penalize the norm of\\nthe Jacobian of the model to control its Lipschitz constant . Other approaches perform data augmentation\\nby producing and training on adversarial examples . Unfortunately, all\\nof these methods add signi\\ufb01cant computational overhead to ERM. Here, we show that mixup can\\nsigni\\ufb01cantly improve the robustness of neural networks without hindering the speed of ERM by\\npenalizing the norm of the gradient of the loss w.r.t a given input along the most plausible directions\\n(e.g. the directions to other training points). Indeed, Figure 2 shows that mixup results in models\\nhaving a smaller loss and gradient norm between examples compared to vanilla ERM.\\nTo assess the robustness of mixup models to adversarial examples, we use three ResNet-101 models:\\ntwo of them trained using ERM on ImageNet-2012, and the third trained using mixup. In the \\ufb01rst\\nset of experiments, we study the robustness of one ERM model and the mixup model against white\\nbox attacks. That is, for each of the two models, we use the model itself to generate adversarial\\nexamples, either using the Fast Gradient Sign Method (FGSM) or the Iterative FGSM (I-FGSM)\\nmethods , allowing a maximum perturbation of \\u03f5 = 4 for every pixel. For\\nI-FGSM, we use 10 iterations with equal step size. In the second set of experiments, we evaluate\\nrobustness against black box attacks. That is, we use the \\ufb01rst ERM model to produce adversarial\\nexamples using FGSM and I-FGSM. Then, we test the robustness of the second ERM model and the\\nmixup model to these examples. The results of both settings are summarized in Table 3.\\nFor the FGSM white box attack, the mixup model is 2.7 times more robust than the ERM model in\\nterms of Top-1 error. For the FGSM black box attack, the mixup model is 1.25 times more robust\\nthan the ERM model in terms of Top-1 error. Also, while both mixup and ERM are not robust to\\nwhite box I-FGSM attacks, mixup is about 40% more robust than ERM in the black box I-FGSM\\nsetting. Overall, mixup produces neural networks that are signi\\ufb01cantly more robust than ERM against\\nadversarial examples in white box and black settings without additional overhead compared to ERM.\\nPublished as a conference paper at ICLR 2018\\nArrhythmia\\nTable 4: ERM and mixup classi\\ufb01cation errors on the UCI datasets.\\nmixup GAN (\\u03b1 = 0.2)\\nFigure 5: Effect of mixup on stabilizing GAN training at iterations 10, 100, 1000, 10000, and 20000.\\nTABULAR DATA\\nTo further explore the performance of mixup on non-image data, we performed a series of experiments\\non six arbitrary classi\\ufb01cation problems drawn from the UCI dataset . The neural\\nnetworks in this section are fully-connected, and have two hidden layers of 128 ReLU units. The\\nparameters of these neural networks are learned using Adam with default\\nhyper-parameters, over 10 epochs of mini-batches of size 16. Table 4 shows that mixup improves the\\naverage test error on four out of the six considered datasets, and never underperforms ERM.\\nSTABILIZATION OF GENERATIVE ADVERSARIAL NETWORKS (GANS)\\nGenerative Adversarial Networks, also known as GANs , are a powerful\\nfamily of implicit generative models. In GANs, a generator and a discriminator compete against\\neach other to model a distribution P. On the one hand, the generator g competes to transform noise\\nvectors z \\u223cQ into fake samples g(z) that resemble real samples x \\u223cP. On the other hand, the\\ndiscriminator competes to distinguish between real samples x and fake samples g(z). Mathematically,\\ntraining a GAN is equivalent to solving the optimization problem\\nx,z \\u2113(d(x), 1) + \\u2113(d(g(z)), 0),\\nwhere \\u2113is the binary cross entropy loss. Unfortunately, solving the previous min-max equation is a\\nnotoriously dif\\ufb01cult optimization problem , since the discriminator often provides\\nthe generator with vanishing gradients. We argue that mixup should stabilize GAN training because it\\nacts as a regularizer on the gradients of the discriminator, akin to the binary classi\\ufb01er in Figure 1b.\\nThen, the smoothness of the discriminator guarantees a stable source of gradient information to the\\ngenerator. The mixup formulation of GANs is:\\nx,z,\\u03bb \\u2113(d(\\u03bbx + (1 \\u2212\\u03bb)g(z)), \\u03bb).\\nFigure 5 illustrates the stabilizing effect of mixup the training of GAN (orange samples) when\\nmodeling two toy datasets (blue samples). The neural networks in these experiments are fullyconnected and have three hidden layers of 512 ReLU units. The generator network accepts twodimensional Gaussian noise vectors. The networks are trained for 20,000 mini-batches of size\\n128 using the Adam optimizer with default parameters, where the discriminator is trained for \\ufb01ve\\niterations before every generator iteration. The training of mixup GANs seems promisingly robust to\\nhyper-parameter and architectural choices.\\nPublished as a conference paper at ICLR 2018\\nSpeci\\ufb01cation\\nWeight decay\\nmix labels and latent\\nrepresentations\\nmix inputs only\\nSC + KNN \\nlabel smoothing\\n \\nmix inputs +\\nlabel smoothing\\nadd Gaussian noise\\nTable 5: Results of the ablation studies on the CIFAR-10 dataset. Reported are the median test errors\\nof the last 10 epochs. A tick (\\u0013) means the component is different from standard ERM training,\\nwhereas a cross (\\u0017) means it follows the standard training practice. AC: mix between all classes. SC:\\nmix within the same class. RP: mix between random pairs. KNN: mix between k-nearest neighbors\\n(k=200). Please refer to the text for details about the experiments and interpretations.\\nABLATION STUDIES\\nmixup is a data augmentation method that consists of only two parts: random convex combination of\\nraw inputs, and correspondingly, convex combination of one-hot label encodings. However, there are\\nseveral design choices to make. For example, on how to augment the inputs, we could have chosen\\nto interpolate the latent representations (i.e. feature maps) of a neural network, and we could have\\nchosen to interpolate only between the nearest neighbors, or only between inputs of the same class.\\nWhen the inputs to interpolate come from two different classes, we could have chosen to assign a\\nsingle label to the synthetic input, for example using the label of the input that weights more in the\\nconvex combination. To compare mixup with these alternative possibilities, we run a set of ablation\\nstudy experiments using the PreAct ResNet-18 architecture on the CIFAR-10 dataset.\\nSpeci\\ufb01cally, for each of the data augmentation methods, we test two weight decay settings (10\\u22124\\nwhich works well for mixup, and 5 \\u00d7 10\\u22124 which works well for ERM). All the other settings and\\nhyperparameters are the same as reported in Section 3.2.\\nTo compare interpolating raw inputs with interpolating latent representations, we test on random\\nconvex combination of the learned representations before each residual block (denoted Layer 1-4)\\nor before the uppermost \\u201caverage pooling + fully connected\\u201d layer (denoted Layer 5). To compare\\nmixing random pairs of inputs (RP) with mixing nearest neighbors (KNN), we \\ufb01rst compute the 200\\nnearest neighbors for each training sample, either from the same class (SC) or from all the classes\\n(AC). Then during training, for each sample in a minibatch, we replace the sample with a synthetic\\nsample by convex combination with a random draw from its nearest neighbors. To compare mixing\\nall the classes (AC) with mixing within the same class (SC), we convex combine a minibatch with a\\nPublished as a conference paper at ICLR 2018\\nrandom permutation of its sample index, where the permutation is done in a per-batch basis (AC) or a\\nper-class basis (SC). To compare mixing inputs and labels with mixing inputs only, we either use a\\nconvex combination of the two one-hot encodings as the target, or select the one-hot encoding of the\\ncloser training sample as the target. For label smoothing, we follow Szegedy et al. and use\\n10 as the target for incorrect classes, and 1 \\u22129\\u03f5\\n10 as the target for the correct class.Adding Gaussian\\nnoise to inputs is used as another baseline. We report the median test errors of the last 10 epochs.\\nResults are shown in Table 5.\\nFrom the ablation study experiments, we have the following observations. First, mixup is the best\\ndata augmentation method we test, and is signi\\ufb01cantly better than the second best method (mix input\\n+ label smoothing). Second, the effect of regularization can be seen by comparing the test error with a\\nsmall weight decay (10\\u22124) with a large one (5 \\u00d7 10\\u22124). For example, for ERM a large weight decay\\nworks better, whereas for mixup a small weight decay is preferred, con\\ufb01rming its regularization effects.\\nWe also see an increasing advantage of large weight decay when interpolating in higher layers of latent\\nrepresentations, indicating decreasing strength of regularization. Among all the input interpolation\\nmethods, mixing random pairs from all classes (AC + RP) has the strongest regularization effect.\\nLabel smoothing and adding Gaussian noise have a relatively small regularization effect. Finally,\\nwe note that the SMOTE algorithm does not lead to a noticeable gain in\\nperformance.\\nRELATED WORK\\nData augmentation lies at the heart of all successful applications of deep learning, ranging from image\\nclassi\\ufb01cation to speech recognition . In all cases, substantial domain knowledge is leveraged to design suitable data transformations\\nleading to improved generalization. In image classi\\ufb01cation, for example, one routinely uses rotation,\\ntranslation, cropping, resizing, \\ufb02ipping , and\\nrandom erasing to enforce visually plausible invariances in the model through\\nthe training data. Similarly, in speech recognition, noise injection is a prevalent practice to improve\\nthe robustness and accuracy of the trained models .\\nMore related to mixup, Chawla et al. propose to augment the rare class in an imbalanced\\ndataset by interpolating the nearest neighbors; DeVries & Taylor show that interpolation and\\nextrapolation the nearest neighbors of the same class in feature space can improve generalization.\\nHowever, their proposals only operate among the nearest neighbors within a certain class at the\\ninput / feature level, and hence does not account for changes in the corresponding labels. Recent\\napproaches have also proposed to regularize the output distribution of a neural network by label\\nsmoothing , or penalizing high-con\\ufb01dence softmax distributions . These methods bear similarities with mixup in the sense that supervision depends on multiple\\nsmooth labels, rather than on single hard labels as in traditional ERM. However, the label smoothing\\nin these works is applied or regularized independently from the associated feature values.\\nmixup enjoys several desirable aspects of previous data augmentation and regularization schemes\\nwithout suffering from their drawbacks. Like the method of DeVries & Taylor , it does not\\nrequire signi\\ufb01cant domain knowledge. Like label smoothing, the supervision of every example is not\\noverly dominated by the ground-truth label. Unlike both of these approaches, the mixup transformation\\nestablishes a linear relationship between data augmentation and the supervision signal. We believe\\nthat this leads to a strong regularizer that improves generalization as demonstrated by our experiments.\\nThe linearity constraint, through its effect on the derivatives of the function approximated, also relates\\nmixup to other methods such as Sobolev training of neural networks or\\nWGAN-GP .\\nDISCUSSION\\nWe have proposed mixup, a data-agnostic and straightforward data augmentation principle. We\\nhave shown that mixup is a form of vicinal risk minimization, which trains on virtual examples\\nconstructed as the linear interpolation of two random examples from the training set and their labels.\\nIncorporating mixup into existing training pipelines reduces to a few lines of code, and introduces\\nlittle or no computational overhead. Throughout an extensive evaluation, we have shown that mixup\\nPublished as a conference paper at ICLR 2018\\nimproves the generalization error of state-of-the-art models on ImageNet, CIFAR, speech, and\\ntabular datasets. Furthermore, mixup helps to combat memorization of corrupt labels, sensitivity to\\nadversarial examples, and instability in adversarial training.\\nIn our experiments, the following trend is consistent: with increasingly large \\u03b1, the training error on\\nreal data increases, while the generalization gap decreases. This sustains our hypothesis that mixup\\nimplicitly controls model complexity. However, we do not yet have a good theory for understanding\\nthe \\u2018sweet spot\\u2019 of this bias-variance trade-off. For example, in CIFAR-10 classi\\ufb01cation we can\\nget very low training error on real data even when \\u03b1 \\u2192\\u221e(i.e., training only on averages of pairs\\nof real examples), whereas in ImageNet classi\\ufb01cation, the training error on real data increases\\nsigni\\ufb01cantly with \\u03b1 \\u2192\\u221e. Based on our ImageNet and Google commands experiments with different\\nmodel architectures, we conjecture that increasing the model capacity would make training error less\\nsensitive to large \\u03b1, hence giving mixup a more signi\\ufb01cant advantage.\\nmixup also opens up several possibilities for further exploration. First, is it possible to make\\nsimilar ideas work on other types of supervised learning problems, such as regression and structured\\nprediction? While generalizing mixup to regression problems is straightforward, its application\\nto structured prediction problems such as image segmentation remains less obvious. Second, can\\nsimilar methods prove helpful beyond supervised learning? The interpolation principle seems like a\\nreasonable inductive bias which might also help in unsupervised, semi-supervised, and reinforcement\\nlearning. Can we extend mixup to feature-label extrapolation to guarantee a robust model behavior\\nfar away from the training data? Although our discussion of these directions is still speculative, we\\nare excited about the possibilities mixup opens up, and hope that our observations will prove useful\\nfor future development.\\nACKNOWLEDGEMENTS\\nWe would like to thank Priya Goyal, Yossi Adi and the PyTorch team. We also thank the Anonymous\\nReview 2 for proposing the mixup + dropout experiments.\",\n          \"Web-Scale Training for Face Identi\\ufb01cation\\nYaniv Taigman, Ming Yang, Marc\\u2019Aurelio Ranzato\\nFacebook AI Research\\nMenlo Park, CA 94025, USA\\n{yaniv, mingyang, ranzato}@fb.com\\nTel Aviv University\\nTel Aviv, Israel\\n \\nScaling machine learning methods to very large datasets\\nhas attracted considerable attention in recent years, thanks\\nto easy access to ubiquitous sensing and data from the\\nweb. We study face recognition and show that three distinct properties have surprising effects on the transferability of deep convolutional networks (CNN): (1) The bottleneck of the network serves as an important transfer learning regularizer, and (2) in contrast to the common wisdom,\\nperformance saturation may exist in CNN\\u2019s (as the number\\nof training samples grows); we propose a solution for alleviating this by replacing the naive random subsampling\\nof the training set with a bootstrapping process.\\nMoreover, (3) we \\ufb01nd a link between the representation norm\\nand the ability to discriminate in a target domain, which\\nsheds lights on how such networks represent faces. Based\\non these discoveries, we are able to improve face recognition accuracy on the widely used LFW benchmark, both\\nin the veri\\ufb01cation (1:1) and identi\\ufb01cation (1:N) protocols,\\nand directly compare, for the \\ufb01rst time, with the state of the\\nart Commercially-Off-The-Shelf system and show a sizable\\nleap in performance.\\n1. Introduction\\nFace identi\\ufb01cation is a recognition task of great practical interest for which (i) much larger labeled datasets exist, containing billions of images; (ii) the number of classes\\ncan reach tens of millions or more; and (iii) complex features are necessary in order to encode subtle differences\\nbetween subjects, while maintaining invariance to factors\\nsuch as pose, illumination, and aging.\\nPerformance improvements in recent years have been staggering, and automatic recognition systems cope much better today with\\nthe challenges of the \\ufb01eld than they did just a few years\\nago. These challenges are well documented and include\\nchanging illumination, pose and facial expression of the\\nsubject, occlusion, variability of facial features due to aging, and more. In the past, impressive performance was\\nonly demonstrated for carefully registered face images that\\ndid not exhibit much variability, a set up commonly referred\\nto as constrained conditions. Current state-of-the-art methods for unconstrained face recognition and veri\\ufb01cation (the\\ntask of predicting whether two face images belong to the\\nsame person or not) employ a similar protocol:\\nthey use a fairly large collection of images to learn a robust representation or metric, and then they perform transfer learning to predict the identity or the similarity between\\ntwo face images. These methods are trained on hundreds\\nof thousands or a few million images and recognize up to\\na few thousand different subjects. This is orders of magnitude larger than what was ever attempted in the past, yet\\ntwo or three orders of magnitude smaller than the actual\\ndatasets available today. Our starting point is the DeepFace\\narchitecture , which is based on recent advances in deep\\nlearning. Our contribution is not only to rede\\ufb01ne the state\\nof the art on a public benchmark using an improved system,\\nbut also: (i) we study the role of the bottleneck as a transfer learning regularizer and (ii) we propose a new way to\\nutilize large datasets by replacing the standard random sub\\nsampling procedure with a bootstrapping procedure; (iii) we\\ndiscover a three-way link between the representation norm,\\nthe image quality, and the classi\\ufb01cation con\\ufb01dence.\\nThese discoveries lead to an improvement in veri\\ufb01cation performance on a widely used benchmark dataset, the\\nLFW dataset . We then turn our attention to the 1:N\\nidenti\\ufb01cation problem, in which the image is identi\\ufb01ed out\\nof a gallery of N persons. While the usage of veri\\ufb01cation\\ndatasets has advanced the \\ufb01eld of computer vision greatly,\\nthe 1:N scenario is much more directly related to face identi\\ufb01cation applications. For instance, vendor tests, e.g., ,\\nfocus on 1:N protocols. As a result, the research community was unable to directly compare academic contributions\\nto the most prominent commercial systems. By making use\\nof a recently proposed 1:N benchmark built on top of the\\nLFW images , we are able to perform this comparison\\nfor the \\ufb01rst time.\\n \\n2. Previous work\\nThe U.S. National Institute of Standards and Technology (NIST) publicly reports every several years its internal benchmark on face recognition; systems taking part in\\nthe competition are developed by leading commercial vendors as well as a few research labs. For instance, in the\\nMBE 2010 report , the three top-ranked Commercial Off\\nThe Shelf (COTS) correctly matched probed faces against a\\nlarge collection (\\u201cgallery\\u201d in the commonly used terminology) of 1.6 million identities with an 82%-92% accuracy\\nrate. The datasets used by are not publicly available,\\nand therefore it is hard to compare the performance of academic systems on the same benchmarks. Fortunately, the\\nsame COTS system was recently tested on a 1:N identi\\ufb01cation benchmark constructed using the images of the\\npublic Labeled Faces in the Wild (LFW) dataset. On\\nthis benchmark, the rank-1 accuracy of the COTS system\\ndropped to about 56% , even though the gallery has only\\na couple of thousands identities. This \\ufb01nding demonstrates\\nthat although constrained face recognition has reached an\\nimpressive accuracy, the unconstrained one is still far from\\nbeing solved.\\nFace veri\\ufb01cation, which is the task of determining\\nwhether two face images belong to the same subject, has\\ngreatly advanced in recent years, especially in the unconstrained setting. In fact, recent contributions \\nreported nearly human level performance on the LFW veri\\ufb01cation task using deep neural networks, but no test is reported on the probe-gallery identi\\ufb01cation task.\\nScaling up face recognition is a non-trivial challenge.\\nThe baseline DeepFace system has about 100 million\\nparameters to start with. It is very hard to distribute ef\\ufb01ciently . It produced features that are lower dimensional than engineered features but still contain several\\nthousand dimensions; and it needs massive amounts of data\\nto generalize well . There is no known method to\\neffectively train such a large system on billions of images\\nwith millions of labels, using thousands of features. In the\\nmachine learning literature, several methods have been proposed to deal with very large datasets. The simplest method\\nis to randomly down-sample the dataset, which is a clearly\\nsub-optimal approach.\\nA more suitable alternative is to\\nemploy bootstrapping procedures that aim at focusing on\\nthe hardest cases ignoring or down-weighing the easy ones,\\nlike in boosting . The approach we advocate for is, in\\nessence, a bootstrapping one since we focus the training effort on a cleverly selected subset of the samples that are\\nhard to classify. However, the selection process is made\\nmuch more ef\\ufb01cient than in standard bootstrapping because\\nwe do not need to \\ufb01rst evaluate each training sample in order to perform our selection.\\nAs part of our sample selection process, we utilize the similarity between classes based on the parameters of their clas-\\nFigure 1. The initial baseline network architecture. A front end of\\nconvolutional, pooling, convolutional layers is followed by three locally\\nconnected layers and two fully connected layers.\\nsi\\ufb01ers. Similarity of classi\\ufb01ers has been used in the literature in other contexts. In multiple SVM classi\\ufb01ers,\\neach based on a single positive sample, are used to construct\\na powerful descriptor of the learned class. Note that in our\\ncase, the \\ufb01nal descriptor is trained in one multi-class classi\\ufb01cation network, whereas SVMs are only used to select\\nlabels for training this network.\\nOur contribution continues a line of work that has gained\\nconsiderable recent attention \\u2013 understanding the underlying mechanisms behind the \\u201cunreasonable\\u201d success of deep\\nnetworks. In the deep activations are propagated back\\nto the image in order to give insights into the role of the\\nintermediate layers and the operation of the classi\\ufb01cation\\nlayer. In , optimization is used to trick the CNN to misclassify clear input images and to compute the stability of\\neach layer. Recently, studied empirically the tradeoff\\nbetween the generality and speci\\ufb01city of each layer.\\nThese contributions mostly focus on image classi\\ufb01cation\\ntrained on ImageNet, a highly varied dataset with a few million images and 1,000 classes. Faces, since they have a clear\\nstructure, training data in abundance, and well understood\\nchallenges, provide a unique opportunity for understanding\\nthe basic properties of CNN-based transfer learning.\\n3. Transfer Learning in Faces\\nIn this section, we describe our framework, starting with\\nan initial face representation, which was trained similarly to\\nDeepFace , and exploit discoveries in this network to\\nscale the training up to, the previously unexplored range of,\\nhundreds of millions of training images.\\nDeepFace was shown to achieve good generalization.\\nHowever, the association between the transferability of the\\nnetwork to its design was left largely unexplored. We report three properties that strongly affect the quality of the\\ntransfer. First, we show that the dimensionality of the representation layer (F7), which we will refer to as bottleneck,\\ndramatically affects transferability. Second, we \\ufb01nd that selecting random samples from a very large pool of samples\\nleads to performance saturation, which can be alleviated by\\nreplacing the naive random subsampling practice with a semantic bootstrapping method. Third, we link measurable\\nproperties of the transferred representations to the expected\\nperformance at the target domain.\\nFigure 2. The bottleneck.\\nThe representation layer splits the network\\nbetween the part that converts the input into a generic face descriptor and\\nthe part that performs linear classi\\ufb01cation to speci\\ufb01c K classes. FC7 and\\nFC8 are the low-rank matrices that project to- and from the bottleneck.\\n3.1. Baseline DeepFace Representation\\nA total of four million face images belonging to 4,030\\nanonymized identities (classes), were aligned with a 3D\\nmodel and used for learning an initial face representation,\\nbased on a deep convolutional neural network. As shown\\nin Fig. 1, the network consists of a front-end two convolutional layers with a single max-pooling layer in between (C1-M2-C3), followed by three locally-connected\\nlayers L4-L5-L6, without weight sharing, and two fullyconnected layers F7-F8.\\nThe output of F8 is fed to a\\n4030-way softmax which produces a distribution over the\\nclass labels. Denote by oi(x) the i-th output of the network on a given input x, the probability assigned to the\\ni-th class is the output of the softmax function: pi(x) =\\nexp(oi(x))/ P\\nj exp(oj(x)). The ReLU(a) = max(0, a)\\nnonlinearity is applied after every layer (except for F8)\\nand optimization is done through stochastic gradient descent and standard back-propagation , minimizing\\nthe cross-entropy loss. If k is the index of the true label\\nfor a given input x, the loss associated with this sample is:\\nL(x) = \\u2212log pk(x). Once trained, the representation used\\nis the normalized feature vector of layer F7 .\\n3.2. Bottleneck and Transferability\\nWe show that the dimensionality of the last fullyconnected layers (F7 & F8) critically affects the balance between generality and speci\\ufb01city. As illustrated in Fig 2, in\\na K-way multiclass network with binary targets, the classi\\ufb01cation layer (F8) is a collection of K linear (dependent)\\nclassi\\ufb01ers.\\nBy compressing the preceding representation layer (F7)\\nthrough a lower rank weight matrix, we reduce the ability of the network to encode training-set speci\\ufb01c information in this layer, thereby shifting much of the speci\\ufb01city\\nto the subsequent classi\\ufb01cation layer (F8). For the purpose\\nof transfer learning, the classi\\ufb01cation layer is ignored once\\ntraining \\ufb01nishes, and the network is regarded as a feature\\nextractor. A compact bottleneck, therefore, decreases the\\nnetwork specialization and increases the representation generality. However, a narrower bottleneck increases the dif\\ufb01culty of optimizing the network when training from scratch.\\nFor the architecture described in Sec. 3.1, we are able to\\neffectively train with bottlenecks of dimensionality as low\\nas 1024, but not lower. For smaller dimensions the training error stopped decreasing early on. However, we note\\na useful property: by pre-loading the weights of the initial\\nnetwork, except for the last layer, we were able to learn\\nmuch smaller embeddings effectively, as furthered detailed\\nin Sec. 5. Remarkably, a bottleneck of only 256 dimensions\\nyielded convincingly better accuracies on the target domain\\nin all con\\ufb01gurations.\\nThis might be counter intuitive, since the common wisdom is that the representation should be as large as the\\nnumber of samples can support.\\nHowever, as we show,\\ndecreasing the representation size is bene\\ufb01cial even if the\\nnumber of training samples is virtually unlimited. The reason is that we learn our representation in one domain and\\ntest it on another, which does not share the same underlying distribution. Therefore, regularization is warranted. In\\naddition to generalization, such compact representations enable us, ef\\ufb01ciency wise, to scale up the experiments by an\\norder of magnitude, exploring much larger con\\ufb01gurations\\nas we describe next.\\n3.3. Semantic Bootstrapping\\nA conjecture is made in that \\u201cresults can be improved simply by waiting for faster GPUs and bigger\\ndatasets to become available\\u201d. Our \\ufb01ndings reveal that this\\nholds only to a certain degree. Unlike the popular ImageNet\\nchallenge, where a closed collection of 1.6 million images\\nis split into train and test, we can access a much larger\\ndataset in order to test, for the \\ufb01rst time, both the transferability and scalability properties of deep convolutional\\nnets at scales three orders of magnitude larger. We \\ufb01nd that\\nusing the standard Stochastic Gradient Descent (SGD) and\\nback-propagation leads to performance saturation in the target domain when the training set size in the source domain\\ngrows beyond a certain point, as shown in Sec. 5. This holds\\neven when we change the architecture of network, by either\\nadding more layers and/or increasing their capacity. By judiciously selecting samples as opposed to picking them at\\nrandom, we were able to improve performance further.\\nWe leverage the compressed representation presented\\nabove in order to train ef\\ufb01ciently. The dataset at our disposal contains 10 million anonymized subjects with 50 images each in average. This is to be compared to around\\n4000 identities and 4 million images (DB1) in . This\\nnew dataset was randomly sampled from a social network.\\nBy running string matching, we veri\\ufb01ed that the identities\\ndo not intersect those of LFW.\\nHaving this larger dataset at our disposal, we search for\\nimpostor samples to be used in a second round of training,\\nas normally done in bootstrapping. The most basic method\\nwould be to sample a large pool of face representations,\\neach represented by the compact feature, and select the\\nnearest neighbors from other identities (\\u201cimpostors\\u201d). However, for reasons speci\\ufb01ed below, a signi\\ufb01cant improvement\\nin both scalability and reliability is obtained by working in\\nthe space of linear models, trained discriminatively on top\\nof the compressed representations, as opposed to directly\\nworking with the compressed features.\\nAs a \\ufb01rst step, we represent each class by a single classi-\\n\\ufb01er, i.e., for each identity, we learn a hyperplane trained in\\na binary classi\\ufb01cation setting of one-vs-all, where the positive instances (representations) are of the same identity and\\nthe negatives are a random subset of other identities. Each\\nidentity is associated with an average of 50 face images.\\nTherefore, working with linear models, instead of the underlying instances, enables us to scale the exploration of\\nimpostor identities by another order of magnitude. In terms\\nof ef\\ufb01ciency, training such linear models can be easily parallelized, and it is very ef\\ufb01cient especially when using compact features.\\nIn addition to being more scalable, this semantic distance\\nperforms better than instance-based bootstrapping, probably due to its added robustness to human labeling errors of\\nthe ground-truth. When sampling pairs of same/not same\\nsamples based on instance similarity, we notice that many of\\nthe pairs sampled, in particular those that are labeled as the\\n\\u2019same\\u2019 identity, are of different individuals due to labeling\\nerrors. Performing, for example, metric learning on such\\ndata leads to reduced performance compared to the baseline representation. Speci\\ufb01cally, a Siamese network trained\\non 4M nearest neighbor pair instances, belonging either to\\nthe same class or not (=impostors), obtained substantially\\nworse results than the baseline system. This observation is\\nalso consistent with the fact that the initial representation\\nis already on-par with humans w.r.t. pair-wise veri\\ufb01cation\\nperformance, thereby \\ufb02ushing out the ground-truth errors.\\nBootstrapping is performed as follows: we randomly select 100 identities, as seeds, among the 10 million models.\\nFor each seed, we search for the 1000 nearest models, where\\nthe similarity between any two models h1, h2 is de\\ufb01ned as\\nthe cosine of the angle between the associated hyperplanes:\\nS(h1, h2) =< h1, h2 > /(\\u2225h1\\u2225\\u2225h2\\u2225). The union of all\\nimages of all retrieved identities constitutes the new bootstrapped dataset DB2, containing 55,000 identities overall.\\nNote that in contrast to negatives sampling , as normally\\ndone in bootstrapping, DB2 consists of both easy & hard\\nsamples - separating between seeds is as easy as before, but\\nmuch harder inside the neighborhood of each seed, by construction.\\nIn terms of ef\\ufb01ciency, the training of 107 hyperplanes\\nFigure 3. The bootstrapping method. An initial 256D-compressed representation trained on DB1 is used to \\ufb01nd the semantically-nearest identities of randomly picked 100 seeds, in a large pool of pre-trained hyperplanes. The union of all 100 groups of selected identities de\\ufb01ne the bootstrapped dataset DB2. A larger capacity network with enlarged locallyconnected layers and a 1024D representation is then trained.\\nwas completed in 2 days utilizing several 256G RAM commodity servers. Evaluating the distance between each seed\\nand a gallery pool of these 107 hyperplanes reduces to a\\nmatrix-multiplication Wsi where W is a matrix of 107 \\u00d7\\n256 and seed si \\u2208R256 is a single seed. The run time of\\nthis step on a single server is about 1 second per seed query.\\nOverall, the entire subset selection process takes an hour on\\na single high memory server.\\n3.4. Final Network Architecture\\nDB2 is a challenging dataset, and our objective is to\\ntrain feature representation that can discriminate between\\nthe new selected identities. Increasing the dimensionality\\nof the representation and consequently training a bigger network is, therefore, essential in order to model the subtle differences between lookalike subjects found more abundantly\\nin the bootstrapped training set (see Sec 5).\\nSpeci\\ufb01cally, we pre-load C1 and C2 layers from the\\ninitial network, and double the number of \\ufb01lters of each\\nlocally-connected layer from 16 to 32. In addition, we enlarge the representation layer F7 from 256 to 1024. All new\\nlayers, except for C1 and C2, are randomly initialized and\\ntrained on DB2, with the same algorithm as before. The\\ntwo \\ufb01rst convolutional layers C1 and C2 are merely feature extractors, which include less than 1% of the overall\\nweights. Empirically, we found that \\ufb01xing these layers did\\nnot affect performance while speeding up training considerably. Unlike boosting and cascaded architectures , we\\nend up with a single classi\\ufb01er and are interested in learning\\nrepresentations that are useful for transfer learning. Figure\\n3 visualizes the process.\\n4. The Representation Norm\\nOne of the biggest bene\\ufb01ts of DNNs is their ability to\\nlearn representations that can generalize across datasets and\\ntasks . Faces, since they have a clear structure, training\\ndata in abundance, and well understood challenges, provide\\nFigure 4. Examples of L6 activations for various faces. In each pair\\nthe original image is compared to the sum of the channels of its\\nL6 activations: (left) pairs depicting good quality images. (right)\\nexamples of poor quality images (occluded and/or misaligned).\\nBluer is lower, red is higher. Best viewed in color.\\na unique opportunity to discover basic properties of this increasingly popular form of transfer learning.\\nThe input faces are aligned, and since the spatial structure of our deep architecture is preserved until the representation layer, the feature maps remain a well localized\\ndescription of the underlying face. Inspecting the topmost\\nlocal layer (L6) provides easy to decipher information regarding the underlying image. Consider Fig. 4 that shows\\nthe original image and its L6 (summing across all feature\\nmaps) side by side. Occlusions and other types of local distortions in the input face image lead to weaker activations\\nin the corresponding areas of L6.\\nA pixel of the input image contributes to a value in a\\ndeep layer only if there is a path, from this location in the\\ninput layer to the deep layer, for which all activations pass\\nthe ReLU thresholds. Paths originating in disrupted regions\\ntend to get blocked, leading to darkened matching regions\\nin the top local layers.\\nIn fact, the information of which unit is activated in the\\nrepresentation holds most of the discriminative information.\\nWhen applying a simple threshold at zero to the image representation (F7), the resulting binary vector remains highly\\ndiscriminative. On the LFW benchmark, the performance\\ndrop that follows the binarization of the representation is\\ntypically only 1% or less.\\nSince image disruptions lead to localized L6 inactivity,\\nand since F7 is a linear projection of L6 followed by a\\nthreshold, these disruptions lead to a reduced norm of the\\nrepresentation vector F7. This can be seen in Fig. 5(a), in\\nwhich the link between the norm of a pair of face representations is compared to the certainty of the classi\\ufb01er mapping\\npairs of images to the same/not-same identity. Curiously\\nenough, while the representation in a ReLU-network is directly linked to the norm of the representation, the link between the image mean intensity value and its representation\\nnorm is much weaker, as can be seen, for all LFW images\\nin Fig. 5(b).\\nA more direct way to explore the link between the representation norm and the image discriminativity is to consider\\nthe entropy of the classi\\ufb01cation layer, i.e., the outputs of\\nthe softmax layer. This is shown in Fig. 5(c), where for all\\nLFW images the representation norm is plotted against the\\nentropy of the probabilities obtained by the original deepface network, trained on the 50,000 identities of DB2. This\\nlink is very strong and a correlation below -0.65 is obtained\\nbetween the norm and the entropy.\\nIn order to explain the link between the representation\\nnorm and the entropy, we use the \\ufb01rst order Taylor expansion of the classi\\ufb01er output. Let r be the vector of activations of F7. Let z be the vector of F8 activations obtained\\nfor r, then, z = Wr, where W contains the weights mapping F7 to F8. The softmax function maps z to probabilities:\\nj zj , where the last approximation\\nholds for small zi, since ex \\u22481 + x for small x.\\nThe approximation of the entropy is therefore H(p) \\u2248\\nj zj . Since the softmax vector of\\nz is the same as the softmax vector of z + b for all b, we\\nassume, w.l.o.g, that the mean of z is zero and obtain that\\nN . By the approximation above,\\nlog(1 + zi) \\u2248zi and we obtain:\\n(zi \\u2212log(N)) .\\nWe now consider a family of scaled version of r: sr,\\nwhere s is a scale factor. Since z = Wr, the activations of\\nF8 also scale with s, and the entropy approximation of the\\nscaled activations become \\u2212P\\n(szi \\u2212log(N)). This\\nexpression is dominated, for small values of s, by a linear\\nfunction of s, which explains the behavior seen on the left\\nside of Fig. 5(c).\\nTo conclude, lower representation norms are negatively\\nassociated with prediction con\\ufb01dence. In the region of low\\nnorms, there is a linear relation between the norm and the\\nprediction entropy, and this can be further used also to reject\\nsamples at classi\\ufb01cation time.\\n5. Experiments\\nWe \\ufb01rst evaluate the learned representations on cropped1\\nfaces of the Labeled Faces in the Wild (LFW) public\\ndataset , using multiple protocols. We also validate our\\n\\ufb01ndings on an internal dataset, probing 100K faces among\\n10K subjects with a probe-gallery identi\\ufb01cation protocol.\\nThe LFW dataset consists of 13,233 web photos of 5,749\\ncelebrities, and is commonly used for benchmarking face\\n1Using the biased background to improve performance is not\\nin the scope of this work.\\nFigure 5. Connecting the representation norm to image distinguishability. (a) The classi\\ufb01er score (signed distance from the separating\\nhyperplane) for same not-same prediction on the LFW benchmark vs. min(||rl||, ||rr||), where rl and rr are the representations of the\\nimage pair. Red denotes misclassi\\ufb01cation. As can be seen, lower con\\ufb01dence predictions and mistakes tend to have lower representation\\nnorms. (b) The mean intensity value of the face region vs. the representation norm. High intensity images are not typically linked\\nwith higher representation norms. (c) Representation norm vs. prediction entropy. The higher the representation norm is, the lower the\\nuncertainty is expected to be. (d) Retrieval rank vs. mean representation norm on our internal validation set (Sec. 5.4). Misclassi\\ufb01ed probes\\n(rank>1) tend to have a lower norm than correctly matched probes (rank=1).\\nveri\\ufb01cation.\\nIn this work, we focus on two new Probe-\\nGallery unsupervised protocols proposed in (the original splits are used):\\n1. A closed set identi\\ufb01cation task, where the gallery set\\nincludes 4,249 identities, each with only a single example, and the probe set includes 3,143 faces belonging to the same set of identities. The performance is\\nmeasured by the Rank-1 identi\\ufb01cation accuracy.\\n2. An open set identi\\ufb01cation task, where not all probe\\nfaces have a true mate in the gallery. The gallery includes 596 identities, each with a single example, and\\nthe probe set includes 596 genuine probes and 9,491\\nimpostor ones. Here the performance is measured by\\nthe Rank-1 Detection and Identi\\ufb01cation Rate (DIR),\\nwhich is the fraction of genuine probes matched correctly in Rank-1 at a 1% False Alarm Rate (FAR) of\\nimpostor probes that are not rejected.\\nAs the veri\\ufb01cation protocol, we follow the LFW unrestricted protocol (which uses only the same-not-same\\nlabels), and similarly to train a kernel SVM (with C=1)\\non top of the \\u03c72-distance vectors derived from the computed\\nrepresentations. For the open and closed set identi\\ufb01cation\\nexperiments, we simply use the cosine similarity (normalized dot product). A critical difference between the LFW\\nveri\\ufb01cation protocols and the Probe-Gallery ones is that the\\nlatter does not permit training on the LFW dataset. They,\\ntherefore, demonstrate how face recognition algorithms perform on an unseen data distribution, as close as possible to\\nreal-life applications. Also, as pointed out in , the Probe-\\nGallery protocols correspond to many challenging practical scenarios, such as retrieval .\\nThe importance of\\nsuch protocols as tools that differentiate face recognition\\nmethods based on performance is con\\ufb01rmed by our results,\\nsince methods that exhibit very similar results on the LFW\\nveri\\ufb01cation protocol display large performance gaps on the\\nProbe-Gallery ones, as shown in Table 1. In all of our experiments we follow the unrestricted protocol using labeled\\noutside data . We reiterate the importance of having a\\nlarge scale training set in hand: training the initial net \\nusing 500K images on CASIA , the largest public face\\ndataset available today, obtains veri\\ufb01cation accuracy of only\\n93.5% on LFW, largely due to over-\\ufb01tting. The code for\\ntraining the nets is written in Torch and uses the fbcunn extensions (see: ).\\n5.1. Compressed Representations\\nWe \\ufb01rst evaluate different compressed representations,\\nall utilizing the initial face representation system, with sizes\\nranging from 4096 dimensions down to 8. These networks\\nwere retrained as described in Sec. 3.2 on the 4 million images associated with 4,030 random identities used in .\\nTable 1 shows that compression improves generalization considerably. With only 256 dimensions, the obtained\\nRank-1 accuracy stands on 72.3% on the Closed Set protocol, and DIR 46.3% at 1% FAR on the Open Set, and greatly\\noutperforms the original 4096D representation. Note, however, that the difference in the veri\\ufb01cation protocol remains\\nwithin a 1% range when compressing the 4096 dimensions\\ndown to 64, and either due to performance saturation and/or\\nthe type of benchmark, differences in face recognition capabilities are not captured well.\\n5.2. Bootstrapped Representations\\nWe now compare the representation learned on the 55K\\nbootstrapped identities (4.5M faces) with those learned\\nfrom randomly selected 108K identities (3.2M faces) and\\neven 250K identities (7.5M faces), as shown in Table 2.\\nWe note that: (i) The deep neural network (DNN) can bene\\ufb01t from additional amount of training data, e.g., 250K\\nidentities, boost the recognition performance over those\\ntrained on 4K identities in Table 1. (ii) The bootstrapped\\nFigure 6. Left: The ROC curves on the face veri\\ufb01cation unrestricted protocol. Right: The DIR vs. FAR curves on the Open Set protocol.\\nAs mentioned above, for identi\\ufb01cation, training on LFW images is not permitted as it invalidates the comparison to baselines; had we\\njointly \\ufb01t a multi-class linear SVM to the gallery, the best model would achieve 69% DIR @ 1% on the open set. Best viewed in color. The\\nordinate scales are different. COTS graphs were reconstructed from .\\n4096 1024 512\\n97.00 96.72 96.78 97.17 96.42 96.10 94.50 92.75 89.42\\n60.9 64.9 67.4 72.3 69.1 66.5 39.6 23.2 7.00\\n78.7 83.9 85.2 90.4 88.8 87.7 70.8 52.9 24.7\\nDIR@1% 41.9 44.7 46.1 46.3 44.1 36.7 12.2 5.37 0.33\\nTable 1. Performance on the three protocols, when varying the dimensionality of the representations. Performance is measured in\\nterms of the veri\\ufb01cation accuracy (%) of the unrestricted veri\\ufb01cation protocol, Rank-1 and Rank-10 accuracy (%) on the Closed\\nSet, and the DIR (%) at 1% FAR on the Open Set.\\ntraining set of 55K identities, although \\ufb01ve times smaller\\nthan the biggest training set used, delivers better Probe-\\nGallery performance. (iii) An even larger improvement is\\nobtained when the locally-connected layers (L4-L5-L6) are\\nexpanded as described in Sec. 3.4, and the extended 256D\\nand 1024D representations (denoted as 256+ and 1024+)\\ngeneralize better than their unmodi\\ufb01ed counterparts. Larger\\nnetworks were also attempted but failed to improve performance, e.g. 2048+ reduced Rank-1 accuracy by 4.21% on\\nthe closed set.\\n5.3. Comparison with the State-of-the-art\\nThe state of the art COTS face recognition system,\\nas evaluated by NIST in , and diligently benchmarked\\nby on the LFW open and closed set protocols provides a unique insight as to how well our system compares to the best commercial system available. The authors\\nof have also employed an additional vendor that recti\\ufb01es non-frontal images by employing 3D face modeling\\nand improved the results of the baseline COTS-s1 system,\\nthe combined system is denoted COTS-s1+s4. For further\\ncomparison, we have evaluated the publicly available LFW\\nRandom 108K\\nRandom 250K\\nBootstrap 55K\\nTable 2. Performance of the three protocols on different training\\nsets. The three rightmost columns report results using the architecture discussed in Sec. 3.4.\\nhigh dimensional LBP features of , denoted as BLS,\\nwhich are published online in their raw format, i.e. before\\napplying the prescribed supervised metric learning method.\\nFinally, in order to push our results further, we fuse four of\\nthe networks trained in our experiments (namely, the initial,\\nRandom-108K, Random-250K and 1024+ Bootstrap-55K,\\nall 1024d) by simply concatenating their features, and report a slight improvement, denoted as Fusion. Note that\\nfor the LFW veri\\ufb01cation benchmark only, reports an\\naccuracy of 99.15%. In contrast to our work, this result\\nemploys hundreds of CNNs fused with background information from the LFW images. Our network is limited to\\nTable 3. Comparison to state of the art that includes the COTS\\nmethod and two recent methods, in terms of the Probe-Gallery\\u2019s\\nRank-1 accuracy (%) on the Closed Set, the DIR at 1% FAR on the\\nOpen Set, as well as the veri\\ufb01cation protocol. \\u2217For only the\\npublished raw features are used and not the full system. The full\\nsystem achieves 95.17% on the veri\\ufb01cation task. For both COTS\\nthe veri\\ufb01cation performance was not reported.\\nFigure 7. Examples of successful (left) and failed (right) probed\\nidenti\\ufb01cations on our validation set. A successful match is when a\\nprobe achieves the highest similarity to its true-mate in the gallery\\n(Rank-1), failure otherwise. Permission granted by the subjects.\\nthe face region (\\u2018cropped\\u2019) which helps remove important\\ndataset-speci\\ufb01c biases and even with a single network\\nwe achieve 98.0% veri\\ufb01cation performance. For instance, a\\nsingle network of obtains up to 95.43%. Table 3 and\\nFigure 6 summarize these results. Our best method lowers the state of the art miss rate on the closed set protocol\\nby 57%, and by 45%, at the same precision level, on the\\nopen set protocol. The error in the veri\\ufb01cation protocol is\\nreduced by 38% with respect to the initial baseline system.\\n5.4. Model Selection\\nSince the LFW veri\\ufb01cation protocol does not allow\\nmodel selection of any kind, we selected the best model using a separate internal validation dataset, that consists of\\n10,000 individuals. As gallery we take 55,000 images (an\\naverage of 5.5 face images, per identity). At test time, we\\nperform 10 queries per person, using an additional set of\\n100,000 images. We search the probe faces with the cosine\\nsimilarity using the 4096 dimensional initial representation\\nin , the compressed 256D and the bootstrapped 1024+\\nrepresentations. Performance is measured by the Rank-1\\naccuracy and the DIR at 1% and 0.1% FAR. The results are\\nlisted in Table 4, and con\\ufb01rm that (i) the compressed 256D\\nrepresentation generalizes better than the 4096D one; and\\n(ii) the bootstrapped 1024+ improves the DIR substantially\\nfor the low 0.1% FAR. A few examples of successful and\\nfailed probed faces are shown in Fig. 7.\\nRepresentation norm in retrieval.\\nWe have also veri\\ufb01ed\\non this 10, 000 identity validation set that the representation\\nnorm is linked to the identi\\ufb01cation accuracy. As shown in\\nDIR @ 0.1%\\nTable 4. Probe-Gallery results on an internal dataset used for\\nmodel selection.\\nFig. 5(d), there\\u2019s a clear correlation between the retrieval\\nsuccess, measured in terms of the minimum rank of one of\\nthe true gallery images in the retrieval list, and the representation norm. The correlation is extremely strong (\\u03c1 =\\n-0.251), and there is a sizable gap between the mean norm\\nof the successful rank-1 queries and the unsuccessful ones.\\n6. Summary\\nFace recognition is unlike any other recognition task in\\nseveral ways.\\nFirst, although it is only one object, it is\\nby far the most frequent entity in the media, and there are\\nbillions of unique instances (identities) to differentiate between. Second, since the universe of faces is open in practice, the most interesting problems are Transfer Learning\\ntasks, where it is required to learn how to represent faces in\\ngeneral. This representation is then tested on unseen identities. This is in contrast to training on closed tasks such as\\nImageNet\\u2019s Image Classi\\ufb01cation challenge where recognition performance is optimized directly on a \\ufb01xed set of 1000\\nclasses. Accurate face alignment enables us to concentrate\\nsolely on the underlying inter-personal variances that exist\\nin face recognition using deep convolutional nets. Recent\\nworks in the domain of face recognition have yielded impressive results by utilizing large and deep convolutional\\nnets. However, there is no clear understanding of why they\\nperform so well and what the important factors in such systems are. We explore the task of transferring representation\\nof faces in a number ways. First, we identify and explain\\nthe role of the network\\u2019s bottleneck as an important regularizer between training-set speci\\ufb01city and generality. Second,\\nwe have identi\\ufb01ed a saturation point in performance, as the\\nnumber of training samples grows beyond what has been\\nexplored in the past, and provided an ef\\ufb01cient method that\\nalleviates this by modifying the common practice of randomly subsampling the training set. Third, we show how\\nthe representation layer of faces is constructed and affected\\nby distortions, linking between its reduced norm to the undesirable increase in uncertainty. Lastly, our work is unique\\nin that it allows a direct comparison of commercial systems\\nto those published in the academic literature, on the benchmark task used in practice to compare actual commercial\\nsystems. We conjecture that the impact of these discoveries goes beyond face recognition, and is applicable to other\\nlarge scale learning tasks.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"referenced_paper_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"p0160\",\n          \"p0808\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"is_referenced\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPXWydNChfZ1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "f34bf74d-7a3b-4584-d1da-62f55d665789"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    paper paper_id  \\\n",
              "237922  Pyramid Vision Transformer: A Versatile Backbo...    p3178   \n",
              "\n",
              "                                         referenced_paper referenced_paper_id  \n",
              "237922  Squeeze-and-Excitation Networks\\nJie Hu[0000−0...               p1129  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2b963110-c2e9-49e1-8b71-40e9ba012dd5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper</th>\n",
              "      <th>paper_id</th>\n",
              "      <th>referenced_paper</th>\n",
              "      <th>referenced_paper_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>237922</th>\n",
              "      <td>Pyramid Vision Transformer: A Versatile Backbo...</td>\n",
              "      <td>p3178</td>\n",
              "      <td>Squeeze-and-Excitation Networks\\nJie Hu[0000−0...</td>\n",
              "      <td>p1129</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2b963110-c2e9-49e1-8b71-40e9ba012dd5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2b963110-c2e9-49e1-8b71-40e9ba012dd5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2b963110-c2e9-49e1-8b71-40e9ba012dd5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_cln",
              "summary": "{\n  \"name\": \"df_cln\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"paper\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction\\nwithout Convolutions\\nWenhai Wang1, Enze Xie2, Xiang Li3, Deng-Ping Fan4B,\\nKaitao Song3, Ding Liang5, Tong Lu1B, Ping Luo2, Ling Shao4\\n1Nanjing University\\n2The University of Hong Kong\\n3Nanjing University of Science and Technology\\n5SenseTime Research\\n \\n(a) CNNs: VGG , ResNet , etc.\\nTransformer\\n(b) Vision Transformer \\nTransformer\\n(c) Pyramid Vision Transformer (ours)\\nFigure 1: Comparisons of different architectures, where \\u201cConv\\u201d and \\u201cTF-E\\u201d stand for \\u201cconvolution\\u201d and \\u201cTransformer\\nencoder\\u201d, respectively. (a) Many CNN backbones use a pyramid structure for dense prediction tasks such as object detection\\n(DET), instance and semantic segmentation (SEG). (b) The recently proposed Vision Transformer (ViT) is a \\u201ccolumnar\\u201d\\nstructure speci\\ufb01cally designed for image classi\\ufb01cation (CLS). (c) By incorporating the pyramid structure from CNNs, we\\npresent the Pyramid Vision Transformer (PVT), which can be used as a versatile backbone for many computer vision tasks,\\nbroadening the scope and impact of ViT. Moreover, our experiments also show that PVT can easily be combined with\\nDETR to build an end-to-end object detection system without convolutions.\\nAlthough convolutional neural networks (CNNs) have\\nachieved great success in computer vision, this work investigates a simpler, convolution-free backbone network useful for many dense prediction tasks. Unlike the recentlyproposed Vision Transformer (ViT) that was designed for\\nimage classi\\ufb01cation speci\\ufb01cally, we introduce the Pyramid Vision Transformer (PVT), which overcomes the dif\\ufb01culties of porting Transformer to various dense prediction\\ntasks. PVT has several merits compared to current state\\nof the arts. (1) Different from ViT that typically yields lowresolution outputs and incurs high computational and memory costs, PVT not only can be trained on dense partitions\\nof an image to achieve high output resolution, which is important for dense prediction, but also uses a progressive\\nshrinking pyramid to reduce the computations of large feature maps. (2) PVT inherits the advantages of both CNN\\nand Transformer, making it a uni\\ufb01ed backbone for vari-\\nB Corresponding authors: Deng-Ping Fan ( );\\nTong Lu ( ).\\nous vision tasks without convolutions, where it can be used\\nas a direct replacement for CNN backbones. (3) We validate PVT through extensive experiments, showing that it\\nboosts the performance of many downstream tasks, including object detection, instance and semantic segmentation.\\nFor example, with a comparable number of parameters,\\nPVT+RetinaNet achieves 40.4 AP on the COCO dataset,\\nsurpassing ResNet50+RetinNet (36.3 AP) by 4.1 absolute\\nAP (see Figure 2). We hope that PVT could serve as an\\nalternative and useful backbone for pixel-level predictions\\nand facilitate future research.\\n1. Introduction\\nConvolutional neural network (CNNs) have achieved remarkable success in computer vision, making them a versatile and dominant approach for almost all tasks . Nevertheless, this work aims to explore an alternative backbone network beyond CNN, which\\ncan be used for dense prediction tasks such as object detec-\\n \\nCOCO BBox AP (%)\\n#Parameter (M)\\nX101-32x4d\\nX101-64x4d\\n#Param (M) AP\\nPVT-T (ours)\\nPVT-S (ours)\\nX101-32x4d \\nViT-S/32 \\nPVT-M (ours)\\nX101-64x4d \\nPVT-L (ours)\\nFigure 2: Performance comparison on COCO val2017\\nof different backbones using RetinaNet for object detection, where \\u201cT\\u201d, \\u201cS\\u201d, \\u201cM\\u201d and \\u201cL\\u201d denote our PVT models\\nwith tiny, small, medium and large size. We see that when\\nthe number of parameters among different models are comparable, PVT variants signi\\ufb01cantly outperform their corresponding counterparts such as ResNets (R) , ResNeXts\\n(X) , and ViT .\\ntion , semantic and instance segmentation ,\\nin addition to image classi\\ufb01cation .\\nInspired by the success of Transformer in natural language processing, many researchers have explored\\nits application in computer vision.\\nFor example, some\\nworks model the vision task as a dictionary lookup problem with learnable queries, and use the\\nTransformer decoder as a task-speci\\ufb01c head on top of the\\nCNN backbone. Although some prior arts have also incorporated attention modules into CNNs, as far\\nas we know, exploring a clean and convolution-free Transformer backbone to address dense prediction tasks in computer vision is rarely studied.\\nRecently, Dosovitskiy et al. introduced the Vision\\nTransformer (ViT) for image classi\\ufb01cation. This is an interesting and meaningful attempt to replace the CNN backbone with a convolution-free model. As shown in Figure 1\\n(b), ViT has a columnar structure with coarse image patches\\nas input.1\\nAlthough ViT is applicable to image classi\\ufb01cation, it is challenging to directly adapt it to pixel-level\\ndense predictions such as object detection and segmentation, because (1) its output feature map is single-scale and\\nlow-resolution, and (2) its computational and memory costs\\nare relatively high even for common input image sizes (e.g.,\\n1Due to resource constraints, ViT cannot use \\ufb01ne-grained image\\npatches (e.g., 4\\u00d74 pixels per patch) as input, instead only receive coarse\\npatches (e.g., 32\\u00d732 pixels per patch) as input, which leads to its low output resolution (e.g., 32-stride).\\nshorter edge of 800 pixels in the COCO benchmark ).\\nTo address the above limitations, this work proposes a\\npure Transformer backbone, termed Pyramid Vision Transformer (PVT), which can serve as an alternative to the CNN\\nbackbone in many downstream tasks, including image-level\\nprediction as well as pixel-level dense predictions. Speci\\ufb01cally, as illustrated in Figure 1 (c), our PVT overcomes the\\ndif\\ufb01culties of the conventional Transformer by (1) taking\\n\\ufb01ne-grained image patches (i.e., 4\\u00d74 pixels per patch) as input to learn high-resolution representation, which is essential for dense prediction tasks; (2) introducing a progressive\\nshrinking pyramid to reduce the sequence length of Transformer as the network deepens, signi\\ufb01cantly reducing the\\ncomputational cost, and (3) adopting a spatial-reduction attention (SRA) layer to further reduce the resource consumption when learning high-resolution features.\\nOverall, the proposed PVT possesses the following merits.\\nFirstly, compared to the traditional CNN backbones\\n(see Figure 1 (a)), which have local receptive \\ufb01elds that increase with the network depth, our PVT always produces a\\nglobal receptive \\ufb01eld, which is more suitable for detection\\nand segmentation. Secondly, compared to ViT (see Figure 1 (b)), thanks to its advanced pyramid structure, our\\nmethod can more easily be plugged into many representative dense prediction pipelines, e.g., RetinaNet and\\nMask R-CNN . Thirdly, we can build a convolutionfree pipeline by combining our PVT with other task-speci\\ufb01c\\nTransformer decoders, such as PVT+DETR for object detection. To our knowledge, this is the \\ufb01rst entirely\\nconvolution-free object detection pipeline.\\nOur main contributions are as follows:\\n(1) We propose Pyramid Vision Transformer (PVT),\\nwhich is the \\ufb01rst pure Transformer backbone designed for\\nvarious pixel-level dense prediction tasks. Combining our\\nPVT and DETR, we can construct an end-to-end object detection system without convolutions and handcrafted components such as dense anchors and non-maximum suppression (NMS).\\n(2) We overcome many dif\\ufb01culties when porting Transformer to dense predictions, by designing a progressive\\nshrinking pyramid and a spatial-reduction attention (SRA).\\nThese are able to reduce the resource consumption of Transformer, making PVT \\ufb02exible to learning multi-scale and\\nhigh-resolution features.\\n(3) We evaluate the proposed PVT on several different tasks, including image classi\\ufb01cation, object detection,\\ninstance and semantic segmentation, and compare it with\\npopular ResNets and ResNeXts .\\nAs presented\\nin Figure 2, our PVT with different parameter scales can\\nconsistently archived improved performance compared to\\nthe prior arts. For example, under a comparable number\\nof parameters, using RetinaNet for object detection,\\nPVT-Small achieves 40.4 AP on COCO val2017, outperforming ResNet50 by 4.1 points (40.4 vs. 36.3). Moreover,\\nPVT-Large achieves 42.6 AP, which is 1.6 points better than\\nResNeXt101-64x4d, with 30% less parameters.\\n2. Related Work\\n2.1. CNN Backbones\\nCNNs are the work-horses of deep neural networks in visual recognition. The standard CNN was \\ufb01rst introduced in\\n to distinguish handwritten numbers. The model contains convolutional kernels with a certain receptive \\ufb01eld\\nthat captures favorable visual context. To provide translation equivariance, the weights of convolutional kernels\\nare shared over the entire image space.\\nMore recently,\\nwith the rapid development of the computational resources\\n(e.g., GPU), the successful training of stacked convolutional\\nblocks on large-scale image classi\\ufb01cation datasets\\n(e.g., ImageNet ) has become possible. For instance,\\nGoogLeNet demonstrated that a convolutional operator containing multiple kernel paths can achieve very competitive performance.\\nThe effectiveness of a multi-path\\nconvolutional block was further validated in Inception series , ResNeXt , DPN , MixNet and\\nSKNet . Further, ResNet introduced skip connections into the convolutional block, making it possible to create/train very deep networks and obtaining impressive results in the \\ufb01eld of computer vision. DenseNet introduced a densely connected topology, which connects each\\nconvolutional block to all previous blocks. More recent advances can be found in recent survey/review papers .\\nUnlike the full-blown CNNs, the vision Transformer\\nbackbone is still in its early stage of development. In this\\nwork, we try to extend the scope of Vision Transformer by\\ndesigning a new versatile Transformer backbone suitable\\nfor most vision tasks.\\n2.2. Dense Prediction Tasks\\nPreliminary. The dense prediction task aims to perform\\npixel-level classi\\ufb01cation or regression on a feature map.\\nObject detection and semantic segmentation are two representative dense prediction tasks.\\nObject Detection.\\nIn the era of deep learning,\\nCNNs have become the dominant framework for object detection, which includes single-stage detectors (e.g.,\\nSSD , RetinaNet , FCOS , GFL , PolarMask and OneNet ) and multi-stage detectors\\n(Faster R-CNN , Mask R-CNN , Cascade R-CNN\\n and Sparse R-CNN ). Most of these popular object detectors are built on high-resolution or multi-scale feature maps to obtain good detection performance. Recently,\\nDETR and deformable DETR combined the CNN\\nbackbone and the Transformer decoder to build an endto-end object detector. Likewise, they also require highresolution or multi-scale feature maps for accurate object\\ndetection.\\nSemantic Segmentation. CNNs also play an important\\nrole in semantic segmentation. In the early stages, FCN\\n introduced a fully convolutional architecture to generate a spatial segmentation map for a given image of any\\nsize. After that, the deconvolution operation was introduced\\nby Noh et al. and achieved impressive performance on\\nthe PASCAL VOC 2012 dataset . Inspired by FCN, U-\\nNet was proposed for the medical image segmentation\\ndomain speci\\ufb01cally, bridging the information \\ufb02ow between\\ncorresponding low-level and high-level feature maps of the\\nsame spatial sizes. To explore richer global context representation, Zhao et al. designed a pyramid pooling\\nmodule over various pooling scales, and Kirillov et al. \\ndeveloped a lightweight segmentation head termed Semantic FPN, based on FPN . Finally, the DeepLab family\\n applies dilated convolutions to enlarge the receptive\\n\\ufb01eld while maintaining the feature map resolution. Similar\\nto object detection methods, semantic segmentation models\\nalso rely on high-resolution or multi-scale feature maps.\\n2.3. Self-Attention and Transformer in Vision\\nAs convolutional \\ufb01lter weights are usually \\ufb01xed after\\ntraining, they cannot be dynamically adapted to different\\ninputs. Many methods have been proposed to alleviate this\\nproblem using dynamic \\ufb01lters or self-attention operations . The non-local block attempts to model\\nlong-range dependencies in both space and time, which\\nhas been shown bene\\ufb01cial for accurate video classi\\ufb01cation.\\nHowever, despite its success, the non-local operator suffers from the high computational and memory costs.\\nCriss-cross further reduces the complexity by generating sparse attention maps through a criss-cross path.\\nRamachandran et al. proposed the stand-alone selfattention to replace convolutional layers with local selfattention units.\\nAANet achieves competitive results\\nwhen combining the self-attention and convolutional operations. LambdaNetworks uses the lambda layer, an ef-\\n\\ufb01cient self-attention to replace the convolution in the CNN.\\nDETR utilizes the Transformer decoder to model object detection as an end-to-end dictionary lookup problem\\nwith learnable queries, successfully removing the need for\\nhandcrafted processes such as NMS. Based on DETR, deformable DETR further adopts a deformable attention layer to focus on a sparse set of contextual elements,\\nobtaining faster convergence and better performance. Recently, Vision Transformer (ViT) employs a pure\\nTransformer model for image classi\\ufb01cation by treating an image as a sequence of patches. DeiT further\\nextends ViT using a novel distillation approach. Different\\nfrom previous models, this work introduces the pyramid\\nstructure into Transformer to present a pure Transformer\\nTransformer Encoder (\\ud835\\udc3f%\\u00d7)\\n\\u00d7(\\ud835\\udc43!$\\ud835\\udc36!\\\"#)\\nPosition Embedding\\nElement-wise Add\\nFeature Map\\nMulti-Head\\nFigure 3: Overall architecture of Pyramid Vision Transformer (PVT). The entire model is divided into four stages, each\\nof which is comprised of a patch embedding layer and a Li-layer Transformer encoder. Following a pyramid structure, the\\noutput resolution of the four stages progressively shrinks from high (4-stride) to low (32-stride).\\nbackbone for dense prediction tasks, rather than a taskspeci\\ufb01c head or an image classi\\ufb01cation model.\\n3. Pyramid Vision Transformer (PVT)\\n3.1. Overall Architecture\\nOur goal is to introduce the pyramid structure into the\\nTransformer framework, so that it can generate multi-scale\\nfeature maps for dense prediction tasks (e.g., object detection and semantic segmentation). An overview of PVT is\\ndepicted in Figure 3. Similar to CNN backbones , our\\nmethod has four stages that generate feature maps of different scales. All stages share a similar architecture, which\\nconsists of a patch embedding layer and Li Transformer encoder layers.\\nIn the \\ufb01rst stage, given an input image of size H\\u00d7W\\u00d73,\\nwe \\ufb01rst divide it into HW\\npatches,2 each of size 4\\u00d74\\u00d73.\\nThen, we feed the \\ufb02attened patches to a linear projection\\nand obtain embedded patches of size HW\\n42 \\u00d7C1. After that,\\nthe embedded patches along with a position embedding are\\npassed through a Transformer encoder with L1 layers, and\\nthe output is reshaped to a feature map F1 of size H\\nIn the same way, using the feature map from the previous stage as input, we obtain the following feature maps:\\nF2, F3, and F4, whose strides are 8, 16, and 32 pixels\\nwith respect to the input image. With the feature pyramid\\n{F1, F2, F3, F4}, our method can be easily applied to most\\n2As done for ResNet, we keep the highest resolution of our output feature map at 4-stride.\\ndownstream tasks, including image classi\\ufb01cation, object detection, and semantic segmentation.\\n3.2. Feature Pyramid for Transformer\\nUnlike CNN backbone networks , which use\\ndifferent convolutional strides to obtain multi-scale feature\\nmaps, our PVT uses a progressive shrinking strategy to control the scale of feature maps by patch embedding layers.\\nHere, we denote the patch size of the i-th stage as Pi. At\\nthe beginning of stage i, we \\ufb01rst evenly divide the input feature map Fi\\u22121 \\u2208RHi\\u22121\\u00d7Wi\\u22121\\u00d7Ci\\u22121 into Hi\\u22121Wi\\u22121\\npatches, and\\nthen each patch is \\ufb02atten and projected to a Ci-dimensional\\nembedding. After the linear projection, the shape of the embedded patches can be viewed as Hi\\u22121\\nPi \\u00d7Ci, where\\nthe height and width are Pi times smaller than the input.\\nIn this way, we can \\ufb02exibly adjust the scale of the feature\\nmap in each stage, making it possible to construct a feature\\npyramid for Transformer.\\n3.3. Transformer Encoder\\nThe Transformer encoder in the stage i has Li encoder\\nlayers, each of which is composed of an attention layer\\nand a feed-forward layer . Since PVT needs to process\\nhigh-resolution (e.g., 4-stride) feature maps, we propose a\\nspatial-reduction attention (SRA) layer to replace the traditional multi-head attention (MHA) layer in the encoder.\\nSimilar to MHA, our SRA receives a query Q, a key K,\\nand a value V as input, and outputs a re\\ufb01ned feature. The\\ndifference is that our SRA reduces the spatial scale of K\\nMulti-Head Attention\\nMulti-Head\\nSpatial-Reduction Attention (ours)\\nMulti-Head\\nV (\\ud835\\udc3b! \\ud835\\udc4a!)\\u00d7\\ud835\\udc36!\\nMulti-head attention (MHA) vs. spatialreduction attention (SRA). With the spatial-reduction operation, the computational/memory cost of our SRA is\\nmuch lower than that of MHA.\\nand V before the attention operation (see Figure 4), which\\nlargely reduces the computational/memory overhead. Details of the SRA in the stage i can be formulated as follows:\\nSRA(Q, K, V ) = Concat(head0, ..., headNi)W O,\\nheadj = Attention(QW Q\\nj , SR(K)W K\\nj , SR(V)W V\\nwhere Concat(\\u00b7) is the concatenation operation as in .\\nj \\u2208RCi\\u00d7dhead, W K\\n\\u2208RCi\\u00d7dhead, W V\\nj \\u2208RCi\\u00d7dhead, and\\nW O \\u2208RCi\\u00d7Ci are linear projection parameters. Ni is the\\nhead number of the attention layer in Stage i. Therefore, the\\ndimension of each head (i.e., dhead) is equal to Ci\\nNi . SR(\\u00b7) is\\nthe operation for reducing the spatial dimension of the input\\nsequence (i.e., K or V ), which is written as:\\nSR(x) = Norm(Reshape(x, Ri)W S).\\nHere, x \\u2208R(HiWi)\\u00d7Ci represents a input sequence, and\\nRi denotes the reduction ratio of the attention layers in\\nStage i. Reshape(x, Ri) is an operation of reshaping the\\ninput sequence x to a sequence of size HiWi\\ni Ci)\\u00d7Ci is a linear projection that reduces the dimension of the input sequence to Ci. Norm(\\u00b7) refers to\\nlayer normalization . As in the original Transformer ,\\nour attention operation Attention(\\u00b7) is calculated as:\\nAttention(q, k, v) = Softmax( qkT\\nThrough these formulas, we can \\ufb01nd that the computational/memory costs of our attention operation are R2\\nlower than those of MHA, so our SRA can handle larger\\ninput feature maps/sequences with limited resources.\\n3.4. Model Details\\nIn summary, the hyper parameters of our method are\\nlisted as follows:\\n\\u2022 Pi: the patch size of Stage i;\\n\\u2022 Ci: the channel number of the output of Stage i;\\n\\u2022 Li: the number of encoder layers in Stage i;\\n\\u2022 Ri: the reduction ratio of the SRA in Stage i;\\n\\u2022 Ni: the head number of the SRA in Stage i;\\n\\u2022 Ei: the expansion ratio of the feed-forward layer \\nin Stage i;\\nFollowing the design rules of ResNet , we (1) use small\\noutput channel numbers in shallow stages; and (2) concentrate the major computation resource in intermediate stages.\\nTo provide instances for discussion, we describe a series\\nof PVT models with different scales, namely PVT-Tiny, -\\nSmall, -Medium, and -Large, in Table 1, whose parameter\\nnumbers are comparable to ResNet18, 50, 101, and 152 respectively. More details of employing these models in speci\\ufb01c downstream tasks will be introduced in Section 4.\\n3.5. Discussion\\nThe most related work to our model is ViT . Here,\\nwe discuss the relationship and differences between them.\\nFirst, both PVT and ViT are pure Transformer models without convolutions.\\nThe primary difference between them\\nis the pyramid structure. Similar to the traditional Transformer , the length of ViT\\u2019s output sequence is the same\\nas the input, which means that the output of ViT is singlescale (see Figure 1 (b)). Moreover, due to the limited resource, the input of ViT is coarse-grained (e.g., the patch\\nsize is 16 or 32 pixels), and thus its output resolution is relatively low (e.g., 16-stride or 32-stride). As a result, it is\\ndif\\ufb01cult to directly apply ViT to dense prediction tasks that\\nrequire high-resolution or multi-scale feature maps.\\nOur PVT breaks the routine of Transformer by introducing a progressive shrinking pyramid.\\nIt can generate multi-scale feature maps like a traditional CNN backbone.\\nIn addition, we also designed a simple but effective attention layer\\u2014SRA, to process high-resolution feature maps and reduce computational/memory costs. Bene\\ufb01ting from the above designs, our method has the following advantages over ViT: 1) more \\ufb02exible\\u2014can generate feature maps of different scales/channels in different stages; 2) more versatile\\u2014can be easily plugged and\\nplayed in most downstream task models; 3) more friendly\\nto computation/memory\\u2014can handle higher resolution feature maps or longer sequences.\\n4. Application to Downstream Tasks\\n4.1. Image-Level Prediction\\nImage classi\\ufb01cation is the most classical task of imagelevel prediction. To provide instances for discussion, we\\ndesign a series of PVT models with different scales, namely\\nPVT-Tiny, -Small, -Medium, and -Large, whose parameter\\nnumbers are similar to ResNet18, 50, 101, and 152, respec-\\nOutput Size\\nLayer Name\\nPVT-Medium\\nPatch Embedding\\nP1 = 4; C1 = 64\\nTransformer\\nPatch Embedding\\nP2 = 2; C2 = 128\\nTransformer\\nPatch Embedding\\nP3 = 2; C3 = 320\\nTransformer\\nPatch Embedding\\nP4 = 2; C4 =512\\nTransformer\\nTable 1: Detailed settings of PVT series. The design follows the two rules of ResNet : (1) with the growth of network\\ndepth, the hidden dimension gradually increases, and the output resolution progressively shrinks; (2) the major computation\\nresource is concentrated in Stage 3.\\ntively. Detailed hyper-parameter settings of the PVT series\\nare provided in the supplementary material (SM).\\nFor image classi\\ufb01cation, we follow ViT and\\nDeiT to append a learnable classi\\ufb01cation token to the\\ninput of the last stage, and then employ a fully connected\\n(FC) layer to conduct classi\\ufb01cation on top of the token.\\n4.2. Pixel-Level Dense Prediction\\nIn addition to image-level prediction, dense prediction\\nthat requires pixel-level classi\\ufb01cation or regression to be\\nperformed on the feature map, is also often seen in downstream tasks. Here, we discuss two typical tasks, namely\\nobject detection, and semantic segmentation.\\nWe apply our PVT models to three representative dense\\nprediction methods, namely RetinaNet , Mask R-\\nCNN , and Semantic FPN . RetinaNet is a widely\\nused single-stage detector, Mask R-CNN is the most popular two-stage instance segmentation framework, and Semantic FPN is a vanilla semantic segmentation method\\nwithout special operations (e.g., dilated convolution). Using these methods as baselines enables us to adequately examine the effectiveness of different backbones.\\nThe implementation details are as follows:\\nResNet, we initialize the PVT backbone with the weights\\npre-trained on ImageNet; (2) We use the output feature\\npyramid {F1, F2, F3, F4} as the input of FPN , and\\nthen the re\\ufb01ned feature maps are fed to the follow-up detection/segmentation head; (3) When training the detection/segmentation model, none of the layers in PVT are\\nfrozen; (4) Since the input for detection/segmentation can\\nbe an arbitrary shape, the position embeddings pre-trained\\non ImageNet may no longer be meaningful. Therefore, we\\nperform bilinear interpolation on the pre-trained position\\nembeddings according to the input resolution.\\n5. Experiments\\nWe compare PVT with the two most representative CNN\\nbackbones, i.e., ResNet and ResNeXt , which are\\nwidely used in the benchmarks of many downstream tasks.\\n5.1. Image Classi\\ufb01cation\\nSettings. Image classi\\ufb01cation experiments are performed\\non the ImageNet 2012 dataset , which comprises 1.28\\nmillion training images and 50K validation images from\\n1,000 categories.\\nFor fair comparison, all models are\\ntrained on the training set, and report the top-1 error on the\\nvalidation set. We follow DeiT and apply random cropping, random horizontal \\ufb02ipping , label-smoothing regularization , mixup , CutMix , and random erasing as data augmentations. During training, we employ\\nAdamW with a momentum of 0.9, a mini-batch size of\\n128, and a weight decay of 5 \\u00d7 10\\u22122 to optimize models.\\nThe initial learning rate is set to 1\\u00d710\\u22123 and decreases following the cosine schedule . All models are trained for\\n300 epochs from scratch on 8 V100 GPUs. To benchmark,\\nwe apply a center crop on the validation set, where a 224\\u00d7\\n224 patch is cropped to evaluate the classi\\ufb01cation accuracy.\\nResults. In Table 2, we see that our PVT models are superior to conventional CNN backbones under similar parameter numbers and computational budgets. For example, when\\n#Param (M)\\nTop-1 Err (%)\\nResNet18* \\nResNet18 \\nDeiT-Tiny/16 \\nPVT-Tiny (ours)\\nResNet50* \\nResNet50 \\nResNeXt50-32x4d* \\nResNeXt50-32x4d \\nT2T-ViTt-14 \\nTNT-S \\nDeiT-Small/16 \\nPVT-Small (ours)\\nResNet101* \\nResNet101 \\nResNeXt101-32x4d* \\nResNeXt101-32x4d \\nT2T-ViTt-19 \\nViT-Small/16 \\nPVT-Medium (ours)\\nResNeXt101-64x4d* \\nResNeXt101-64x4d \\nViT-Base/16 \\nT2T-ViTt-24 \\nTNT-B \\nDeiT-Base/16 \\nPVT-Large (ours)\\nTable 2: Image classi\\ufb01cation performance on the ImageNet validation set. \\u201c#Param\\u201d refers to the number of\\nparameters. \\u201cGFLOPs\\u201d is calculated under the input scale\\nof 224 \\u00d7 224. \\u201c*\\u201d indicates the performance of the method\\ntrained under the strategy of its original paper.\\nthe GFLOPs are roughly similar, the top-1 error of PVT-\\nSmall reaches 20.2, which is 1.3 points higher than that of\\nResNet50 (20.2 vs. 21.5). Meanwhile, under similar or\\nlower complexity, PVT models archive performances comparable to the recently proposed Transformer-based models, such as ViT and DeiT (PVT-Large: 18.3 vs.\\nViT(DeiT)-Base/16: 18.3). Here, we clarify that these results are within our expectations, because the pyramid structure is bene\\ufb01cial to dense prediction tasks, but brings little\\nimprovements to image classi\\ufb01cation.\\nNote that ViT and DeiT have limitations as they are\\nspeci\\ufb01cally designed for classi\\ufb01cation tasks, and thus are\\nnot suitable for dense prediction tasks, which usually require effective feature pyramids.\\n5.2. Object Detection\\nSettings. Object detection experiments are conducted on\\nthe challenging COCO benchmark .\\nAll models are\\ntrained on COCO train2017 (118k images) and evaluated on val2017 (5k images). We verify the effectiveness\\nof PVT backbones on top of two standard detectors, namely\\nRetinaNet and Mask R-CNN . Before training, we\\nuse the weights pre-trained on ImageNet to initialize the\\nbackbone and Xavier to initialize the newly added layers. Our models are trained with a batch size of 16 on 8\\nV100 GPUs and optimized by AdamW with an initial learning rate of 1 \\u00d7 10\\u22124. Following common practices , we adopt 1\\u00d7 or 3\\u00d7 training schedule (i.e.,\\n12 or 36 epochs) to train all detection models. The training\\nimage is resized to have a shorter side of 800 pixels, while\\nthe longer side does not exceed 1,333 pixels. When using\\nthe 3\\u00d7 training schedule, we randomly resize the shorter\\nside of the input image within the range of . In\\nthe testing phase, the shorter side of the input image is \\ufb01xed\\nto 800 pixels.\\nResults. As shown in Table 3, when using RetinaNet for\\nobject detection, we \\ufb01nd that under comparable number of\\nparameters, the PVT-based models signi\\ufb01cantly surpasses\\ntheir counterparts. For example, with the 1\\u00d7 training schedule, the AP of PVT-Tiny is 4.9 points better than that of\\nResNet18 (36.7 vs. 31.8). Moreover, with the 3\\u00d7 training\\nschedule and multi-scale training, PVT-Large archive the\\nbest AP of 43.4, surpassing ResNeXt101-64x4d (43.4 vs.\\n41.8), while our parameter number is 30% fewer. These results indicate that our PVT can be a good alternative to the\\nCNN backbone for object detection.\\nSimilar results are found in instance segmentation experiments based on Mask R-CNN, as shown in Table 4. With\\nthe 1\\u00d7 training schedule, PVT-Tiny achieves 35.1 mask AP\\n(APm), which is 3.9 points better than ResNet18 (35.1 vs.\\n31.2) and even 0.7 points higher than ResNet50 (35.1 vs.\\n34.4). The best APm obtained by PVT-Large is 40.7, which\\nis 1.0 points higher than ResNeXt101-64x4d (40.7 vs. 39.7),\\nwith 20% fewer parameters.\\n5.3. Semantic Segmentation\\nSettings. We choose ADE20K , a challenging scene\\nparsing dataset, to benchmark the performance of semantic\\nsegmentation. ADE20K contains 150 \\ufb01ne-grained semantic\\ncategories, with 20,210, 2,000, and 3,352 images for training, validation, and testing, respectively. We evaluate our\\nPVT backbones on the basis of Semantic FPN , a simple segmentation method without dilated convolutions .\\nIn the training phase, the backbone is initialized with the\\nweights pre-trained on ImageNet , and other newly\\nadded layers are initialized with Xavier . We optimize\\nour models using AdamW with an initial learning rate\\nof 1e-4. Following common practices , we train our\\nmodels for 80k iterations with a batch size of 16 on 4 V100\\nGPUs. The learning rate is decayed following the polynomial decay schedule with a power of 0.9. We randomly\\nresize and crop the image to 512 \\u00d7 512 for training, and\\nrescale to have a shorter side of 512 pixels during testing.\\nAs shown in Table 5, when using Semantic FPN for semantic segmentation,\\nmodels consistently outperforms the models based on\\nResNet or ResNeXt .\\nFor example, with al-\\nRetinaNet 1x\\nRetinaNet 3x + MS\\nResNet18 \\nPVT-Tiny (ours)\\n36.7(+4.9)\\n39.4(+4.0)\\nResNet50 \\nPVT-Small (ours)\\n40.4(+4.1)\\n42.2(+3.2)\\nResNet101 \\nResNeXt101-32x4d \\n39.9(+1.4)\\n41.4(+0.5)\\nPVT-Medium (ours)\\n41.9(+3.4)\\n43.2(+2.3)\\nResNeXt101-64x4d \\nPVT-Large (ours)\\n42.6(+1.6)\\n43.4(+1.6)\\nTable 3: Object detection performance on COCO val2017. \\u201cMS\\u201d means that multi-scale training is used.\\nMask R-CNN 1x\\nMask R-CNN 3x + MS\\nResNet18 \\nPVT-Tiny (ours)\\n36.7(+2.7)\\n35.1(+3.9)\\n39.8(+2.9)\\n37.4(+3.8)\\nResNet50 \\nPVT-Small (ours)\\n40.4(+2.4)\\n37.8(+3.4)\\n43.0(+2.0)\\n39.9(+2.8)\\nResNet101 \\nResNeXt101-32x4d \\n41.9(+1.5)\\n37.5(+1.1)\\n44.0(+1.2)\\n39.2(+0.7)\\nPVT-Medium (ours)\\n42.0(+1.6)\\n39.0(+2.6)\\n44.2(+1.4)\\n40.5(+2.0)\\nResNeXt101-64x4d \\nPVT-Large (ours)\\n42.9(+0.1)\\n39.5(+1.1)\\n44.5(+0.1)\\n40.7(+1.0)\\nTable 4: Object detection and instance segmentation performance on COCO val2017. APb and APm denote bounding\\nbox AP and mask AP, respectively.\\nSemantic FPN\\n#Param (M)\\nResNet18 \\nPVT-Tiny (ours)\\n35.7(+2.8)\\nResNet50 \\nPVT-Small (ours)\\n39.8(+3.1)\\nResNet101 \\nResNeXt101-32x4d \\n39.7(+0.9)\\nPVT-Medium (ours)\\n41.6(+2.8)\\nResNeXt101-64x4d \\nPVT-Large (ours)\\n42.1(+1.9)\\nPVT-Large* (ours)\\nTable 5: Semantic segmentation performance of different backbones on the ADE20K validation set. \\u201cGFLOPs\\u201d\\nis calculated under the input scale of 512 \\u00d7 512. \\u201c*\\u201d indicates 320K iterations training and multi-scale \\ufb02ip testing.\\nmost the same number of parameters and GFLOPs, our\\nPVT-Tiny/Small/Medium are at least 2.8 points higher than\\nResNet-18/50/101.\\nIn addition, although the parameter\\nnumber and GFLOPs of our PVT-Large are 20% lower than\\nthose of ResNeXt101-64x4d, the mIoU is still 1.9 points\\nhigher (42.1 vs. 40.2). With a longer training schedule and\\nmulti-scale testing, PVT-Large+Semantic FPN archives the\\nbest mIoU of 44.8, which is very close to the state-of-the-art\\nperformance of the ADE20K benchmark. Note that Semantic FPN is just a simple segmentation head. These results\\ndemonstrate that our PVT backbones can extract better features for semantic segmentation than the CNN backbone,\\nDETR (50 Epochs)\\nResNet50 \\nPVT-Small (ours)\\n34.7(+2.4)\\nTable 6: Performance of the pure Transformer object\\ndetection pipeline. We build a pure Transformer detector\\nby combining PVT and DETR , whose AP is 2.4 points\\nhigher than the original DETR based on ResNet50 .\\nbene\\ufb01ting from the global attention mechanism.\\n5.4. Pure Transformer Detection & Segmentation\\nPVT+DETR. To reach the limit of no convolution, we build\\na pure Transformer pipeline for object detection by simply combining our PVT with a Transformer-based detection\\nhead\\u2014DETR . We train models on COCO train2017\\nfor 50 epochs with an initial learning rate of 1 \\u00d7 10\\u22124.\\nThe learning rate is divided by 10 at the 33rd epoch. We\\nuse random \\ufb02ipping and multi-scale training as data augmentation. All other experimental settings is the same as\\nthose in Sec. 5.2. As reported in Table 6, PVT-based DETR\\narchieves 34.7 AP on COCO val2017, outperforming\\nthe original ResNet50-based DETR by 2.4 points (34.7 vs.\\n32.3). These results prove that a pure Transformer detector\\ncan also works well in the object detection task.\\nPVT+Trans2Seg.We build a pure Transformer model\\nfor semantic segmentation by combining our PVT with\\n#Param (M)\\nResNet50-d8+DeeplabV3+ \\nResNet50-d16+DeeplabV3+ \\nResNet50-d16+Trans2Seg \\nPVT-Small+Trans2Seg\\n42.6(+2.9)\\nTable 7: Performance of the pure Transformer semantic\\nsegmentation pipeline. We build a pure Transformer detector by combining PVT and Trans2Seg . It is 2.9%\\nhigher than ResNet50-d16+Trans2Seg and 1.1% higher\\nthan ResNet50-d8+DeeplabV3+ with lower GFlops. \\u201cd8\\u201d\\nand \\u201cd16\\u201d means dilation 8 and 16, respectively.\\nRetinaNet 1x\\nViT-Small/4 \\nOut of Memory\\nViT-Small/32 \\nPVT-Small (ours)\\nTable 8: Performance comparison between ViT and our\\nPVT using RetinaNet for object detection. ViT-Small/4\\nruns out of GPU memory due to small patch size (i.e.,\\n4\\u00d74 per patch). ViT-Small/32 obtains 31.7 AP on COCO\\nval2017, which is 8.7 points lower than our PVT-Small.\\nTrans2Seg , a Transformer-based segmentation head.\\nAccording to the experimental settings in Sec. 5.3, we\\nperform experiments on ADE20K with 40k iterations training,\\nsingle scale testing,\\nand compare it\\nwith ResNet50+Trans2Seg and DeeplabV3+ with\\nResNet50-d8 (dilation 8) and -d16(dilation 8) in Table\\n7. We \\ufb01nd that our PVT-Small+Trans2Seg achieves 42.6\\nmIoU, outperforming ResNet50-d8+DeeplabV3+ (41.5).\\nNote that, ResNet50-d8+DeeplabV3+ has 120.5 GFLOPs\\ndue to the high computation cost of dilated convolution, and\\nour method has only 31.6 GFLOPs, which is 4 times fewer.\\nIn addition, our PVT-Small+Trans2Seg performs better than\\nResNet50-d16+Trans2Seg (mIoU: 42.6 vs. 39.7, GFlops:\\n31.6 vs. 79.3). These results prove that a pure Transformer\\nsegmentation network is workable.\\n5.5. Ablation Study\\nSettings. We conduct ablation studies on ImageNet \\nand COCO datasets. The experimental settings on ImageNet are the same as the settings in Sec. 5.1. For COCO,\\nall models are trained with a 1\\u00d7 training schedule (i.e., 12\\nepochs) and without multi-scale training, and other settings\\nfollow those in Sec. 5.2.\\nPyramid Structure. A Pyramid structure is crucial when\\napplying Transformer to dense prediction tasks. ViT (see\\nFigure 1 (b)) is a columnar framework, whose output is\\nsingle-scale. This results in a low-resolution output feature map when using coarse image patches (e.g., 32\\u00d732\\npixels per patch) as input, leading to poor detection perfor-\\nCOCO BBox AP (%)\\nPVT-Small w pre-train 1x\\nPVT-Small w/ pre-train 3x\\nPVT-Small w/o pre-train 1x\\nPVT-Small w/o pre-train 3x\\nCOCO BBox AP (%)\\nPVT-Small 1x\\nPVT-Small 3x\\nResNet50 1x\\nResNet50 3x\\nFigure 5: AP curves of RetinaNet on COCO val2017\\nunder different backbone settings. Top: using weights\\npre-trained on ImageNet vs. random initialization. Bottom:\\nPVT-S vs. R50 .\\nRetinaNet 1x\\nWider PVT-Small\\nDeeper PVT-Small\\nTable 9: Deeper vs. Wider. \\u201cTop-1\\u201d denotes the top-1 error\\non the ImageNet validation set. \\u201cAP\\u201d denotes the bounding\\nbox AP on COCO val2017. The deep model (i.e., PVT-\\nMedium) obtains better performance than the wide model\\n(i.e., PVT-Small-Wide ) under comparable parameter number.\\nmance (31.7 AP on COCO val2017),3 as shown in Table\\n8. When using \\ufb01ne-grained image patches (e.g., 4\\u00d74 pixels\\nper patch) as input like our PVT, ViT will exhaust the GPU\\nmemory (32G). Our method avoids this problem through a\\nprogressive shrinking pyramid. Speci\\ufb01cally, our model can\\nprocess high-resolution feature maps in shallow stages and\\nlow-resolution feature maps in deep stages. Thus, it obtains\\na promising AP of 40.4 on COCO val2017, 8.7 points\\nhigher than ViT-Small/32 (40.4 vs. 31.7).\\nDeeper vs. Wider. The problem of whether the CNN backbone should go deeper or wider has been extensively discussed in previous work . Here, we explore this\\n3For adapting ViT to RetinaNet, we extract the features from the layer\\n2, 4, 6, and 8 of ViT-Small/32, and interpolate them to different scales.\\nMask R-CNN 1x\\nResNet50+GC r4 \\nPVT-Small (ours)\\nTable 10: PVT vs. CNN w/ non-local.\\nAPm denotes\\nmask AP. Under similar parameter nubmer and GFLOPs,\\nour PVT outperform the CNN backbone w/ Non-Local\\n(ResNet50+GC r4) by 1.6 APm (37.8 vs. 36.2).\\nproblem in our PVT. For fair comparisons, we multiply\\nthe hidden dimensions {C1, C2, C3, C4} of PVT-Small by\\na scale factor 1.4 to make it have an equivalent parameter\\nnumber to the deep model (i.e., PVT-Medium). As shown\\nin Table 9, the deep model (i.e., PVT-Medium) consistently\\nworks better than the wide model (i.e., PVT-Small-Wide) on\\nboth ImageNet and COCO. Therefore, going deeper is more\\neffective than going wider in the design of PVT. Based on\\nthis observation, in Table 1, we develop PVT models with\\ndifferent scales by increasing the model depth.\\nPre-trained Weights. Most dense prediction models (e.g.,\\nRetinaNet ) rely on the backbone whose weights are\\npre-trained on ImageNet. We also discuss this problem in\\nour PVT. In the top of Figure 5, we plot the validation AP\\ncurves of RetinaNet-PVT-Small w/ (red curves) and w/o\\n(blue curves) pre-trained weights. We \\ufb01nd that the model\\nw/ pre-trained weights converges better than the one w/o\\npre-trained weights, and the gap between their \\ufb01nal AP\\nreaches 13.8 under the 1\\u00d7 training schedule and 8.4 under\\nthe 3\\u00d7 training schedule and multi-scale training. Therefore, like CNN-based models, pre-training weights can also\\nhelp PVT-based models converge faster and better. Moreover, in the bottom of Figure 5, we also see that the convergence speed of PVT-based models (red curves) is faster\\nthan that of ResNet-based models (green curves).\\nPVT vs. \\u201cCNN w/ Non-Local\\u201d To obtain a global receptive \\ufb01eld, some well-engineered CNN backbones, such as\\nGCNet , integrate the non-local block in the CNN framework. Here, we compare the performance of our PVT (pure\\nTransformer) and GCNet (CNN w/ non-local), using Mask\\nR-CNN for instance segmentation. As reported in Table\\n10, we \\ufb01nd that our PVT-Small outperforms ResNet50+GC\\nr4 by 1.6 points in APm (37.8 vs. 36.2), and 2.0 points in\\n75 (38.3 vs. 40.3), under comparable parameter number\\nand GFLOPs. There are two possible reasons for this result:\\n(1) Although a single global attention layer (e.g., nonlocal or multi-head attention (MHA) ) can acquire global-receptive-\\ufb01eld features, the model performance keeps improving as the model deepens. This indicates that stacking multiple MHAs can further enhance the\\nrepresentation capabilities of features. Therefore, as a pure\\nTransformer backbone with more global attention layers,\\nour PVT tends to perform better than the CNN backbone\\nRetinaNet 1x\\nResNet50 \\nPVT-Small (ours)\\nTable 11: Latency and AP under different input scales.\\n\\u201cScale\\u201d and \\u201cTime\\u201d denote the input scale and time cost\\nper image. When the shorter side is 640 pixels, the PVT-\\nSmall+RetinaNet has a lower GFLOPs and time cost (on a\\nV100 GPU) than ResNet50+RetinaNet, while obtaining 2.4\\npoints better AP (38.7 vs. 36.3).\\nequipped with non-local blocks (e.g., GCNet).\\n(2) Regular convolutions can be deemed as special instantiations of spatial attention mechanisms . In other\\nwords, the format of MHA is more \\ufb02exible than the regular\\nconvolution. For example, for different inputs, the weights\\nof the convolution are \\ufb01xed, but the attention weights of\\nMHA change dynamically with the input. Thus, the features\\nlearned by the pure Transformer backbone full of MHA layers, could be more \\ufb02exible and expressive.\\nComputation Overhead. With increasing input scale, the\\ngrowth rate of the GFLOPs of our PVT is greater than\\nResNet , but lower than ViT , as shown in Figure\\n6. However, when the input scale does not exceed 640\\u00d7640\\npixels, the GFLOPs of PVT-Small and ResNet50 are similar. This means that our PVT is more suitable for tasks with\\nmedium-resolution input.\\nOn COCO, the shorter side of the input image is 800\\npixels. Under this condition, the inference speed of RetinaNet based on PVT-Small is slower than the ResNet50based model, as reported in Table 11. (1) A direct solution\\nfor this problem is to reduce the input scale. When reducing the shorter side of the input image to 640 pixels, the\\nmodel based on PVT-Small runs faster than the ResNet50based model (51.7ms vs., 55.9ms), with 2.4 higher AP\\n(38.7 vs. 36.3). 2) Another solution is to develop a selfattention layer with lower computational complexity. This\\nis a worth exploring direction, we recently propose a solution PVTv2 .\\nDetection & Segmentation Results. In Figure 7, we also\\npresent some qualitative object detection and instance segmentation results on COCO val2017 , and semantic\\nsegmentation results on ADE20K . These results indicate that a pure Transformer backbone (i.e., PVT) without\\nconvolutions can also be easily plugged in dense prediction\\nmodels (e.g., RetinaNet , Mask R-CNN , and Semantic FPN ), and obtain high-quality results.\\n6. Conclusions and Future Work\\nWe introduce PVT, a pure Transformer backbone for\\ndense prediction tasks, such as object detection and seman-\\nInput Scale\\nViT-Small/16\\nViT-Small/32\\nPVT-Small (ours)\\nFigure 6: Models\\u2019 GFLOPs under different input scales.\\nThe growth rate of GFLOPs:\\nViT-Small/16 >ViT-\\nSmall/32 >PVT-Small (ours)>ResNet50 . When\\nthe input scale is less than 640 \\u00d7 640, the GFLOPs of PVT-\\nSmall and ResNet50 are similar.\\ntic segmentation. We develop a progressive shrinking pyramid and a spatial-reduction attention layer to obtain highresolution and multi-scale feature maps under limited computation/memory resources. Extensive experiments on object detection and semantic segmentation benchmarks verify that our PVT is stronger than well-designed CNN backbones under comparable numbers of parameters.\\nAlthough PVT can serve as an alternative to CNN backbones (e.g., ResNet, ResNeXt), there are still some speci\\ufb01c\\nmodules and operations designed for CNNs and not considered in this work, such as SE , SK , dilated convolution , model pruning , and NAS . Moreover,\\nwith years of rapid developments, there have been many\\nwell-engineered CNN backbones such as Res2Net ,\\nEf\\ufb01cientNet , and ResNeSt .\\nIn contrast, the\\nTransformer-based model in computer vision is still in\\nits early stage of development.\\nTherefore, we believe\\nthere are many potential technologies and applications (e.g.,\\nOCR , 3D and medical \\nimage analysis) to be explored in the future, and hope that\\nPVT could serve as a good starting point.\\nAcknowledgments\\nThis work was supported by the Natural Science Foundation of China under Grant 61672273 and Grant 61832008,\\nthe Science Foundation for Distinguished Young Scholars of Jiangsu under Grant BK20160021, Postdoctoral Innovative Talent Support Program of China under Grant\\nBX20200168, 2020M681608, the General Research Fund\\nof Hong Kong No. 27208720.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"p3178\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"referenced_paper\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Squeeze-and-Excitation Networks\\nJie Hu[0000\\u22120002\\u22125150\\u22121003]\\nLi Shen[0000\\u22120002\\u22122283\\u22124976]\\nSamuel Albanie[0000\\u22120001\\u22129736\\u22125134]\\nGang Sun[0000\\u22120001\\u22126913\\u22126799]\\nEnhua Wu[0000\\u22120002\\u22122174\\u22121428]\\nAbstract\\u2014The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to\\nconstruct informative features by fusing both spatial and channel-wise information within local receptive \\ufb01elds at each layer. A broad\\nrange of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of\\na CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel\\nrelationship and propose a novel architectural unit, which we term the \\u201cSqueeze-and-Excitation\\u201d (SE) block, that adaptively recalibrates\\nchannel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be\\nstacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate\\nthat SE blocks bring signi\\ufb01cant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost.\\nSqueeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classi\\ufb01cation submission which won \\ufb01rst place and\\nreduced the top-5 error to 2.251%, surpassing the winning entry of 2016 by a relative improvement of \\u223c25%. Models and code are\\navailable at \\nIndex Terms\\u2014Squeeze-and-Excitation, Image representations, Attention, Convolutional Neural Networks.\\nINTRODUCTION\\nONVOLUTIONAL neural networks (CNNs) have proven\\nto be useful models for tackling a wide range of visual\\ntasks , , , . At each convolutional layer in the network, a collection of \\ufb01lters expresses neighbourhood spatial\\nconnectivity patterns along input channels\\u2014fusing spatial\\nand channel-wise information together within local receptive \\ufb01elds. By interleaving a series of convolutional layers\\nwith non-linear activation functions and downsampling operators, CNNs are able to produce image representations\\nthat capture hierarchical patterns and attain global theoretical receptive \\ufb01elds. A central theme of computer vision\\nresearch is the search for more powerful representations that\\ncapture only those properties of an image that are most\\nsalient for a given task, enabling improved performance.\\nAs a widely-used family of models for vision tasks, the\\ndevelopment of new neural network architecture designs\\nnow represents a key frontier in this search. Recent research\\nhas shown that the representations produced by CNNs can\\nbe strengthened by integrating learning mechanisms into\\nthe network that help capture spatial correlations between\\nfeatures. One such approach, popularised by the Inception\\nfamily of architectures , , incorporates multi-scale processes into network modules to achieve improved perfor-\\nJie Hu and Enhua Wu are with the State Key Laboratory of Computer\\nScience, Institute of Software, Chinese Academy of Sciences, Beijing,\\n100190, China.\\nThey are also with the University of Chinese Academy of Sciences, Beijing,\\n100049, China.\\nJie Hu is also with Momenta and Enhua Wu is also with the Faculty of\\nScience and Technology & AI Center at University of Macau.\\nE-mail: \\n \\nGang Sun is with LIAMA-NLPR at the Institute of Automation, Chinese\\nAcademy of Sciences. He is also with Momenta.\\nE-mail: \\nLi Shen and Samuel Albanie are with the Visual Geometry Group at the\\nUniversity of Oxford.\\nE-mail: {lishen,albanie}@robots.ox.ac.uk\\nmance. Further work has sought to better model spatial\\ndependencies , and incorporate spatial attention into\\nthe structure of the network .\\nIn this paper, we investigate a different aspect of network\\ndesign - the relationship between channels. We introduce\\na new architectural unit, which we term the Squeeze-and-\\nExcitation (SE) block, with the goal of improving the quality\\nof representations produced by a network by explicitly modelling the interdependencies between the channels of its convolutional features. To this end, we propose a mechanism\\nthat allows the network to perform feature recalibration,\\nthrough which it can learn to use global information to\\nselectively emphasise informative features and suppress less\\nuseful ones.\\nThe structure of the SE building block is depicted in\\nFig. 1. For any given transformation Ftr mapping the\\ninput X to the feature maps U where U \\u2208RH\\u00d7W \\u00d7C,\\ne.g. a convolution, we can construct a corresponding SE\\nblock to perform feature recalibration. The features U are\\n\\ufb01rst passed through a squeeze operation, which produces a\\nchannel descriptor by aggregating feature maps across their\\nspatial dimensions (H \\u00d7 W). The function of this descriptor\\nis to produce an embedding of the global distribution of\\nchannel-wise feature responses, allowing information from\\nthe global receptive \\ufb01eld of the network to be used by\\nall its layers. The aggregation is followed by an excitation\\noperation, which takes the form of a simple self-gating\\nmechanism that takes the embedding as input and produces a collection of per-channel modulation weights. These\\nweights are applied to the feature maps U to generate\\nthe output of the SE block which can be fed directly into\\nsubsequent layers of the network.\\nIt is possible to construct an SE network (SENet) by\\nsimply stacking a collection of SE blocks. Moreover, these\\nSE blocks can also be used as a drop-in replacement for the\\noriginal block at a range of depths in the network architecarXiv:1709.01507v4 [cs.CV] 16 May 2019\\nFig. 1. A Squeeze-and-Excitation block.\\nture (Section 6.4). While the template for the building block\\nis generic, the role it performs at different depths differs\\nthroughout the network. In earlier layers, it excites informative features in a class-agnostic manner, strengthening\\nthe shared low-level representations. In later layers, the SE\\nblocks become increasingly specialised, and respond to different inputs in a highly class-speci\\ufb01c manner (Section 7.2).\\nAs a consequence, the bene\\ufb01ts of the feature recalibration\\nperformed by SE blocks can be accumulated through the\\nThe design and development of new CNN architectures\\nis a dif\\ufb01cult engineering task, typically requiring the selection of many new hyperparameters and layer con\\ufb01gurations. By contrast, the structure of the SE block is simple and\\ncan be used directly in existing state-of-the-art architectures\\nby replacing components with their SE counterparts, where\\nthe performance can be effectively enhanced. SE blocks are\\nalso computationally lightweight and impose only a slight\\nincrease in model complexity and computational burden.\\nTo provide evidence for these claims, we develop several\\nSENets and conduct an extensive evaluation on the ImageNet dataset . We also present results beyond ImageNet\\nthat indicate that the bene\\ufb01ts of our approach are not\\nrestricted to a speci\\ufb01c dataset or task. By making use of\\nSENets, we ranked \\ufb01rst in the ILSVRC 2017 classi\\ufb01cation\\ncompetition. Our best model ensemble achieves a 2.251%\\ntop-5 error on the test set1. This represents roughly a 25%\\nrelative improvement when compared to the winner entry\\nof the previous year (top-5 error of 2.991%).\\nRELATED WORK\\nDeeper architectures. VGGNets and Inception models showed that increasing the depth of a network could\\nsigni\\ufb01cantly increase the quality of representations that\\nit was capable of learning. By regulating the distribution\\nof the inputs to each layer, Batch Normalization (BN) \\nadded stability to the learning process in deep networks\\nand produced smoother optimisation surfaces . Building\\non these works, ResNets demonstrated that it was possible to learn considerably deeper and stronger networks\\nthrough the use of identity-based skip connections , .\\nHighway networks introduced a gating mechanism to\\nregulate the \\ufb02ow of information along shortcut connections.\\nFollowing these works, there have been further reformulations of the connections between network layers , ,\\n1. \\nwhich show promising improvements to the learning and\\nrepresentational properties of deep networks.\\nAn alternative, but closely related line of research has\\nfocused on methods to improve the functional form of\\nthe computational elements contained within a network.\\nGrouped convolutions have proven to be a popular approach for increasing the cardinality of learned transformations , . More \\ufb02exible compositions of operators can\\nbe achieved with multi-branch convolutions , , ,\\n , which can be viewed as a natural extension of the\\ngrouping operator. In prior work, cross-channel correlations\\nare typically mapped as new combinations of features, either independently of spatial structure , or jointly\\nby using standard convolutional \\ufb01lters with 1 \\u00d7 1\\nconvolutions. Much of this research has concentrated on the\\nobjective of reducing model and computational complexity,\\nre\\ufb02ecting an assumption that channel relationships can be\\nformulated as a composition of instance-agnostic functions\\nwith local receptive \\ufb01elds. In contrast, we claim that providing the unit with a mechanism to explicitly model dynamic,\\nnon-linear dependencies between channels using global information can ease the learning process, and signi\\ufb01cantly\\nenhance the representational power of the network.\\nAlgorithmic Architecture Search. Alongside the works\\ndescribed above, there is also a rich history of research\\nthat aims to forgo manual architecture design and instead\\nseeks to learn the structure of the network automatically.\\nMuch of the early work in this domain was conducted in\\nthe neuro-evolution community, which established methods\\nfor searching across network topologies with evolutionary\\nmethods , . While often computationally demanding, evolutionary search has had notable successes which\\ninclude \\ufb01nding good memory cells for sequence models\\n , and learning sophisticated architectures for largescale image classi\\ufb01cation , , . With the goal of reducing the computational burden of these methods, ef\\ufb01cient\\nalternatives to this approach have been proposed based on\\nLamarckian inheritance and differentiable architecture\\nsearch .\\nBy formulating architecture search as hyperparameter\\noptimisation, random search and other more sophisticated model-based optimisation techniques , can\\nalso be used to tackle the problem. Topology selection\\nas a path through a fabric of possible designs and\\ndirect architecture prediction , have been proposed\\nas additional viable architecture search tools. Particularly\\nstrong results have been achieved with techniques from\\nreinforcement learning , , , , . SE blocks\\ncan be used as atomic building blocks for these search\\nalgorithms, and were demonstrated to be highly effective\\nin this capacity in concurrent work .\\nAttention and gating mechanisms. Attention can be interpreted as a means of biasing the allocation of available\\ncomputational resources towards the most informative components of a signal , , , , , . Attention\\nmechanisms have demonstrated their utility across many\\ntasks including sequence learning , , localisation\\nand understanding in images , , image captioning\\n , and lip reading . In these applications, it\\ncan be incorporated as an operator following one or more\\nlayers representing higher-level abstractions for adaptation\\nbetween modalities. Some works provide interesting studies\\ninto the combined use of spatial and channel attention ,\\n . Wang et al. introduced a powerful trunk-and-mask\\nattention mechanism based on hourglass modules that is\\ninserted between the intermediate stages of deep residual\\nnetworks. By contrast, our proposed SE block comprises a\\nlightweight gating mechanism which focuses on enhancing\\nthe representational power of the network by modelling\\nchannel-wise relationships in a computationally ef\\ufb01cient\\nSQUEEZE-AND-EXCITATION BLOCKS\\nA Squeeze-and-Excitation block is a computational unit\\nwhich can be built upon a transformation Ftr mapping an\\ninput X \\u2208RH\\u2032\\u00d7W \\u2032\\u00d7C\\u2032 to feature maps U \\u2208RH\\u00d7W \\u00d7C.\\nIn the notation that follows we take Ftr to be a convolutional operator and use V = [v1, v2, . . . , vC] to denote\\nthe learned set of \\ufb01lter kernels, where vc refers to the\\nparameters of the c-th \\ufb01lter. We can then write the outputs\\nas U = [u1, u2, . . . , uC], where\\nuc = vc \\u2217X =\\nHere \\u2217denotes convolution, vc = [v1\\nc, . . . , vC\\u2032\\n[x1, x2, . . . , xC\\u2032] and uc \\u2208RH\\u00d7W . vs\\nc is a 2D spatial kernel\\nrepresenting a single channel of vc that acts on the corresponding channel of X. To simplify the notation, bias terms\\nare omitted. Since the output is produced by a summation\\nthrough all channels, channel dependencies are implicitly\\nembedded in vc, but are entangled with the local spatial\\ncorrelation captured by the \\ufb01lters. The channel relationships\\nmodelled by convolution are inherently implicit and local\\n(except the ones at top-most layers). We expect the learning\\nof convolutional features to be enhanced by explicitly modelling channel interdependencies, so that the network is able\\nto increase its sensitivity to informative features which can\\nbe exploited by subsequent transformations. Consequently,\\nwe would like to provide it with access to global information\\nand recalibrate \\ufb01lter responses in two steps, squeeze and\\nexcitation, before they are fed into the next transformation.\\nA diagram illustrating the structure of an SE block is shown\\nin Fig. 1.\\nSqueeze: Global Information Embedding\\nIn order to tackle the issue of exploiting channel dependencies, we \\ufb01rst consider the signal to each channel in the\\noutput features. Each of the learned \\ufb01lters operates with\\na local receptive \\ufb01eld and consequently each unit of the\\ntransformation output U is unable to exploit contextual\\ninformation outside of this region.\\nTo mitigate this problem, we propose to squeeze global\\nspatial information into a channel descriptor. This is\\nachieved by using global average pooling to generate\\nchannel-wise statistics. Formally, a statistic z \\u2208RC is generated by shrinking U through its spatial dimensions H \\u00d7 W,\\nsuch that the c-th element of z is calculated by:\\nzc = Fsq(uc) =\\nDiscussion. The output of the transformation U can be\\ninterpreted as a collection of the local descriptors whose\\nstatistics are expressive for the whole image. Exploiting\\nsuch information is prevalent in prior feature engineering\\nwork , , . We opt for the simplest aggregation\\ntechnique, global average pooling, noting that more sophisticated strategies could be employed here as well.\\nExcitation: Adaptive Recalibration\\nTo make use of the information aggregated in the squeeze\\noperation, we follow it with a second operation which aims\\nto fully capture channel-wise dependencies. To ful\\ufb01l this\\nobjective, the function must meet two criteria: \\ufb01rst, it must\\nbe \\ufb02exible (in particular, it must be capable of learning\\na nonlinear interaction between channels) and second, it\\nmust learn a non-mutually-exclusive relationship since we\\nwould like to ensure that multiple channels are allowed to\\nbe emphasised (rather than enforcing a one-hot activation).\\nTo meet these criteria, we opt to employ a simple gating\\nmechanism with a sigmoid activation:\\ns = Fex(z, W) = \\u03c3(g(z, W)) = \\u03c3(W2\\u03b4(W1z)),\\nwhere \\u03b4 refers to the ReLU function, W1 \\u2208R\\nr . To limit model complexity and aid generalisation, we parameterise the gating mechanism by forming\\na bottleneck with two fully-connected (FC) layers around\\nthe non-linearity, i.e. a dimensionality-reduction layer with\\nreduction ratio r (this parameter choice is discussed in Section 6.1), a ReLU and then a dimensionality-increasing layer\\nreturning to the channel dimension of the transformation\\noutput U. The \\ufb01nal output of the block is obtained by\\nrescaling U with the activations s:\\nexc = Fscale(uc, sc) = sc uc,\\nwhere eX = [ex1, ex2, . . . , exC] and Fscale(uc, sc) refers to\\nchannel-wise multiplication between the scalar sc and the\\nfeature map uc \\u2208RH\\u00d7W .\\nDiscussion. The excitation operator maps the inputspeci\\ufb01c descriptor z to a set of channel weights. In this\\nregard, SE blocks intrinsically introduce dynamics conditioned on the input, which can be regarded as a selfattention function on channels whose relationships are not\\ncon\\ufb01ned to the local receptive \\ufb01eld the convolutional \\ufb01lters\\nare responsive to.\\nGlobal pooling\\nSE-Inception Module\\nInception Module\\nFig. 2. The schema of the original Inception module (left) and the SE-\\nInception module (right).\\nInstantiations\\nThe SE block can be integrated into standard architectures\\nsuch as VGGNet by insertion after the non-linearity\\nfollowing each convolution. Moreover, the \\ufb02exibility of the\\nSE block means that it can be directly applied to transformations beyond standard convolutions. To illustrate this point,\\nwe develop SENets by incorporating SE blocks into several\\nexamples of more complex architectures, described next.\\nWe \\ufb01rst consider the construction of SE blocks for Inception networks . Here, we simply take the transformation\\nFtr to be an entire Inception module (see Fig. 2) and by\\nmaking this change for each such module in the architecture, we obtain an SE-Inception network. SE blocks can\\nalso be used directly with residual networks (Fig. 3 depicts\\nthe schema of an SE-ResNet module). Here, the SE block\\ntransformation Ftr is taken to be the non-identity branch\\nof a residual module. Squeeze and Excitation both act before\\nsummation with the identity branch. Further variants that\\nintegrate SE blocks with ResNeXt , Inception-ResNet\\n , MobileNet and Shuf\\ufb02eNet can be constructed\\nby following similar schemes. For concrete examples of\\nSENet architectures, a detailed description of SE-ResNet-50\\nand SE-ResNeXt-50 is given in Table 1.\\nOne consequence of the \\ufb02exible nature of the SE block\\nis that there are several viable ways in which it could\\nbe integrated into these architectures. Therefore, to assess\\nsensitivity to the integration strategy used to incorporate SE\\nblocks into a network architecture, we also provide ablation\\nexperiments exploring different designs for block inclusion\\nin Section 6.5.\\nMODEL AND COMPUTATIONAL COMPLEXITY\\nFor the proposed SE block design to be of practical use, it\\nmust offer a good trade-off between improved performance\\nand increased model complexity. To illustrate the computational burden associated with the module, we consider\\na comparison between ResNet-50 and SE-ResNet-50 as an\\nexample. ResNet-50 requires \\u223c3.86 GFLOPs in a single\\nforward pass for a 224 \\u00d7 224 pixel input image. Each SE\\nblock makes use of a global average pooling operation in\\nSE-ResNet Module\\nGlobal pooling\\nResNet Module\\nFig. 3. The schema of the original Residual module (left) and the SE-\\nResNet module (right).\\nthe squeeze phase and two small FC layers in the excitation\\nphase, followed by an inexpensive channel-wise scaling\\noperation. In the aggregate, when setting the reduction ratio\\nr (introduced in Section 3.2) to 16, SE-ResNet-50 requires\\n\\u223c3.87 GFLOPs, corresponding to a 0.26% relative increase\\nover the original ResNet-50. In exchange for this slight additional computational burden, the accuracy of SE-ResNet-50\\nsurpasses that of ResNet-50 and indeed, approaches that\\nof a deeper ResNet-101 network requiring \\u223c7.58 GFLOPs\\n(Table 2).\\nIn practical terms, a single pass forwards and backwards\\nthrough ResNet-50 takes 190 ms, compared to 209 ms for\\nSE-ResNet-50 with a training minibatch of 256 images (both\\ntimings are performed on a server with 8 NVIDIA Titan X\\nGPUs). We suggest that this represents a reasonable runtime\\noverhead, which may be further reduced as global pooling\\nand small inner-product operations receive further optimisation in popular GPU libraries. Due to its importance\\nfor embedded device applications, we further benchmark\\nCPU inference time for each model: for a 224 \\u00d7 224 pixel\\ninput image, ResNet-50 takes 164 ms in comparison to 167\\nms for SE-ResNet-50. We believe that the small additional\\ncomputational cost incurred by the SE block is justi\\ufb01ed by\\nits contribution to model performance.\\nWe next consider the additional parameters introduced\\nby the proposed SE block. These additional parameters\\nresult solely from the two FC layers of the gating mechanism\\nand therefore constitute a small fraction of the total network\\ncapacity. Concretely, the total number introduced by the\\nweight parameters of these FC layers is given by:\\nwhere r denotes the reduction ratio, S refers to the number\\nof stages (a stage refers to the collection of blocks operating on feature maps of a common spatial dimension), Cs\\ndenotes the dimension of the output channels and Ns denotes the number of repeated blocks for stage s (when bias\\nterms are used in FC layers, the introduced parameters and\\ncomputational cost are typically negligible). SE-ResNet-50\\nintroduces \\u223c2.5 million additional parameters beyond the\\n(Left) ResNet-50 . (Middle) SE-ResNet-50. (Right) SE-ResNeXt-50 with a 32\\u00d74d template. The shapes and operations with speci\\ufb01c parameter\\nsettings of a residual building block are listed inside the brackets and the number of stacked blocks in a stage is presented outside. The inner\\nbrackets following by fc indicates the output dimension of the two fully connected layers in an SE module.\\nOutput size\\nSE-ResNet-50\\nSE-ResNeXt-50 (32 \\u00d7 4d)\\nconv, 7 \\u00d7 7, 64, stride 2\\nmax pool, 3 \\u00d7 3, stride 2\\nconv, 1 \\u00d7 1, 64\\nconv, 3 \\u00d7 3, 64\\nconv, 1 \\u00d7 1, 256\\nconv, 1 \\u00d7 1, 64\\nconv, 3 \\u00d7 3, 64\\nconv, 1 \\u00d7 1, 256\\nfc, \\nconv, 1 \\u00d7 1, 128\\nconv, 3 \\u00d7 3, 128\\nconv, 1 \\u00d7 1, 256\\nfc, \\nconv, 1 \\u00d7 1, 128\\nconv, 3 \\u00d7 3, 128\\nconv, 1 \\u00d7 1, 512\\nconv, 1 \\u00d7 1, 128\\nconv, 3 \\u00d7 3, 128\\nconv, 1 \\u00d7 1, 512\\nfc, \\nconv, 1 \\u00d7 1, 256\\nconv, 3 \\u00d7 3, 256\\nconv, 1 \\u00d7 1, 512\\nfc, \\nconv, 1 \\u00d7 1, 256\\nconv, 3 \\u00d7 3, 256\\nconv, 1 \\u00d7 1, 1024\\nconv, 1 \\u00d7 1, 256\\nconv, 3 \\u00d7 3, 256\\nconv, 1 \\u00d7 1, 1024\\nfc, \\nconv, 1 \\u00d7 1, 512\\nconv, 3 \\u00d7 3, 512\\nconv, 1 \\u00d7 1, 1024\\nfc, \\nconv, 1 \\u00d7 1, 512\\nconv, 3 \\u00d7 3, 512\\nconv, 1 \\u00d7 1, 2048\\nconv, 1 \\u00d7 1, 512\\nconv, 3 \\u00d7 3, 512\\nconv, 1 \\u00d7 1, 2048\\nfc, \\nconv, 1 \\u00d7 1, 1024\\nconv, 3 \\u00d7 3, 1024\\nconv, 1 \\u00d7 1, 2048\\nfc, \\nglobal average pool, 1000-d fc, softmax\\nSingle-crop error rates (%) on the ImageNet validation set and complexity comparisons. The original column refers to the results reported in the\\noriginal papers (the results of ResNets are obtained from the website: To enable a fair\\ncomparison, we re-train the baseline models and report the scores in the re-implementation column. The SENet column refers to the\\ncorresponding architectures in which SE blocks have been added. The numbers in brackets denote the performance improvement over the\\nre-implemented baselines. \\u2020 indicates that the model has been evaluated on the non-blacklisted subset of the validation set (this is discussed in\\nmore detail in ), which may slightly improve results. VGG-16 and SE-VGG-16 are trained with batch normalization.\\nre-implementation\\ntop-1 err.\\ntop-5 err.\\ntop-1 err.\\ntop-5 err.\\ntop-1 err.\\ntop-5 err.\\nResNet-50 \\n23.29(1.51)\\n6.62(0.86)\\nResNet-101 \\n22.38(0.79)\\n6.07(0.45)\\nResNet-152 \\n21.57(0.85)\\n5.73(0.61)\\nResNeXt-50 \\n21.10(1.01)\\n5.49(0.41)\\nResNeXt-101 \\n20.70(0.48)\\n5.01(0.56)\\nVGG-16 \\n25.22(1.80)\\n7.70(1.11)\\nBN-Inception \\n24.23(1.15)\\n7.14(0.75)\\nInception-ResNet-v2 \\n19.80(0.57)\\n4.79(0.42)\\n\\u223c25 million parameters required by ResNet-50, corresponding to a \\u223c10% increase. In practice, the majority of these\\nparameters come from the \\ufb01nal stage of the network, where\\nthe excitation operation is performed across the greatest\\nnumber of channels. However, we found that this comparatively costly \\ufb01nal stage of SE blocks could be removed at\\nonly a small cost in performance (<0.1% top-5 error on\\nImageNet) reducing the relative parameter increase to \\u223c4%,\\nwhich may prove useful in cases where parameter usage\\nis a key consideration (see Section 6.4 and 7.2 for further\\ndiscussion).\\nEXPERIMENTS\\nIn this section, we conduct experiments to investigate the\\neffectiveness of SE blocks across a range of tasks, datasets\\nand model architectures.\\nImage Classi\\ufb01cation\\nTo evaluate the in\\ufb02uence of SE blocks, we \\ufb01rst perform\\nexperiments on the ImageNet 2012 dataset which\\ncomprises 1.28 million training images and 50K validation\\nimages from 1000 different classes. We train networks on\\nthe training set and report the top-1 and top-5 error on the\\nvalidation set.\\nEach baseline network architecture and its corresponding SE counterpart are trained with identical optimisation\\nschemes. We follow standard practices and perform data\\naugmentation with random cropping using scale and aspect ratio to a size of 224 \\u00d7 224 pixels (or 299 \\u00d7 299\\nfor Inception-ResNet-v2 and SE-Inception-ResNet-v2)\\nand perform random horizontal \\ufb02ipping. Each input image is normalised through mean RGB-channel subtraction.\\nAll models are trained on our distributed learning system\\nROCS which is designed to handle ef\\ufb01cient parallel training\\nof large networks. Optimisation is performed using synchronous SGD with momentum 0.9 and a minibatch size\\nof 1024. The initial learning rate is set to 0.6 and decreased\\nby a factor of 10 every 30 epochs. Models are trained for 100\\nepochs from scratch, using the weight initialisation strategy\\ndescribed in . The reduction ratio r (in Section 3.2) is set\\nto 16 by default (except where stated otherwise).\\nWhen evaluating the models we apply centre-cropping\\nso that 224 \\u00d7 224 pixels are cropped from each image, after\\nSingle-crop error rates (%) on the ImageNet validation set and complexity comparisons. MobileNet refers to \\u201c1.0 MobileNet-224\\u201d in and\\nShuf\\ufb02eNet refers to \\u201cShuf\\ufb02eNet 1 \\u00d7 (g = 3)\\u201d in . The numbers in brackets denote the performance improvement over the re-implementation.\\nre-implementation\\ntop-1 err.\\ntop-5 err.\\ntop-1 err.\\ntop-5 err.\\ntop-1 err.\\ntop-5 err.\\nMobileNet \\nShuf\\ufb02eNet \\nFig. 4. Training baseline architectures and their SENet counterparts on ImageNet. SENets exhibit improved optimisation characteristics and produce\\nconsistent gains in performance which are sustained throughout the training process.\\nits shorter edge is \\ufb01rst resized to 256 (299 \\u00d7 299 from\\neach image whose shorter edge is \\ufb01rst resized to 352 for\\nInception-ResNet-v2 and SE-Inception-ResNet-v2).\\nNetwork depth. We begin by comparing SE-ResNet against\\nResNet architectures with different depths and report the\\nresults in Table 2. We observe that SE blocks consistently\\nimprove performance across different depths with an extremely small increase in computational complexity. Remarkably, SE-ResNet-50 achieves a single-crop top-5 validation error of 6.62%, exceeding ResNet-50 (7.48%) by 0.86%\\nand approaching the performance achieved by the much\\ndeeper ResNet-101 network (6.52% top-5 error) with only\\nhalf of the total computational burden (3.87 GFLOPs vs.\\n7.58 GFLOPs). This pattern is repeated at greater depth,\\nwhere SE-ResNet-101 (6.07% top-5 error) not only matches,\\nbut outperforms the deeper ResNet-152 network (6.34%\\ntop-5 error) by 0.27%. While it should be noted that the SE\\nblocks themselves add depth, they do so in an extremely\\ncomputationally ef\\ufb01cient manner and yield good returns\\neven at the point at which extending the depth of the base\\narchitecture achieves diminishing returns. Moreover, we see\\nthat the gains are consistent across a range of different\\nnetwork depths, suggesting that the improvements induced\\nby SE blocks may be complementary to those obtained by\\nsimply increasing the depth of the base architecture.\\nIntegration with modern architectures. We next study the\\neffect of integrating SE blocks with two further state-ofthe-art architectures, Inception-ResNet-v2 and ResNeXt\\n(using the setting of 32 \\u00d7 4d) , both of which introduce\\nadditional computational building blocks into the base network. We construct SENet equivalents of these networks,\\nSE-Inception-ResNet-v2 and SE-ResNeXt (the con\\ufb01guration\\nof SE-ResNeXt-50 is given in Table 1) and report results\\nin Table 2. As with the previous experiments, we observe\\nsigni\\ufb01cant performance improvements induced by the introduction of SE blocks into both architectures. In particular, SE-ResNeXt-50 has a top-5 error of 5.49% which is\\nsuperior to both its direct counterpart ResNeXt-50 (5.90%\\ntop-5 error) as well as the deeper ResNeXt-101 (5.57% top-5\\nerror), a model which has almost twice the total number of\\nparameters and computational overhead. We note a slight\\ndifference in performance between our re-implementation\\nof Inception-ResNet-v2 and the result reported in .\\nHowever, we observe a similar trend with regard to the\\neffect of SE blocks, \\ufb01nding that SE counterpart (4.79% top-5\\nerror) outperforms our reimplemented Inception-ResNet-v2\\nbaseline (5.21% top-5 error) by 0.42% as well as the reported\\nresult in .\\nWe also assess the effect of SE blocks when operating on\\nnon-residual networks by conducting experiments with the\\nVGG-16 and BN-Inception architecture . To facilitate\\nthe training of VGG-16 from scratch, we add Batch Normalization layers after each convolution. We use identical training schemes for both VGG-16 and SE-VGG-16. The results of\\nthe comparison are shown in Table 2. Similarly to the results\\nreported for the residual baseline architectures, we observe\\nthat SE blocks bring improvements in performance on the\\nnon-residual settings.\\nTo provide some insight into in\\ufb02uence of SE blocks on\\nthe optimisation of these models, example training curves\\nfor runs of the baseline architectures and their respective\\nSE counterparts are depicted in Fig. 4. We observe that SE\\nblocks yield a steady improvement throughout the optimisation procedure. Moreover, this trend is fairly consistent\\nacross a range of network architectures considered as baselines.\\nMobile setting. Finally, we consider two representative\\narchitectures from the class of mobile-optimised networks,\\nMobileNet and Shuf\\ufb02eNet . For these experiments,\\nwe used a minibatch size of 256 and slightly less aggressive\\ndata augmentation and regularisation as in . We trained\\nthe models across 8 GPUs using SGD with momentum (set\\nto 0.9) and an initial learning rate of 0.1 which was reduced\\nby a factor of 10 each time the validation loss plateaued. The\\ntotal training process required \\u223c400 epochs (enabling us\\nto reproduce the baseline performance of ). The results\\nreported in Table 3 show that SE blocks consistently improve\\nClassi\\ufb01cation error (%) on CIFAR-10.\\nResNet-110 \\nResNet-164 \\nWRN-16-8 \\nShake-Shake 26 2x96d + Cutout \\nClassi\\ufb01cation error (%) on CIFAR-100.\\nResNet-110 \\nResNet-164 \\nWRN-16-8 \\nShake-Even 29 2x4x64d + Cutout \\nthe accuracy by a large margin at a minimal increase in\\ncomputational cost.\\nAdditional datasets. We next investigate whether the bene-\\n\\ufb01ts of SE blocks generalise to datasets beyond ImageNet. We\\nperform experiments with several popular baseline architectures and techniques (ResNet-110 , ResNet-164 ,\\nWideResNet-16-8 , Shake-Shake and Cutout ) on\\nthe CIFAR-10 and CIFAR-100 datasets . These comprise\\na collection of 50k training and 10k test 32 \\u00d7 32 pixel RGB\\nimages, labelled with 10 and 100 classes respectively. The integration of SE blocks into these networks follows the same\\napproach that was described in Section 3.3. Each baseline\\nand its SENet counterpart are trained with standard data\\naugmentation strategies , . During training, images\\nare randomly horizontally \\ufb02ipped and zero-padded on each\\nside with four pixels before taking a random 32 \\u00d7 32 crop.\\nMean and standard deviation normalisation is also applied.\\nThe setting of the training hyperparameters (e.g. minibatch\\nsize, initial learning rate, weight decay) match those suggested by the original papers. We report the performance\\nof each baseline and its SENet counterpart on CIFAR-10\\nin Table 4 and performance on CIFAR-100 in Table 5. We\\nobserve that in every comparison SENets outperform the\\nbaseline architectures, suggesting that the bene\\ufb01ts of SE\\nblocks are not con\\ufb01ned to the ImageNet dataset.\\nScene Classi\\ufb01cation\\nWe also conduct experiments on the Places365-Challenge\\ndataset for scene classi\\ufb01cation. This dataset comprises\\n8 million training images and 36, 500 validation images\\nacross 365 categories. Relative to classi\\ufb01cation, the task of\\nscene understanding offers an alternative assessment of a\\nmodel\\u2019s ability to generalise well and handle abstraction.\\nThis is because it often requires the model to handle more\\ncomplex data associations and to be robust to a greater level\\nof appearance variation.\\nWe opted to use ResNet-152 as a strong baseline to\\nassess the effectiveness of SE blocks and follow the training\\nand evaluation protocols described in , . In these\\nexperiments, models are trained from scratch. We report\\nthe results in Table 6, comparing also with prior work. We\\nobserve that SE-ResNet-152 (11.01% top-5 error) achieves a\\nlower validation error than ResNet-152 (11.61% top-5 error),\\nSingle-crop error rates (%) on Places365 validation set.\\ntop-1 err.\\ntop-5 err.\\nPlaces-365-CNN \\nResNet-152 (ours)\\nSE-ResNet-152\\nFaster R-CNN object detection results (%) on COCO minival set.\\nAP@IoU=0.5\\nSE-ResNet-50\\nResNet-101\\nSE-ResNet-101\\nproviding evidence that SE blocks can also yield improvements for scene classi\\ufb01cation. This SENet surpasses the\\nprevious state-of-the-art model Places-365-CNN which\\nhas a top-5 error of 11.48% on this task.\\nObject Detection on COCO\\nWe further assess the generalisation of SE blocks on the\\ntask of object detection using the COCO dataset . As\\nin previous work , we use the minival protocol, i.e.,\\ntraining the models on the union of the 80k training set\\nand a 35k val subset and evaluating on the remaining\\n5k val subset. Weights are initialised by the parameters\\nof the model trained on the ImageNet dataset. We use\\nthe Faster R-CNN detection framework as the basis\\nfor evaluating our models and follow the hyperparameter\\nsetting described in (i.e., end-to-end training with the\\n\\u20192x\\u2019 learning schedule). Our goal is to evaluate the effect\\nof replacing the trunk architecture (ResNet) in the object\\ndetector with SE-ResNet, so that any changes in performance can be attributed to better representations. Table 7\\nreports the validation set performance of the object detector\\nusing ResNet-50, ResNet-101 and their SE counterparts as\\ntrunk architectures. SE-ResNet-50 outperforms ResNet-50\\nby 2.4% (a relative 6.3% improvement) on COCO\\u2019s standard AP metric and by 3.1% on AP@IoU=0.5. SE blocks\\nalso bene\\ufb01t the deeper ResNet-101 architecture achieving\\na 2.0% improvement (5.0% relative improvement) on the\\nAP metric. In summary, this set of experiments demonstrate\\nthe generalisability of SE blocks. The induced improvements\\ncan be realised across a broad range of architectures, tasks\\nand datasets.\\nILSVRC 2017 Classi\\ufb01cation Competition\\nSENets formed the foundation of our submission to the\\nILSVRC competition where we achieved \\ufb01rst place. Our\\nwinning entry comprised a small ensemble of SENets that\\nemployed a standard multi-scale and multi-crop fusion\\nstrategy to obtain a top-5 error of 2.251% on the test set.\\nAs part of this submission, we constructed an additional\\nmodel, SENet-154, by integrating SE blocks with a modi\\ufb01ed\\nResNeXt (the details of the architecture are provided\\nin Appendix). We compare this model with prior work on\\nthe ImageNet validation set in Table 8 using standard crop\\nSingle-crop error rates (%) of state-of-the-art CNNs on ImageNet\\nvalidation set with crop sizes 224 \\u00d7 224 and 320 \\u00d7 320 / 299 \\u00d7 299.\\n320 \\u00d7 320 /\\nResNet-152 \\nResNet-200 \\nInception-v3 \\nInception-v4 \\nInception-ResNet-v2 \\nResNeXt-101 (64 \\u00d7 4d) \\nDenseNet-264 \\nAttention-92 \\nPyramidNet-200 \\nDPN-131 \\nComparison (%) with state-of-the-art CNNs on ImageNet validation set\\nusing larger crop sizes/additional training data. \\u2020This model was\\ntrained with a crop size of 320 \\u00d7 320.\\nVery Deep PolyNet \\nNASNet-A (6 @ 4032) \\nPNASNet-5 (N=4,F=216) \\nSENet-154\\u2020\\nAmoebaNet-C \\nResNeXt-101 32 \\u00d7 48d \\nsizes (224 \\u00d7 224 and 320 \\u00d7 320). We observe that SENet-154\\nachieves a top-1 error of 18.68% and a top-5 error of 4.47%\\nusing a 224 \\u00d7 224 centre crop evaluation, which represents\\nthe strongest reported result.\\nFollowing the challenge there has been a great deal of\\nfurther progress on the ImageNet benchmark. For comparison, we include the strongest results that we are currently\\naware of in Table 9. The best performance using only ImageNet data was recently reported by . This method\\nuses reinforcement learning to develop new policies for\\ndata augmentation during training to improve the performance of the architecture searched by . The best overall\\nperformance was reported by using a ResNeXt-101\\n32\\u00d748d architecture. This was achieved by pretraining their\\nmodel on approximately one billion weakly labelled images\\nand \\ufb01netuning on ImageNet. The improvements yielded by\\nmore sophisticated data augmentation and extensive\\npretraining may be complementary to our proposed\\nchanges to the network architecture.\\nABLATION STUDY\\nIn this section we conduct ablation experiments to gain a\\nbetter understanding of the effect of using different con-\\n\\ufb01gurations on components of the SE blocks. All ablation\\nexperiments are performed on the ImageNet dataset on a\\nsingle machine (with 8 GPUs). ResNet-50 is used as the\\nbackbone architecture. We found empirically that on ResNet\\narchitectures, removing the biases of the FC layers in the\\nexcitation operation facilitates the modelling of channel\\ndependencies, and use this con\\ufb01guration in the following\\nSingle-crop error rates (%) on ImageNet and parameter sizes for\\nSE-ResNet-50 at different reduction ratios. Here, original refers to\\nResNet-50.\\ntop-1 err.\\ntop-5 err.\\nexperiments. The data augmentation strategy follows the\\napproach described in Section 5.1. To allow us to study the\\nupper limit of performance for each variant, the learning\\nrate is initialised to 0.1 and training continues until the\\nvalidation loss plateaus2 (\\u223c300 epochs in total). The learning rate is then reduced by a factor of 10 and then this\\nprocess is repeated (three times in total). Label-smoothing\\nregularisation is used during training.\\nReduction ratio\\nThe reduction ratio r introduced in Eqn. 5 is a hyperparameter which allows us to vary the capacity and computational cost of the SE blocks in the network. To investigate\\nthe trade-off between performance and computational cost\\nmediated by this hyperparameter, we conduct experiments\\nwith SE-ResNet-50 for a range of different r values. The\\ncomparison in Table 10 shows that performance is robust to\\na range of reduction ratios. Increased complexity does not\\nimprove performance monotonically while a smaller ratio\\ndramatically increases the parameter size of the model. Setting r = 16 achieves a good balance between accuracy and\\ncomplexity. In practice, using an identical ratio throughout\\na network may not be optimal (due to the distinct roles\\nperformed by different layers), so further improvements\\nmay be achievable by tuning the ratios to meet the needs\\nof a given base architecture.\\nSqueeze Operator\\nWe examine the signi\\ufb01cance of using global average pooling\\nas opposed to global max pooling as our choice of squeeze\\noperator (since this worked well, we did not consider more\\nsophisticated alternatives). The results are reported in Table 11. While both max and average pooling are effective,\\naverage pooling achieves slightly better performance, justifying its selection as the basis of the squeeze operation.\\nHowever, we note that the performance of SE blocks is fairly\\nrobust to the choice of speci\\ufb01c aggregation operator.\\nExcitation Operator\\nWe next assess the choice of non-linearity for the excitation\\nmechanism. We consider two further options: ReLU and\\ntanh, and experiment with replacing the sigmoid with these\\n2. For reference, training with a 270 epoch \\ufb01xed schedule (reducing\\nthe learning rate at 125, 200 and 250 epochs) achieves top-1 and top-5\\nerror rates for ResNet-50 and SE-ResNet-50 of (23.21%, 6.53%) and\\n(22.20%, 6.00%) respectively.\\nEffect of using different squeeze operators in SE-ResNet-50 on\\nImageNet (error rates %).\\ntop-1 err.\\ntop-5 err.\\nEffect of using different non-linearities for the excitation operator in\\nSE-ResNet-50 on ImageNet (error rates %).\\nExcitation\\ntop-1 err.\\ntop-5 err.\\nalternative non-linearities. The results are reported in Table 12. We see that exchanging the sigmoid for tanh slightly\\nworsens performance, while using ReLU is dramatically\\nworse and in fact causes the performance of SE-ResNet-50\\nto drop below that of the ResNet-50 baseline. This suggests\\nthat for the SE block to be effective, careful construction of\\nthe excitation operator is important.\\nDifferent stages\\nWe explore the in\\ufb02uence of SE blocks at different stages by\\nintegrating SE blocks into ResNet-50, one stage at a time.\\nSpeci\\ufb01cally, we add SE blocks to the intermediate stages:\\nstage 2, stage 3 and stage 4, and report the results in Table 13. We observe that SE blocks bring performance bene\\ufb01ts\\nwhen introduced at each of these stages of the architecture.\\nMoreover, the gains induced by SE blocks at different stages\\nare complementary, in the sense that they can be combined\\neffectively to further bolster network performance.\\nIntegration strategy\\nFinally, we perform an ablation study to assess the in\\ufb02uence\\nof the location of the SE block when integrating it into existing architectures. In addition to the proposed SE design, we\\nconsider three variants: (1) SE-PRE block, in which the SE\\nblock is moved before the residual unit; (2) SE-POST block,\\nin which the SE unit is moved after the summation with\\nthe identity branch (after ReLU) and (3) SE-Identity block,\\nin which the SE unit is placed on the identity connection in\\nparallel to the residual unit. These variants are illustrated\\nin Figure 5 and the performance of each variant is reported\\nin Table 14. We observe that the SE-PRE, SE-Identity and\\nproposed SE block each perform similarly well, while usage\\nEffect of integrating SE blocks with ResNet-50 at different stages on\\nImageNet (error rates %).\\ntop-1 err.\\ntop-5 err.\\nSE Stage 2\\nSE Stage 3\\nSE Stage 4\\nEffect of different SE block integration strategies with ResNet-50 on\\nImageNet (error rates %).\\ntop-1 err.\\ntop-5 err.\\nSE-Identity\\nEffect of integrating SE blocks at the 3x3 convolutional layer of each\\nresidual branch in ResNet-50 on ImageNet (error rates %).\\ntop-1 err.\\ntop-5 err.\\nof the SE-POST block leads to a drop in performance. This\\nexperiment suggests that the performance improvements\\nproduced by SE units are fairly robust to their location,\\nprovided that they are applied prior to branch aggregation.\\nIn the experiments above, each SE block was placed\\noutside the structure of a residual unit. We also construct\\na variant of the design which moves the SE block inside\\nthe residual unit, placing it directly after the 3 \\u00d7 3 convolutional layer. Since the 3 \\u00d7 3 convolutional layer possesses\\nfewer channels, the number of parameters introduced by the\\ncorresponding SE block is also reduced. The comparison in\\nTable 15 shows that the SE 3\\u00d73 variant achieves comparable\\nclassi\\ufb01cation accuracy with fewer parameters than the standard SE block. Although it is beyond the scope of this work,\\nwe anticipate that further ef\\ufb01ciency gains will be achievable\\nby tailoring SE block usage for speci\\ufb01c architectures.\\nROLE OF SE BLOCKS\\nAlthough the proposed SE block has been shown to improve network performance on multiple visual tasks, we\\nwould also like to understand the relative importance of\\nthe squeeze operation and how the excitation mechanism\\noperates in practice. A rigorous theoretical analysis of the\\nrepresentations learned by deep neural networks remains\\nchallenging, we therefore take an empirical approach to\\nexamining the role played by the SE block with the goal of\\nattaining at least a primitive understanding of its practical\\nEffect of Squeeze\\nTo assess whether the global embedding produced by the\\nsqueeze operation plays an important role in performance,\\nwe experiment with a variant of the SE block that adds an\\nequal number of parameters, but does not perform global\\naverage pooling. Speci\\ufb01cally, we remove the pooling operation and replace the two FC layers with corresponding\\n1 \\u00d7 1 convolutions with identical channel dimensions in\\nthe excitation operator, namely NoSqueeze, where the excitation output maintains the spatial dimensions as input.\\nIn contrast to the SE block, these point-wise convolutions\\ncan only remap the channels as a function of the output\\nof a local operator. While in practice, the later layers of a\\ndeep network will typically possess a (theoretical) global\\n(a) Residual block\\n(b) Standard SE block\\n(c) SE-PRE block\\n(d) SE-POST block\\n(e) SE-Identity block\\nFig. 5. SE block integration designs explored in the ablation study.\\n(a) SE_2_3\\n(b) SE_3_4\\n(c) SE_4_6\\n(d) SE_5_1\\n(e) SE_5_2\\n(f) SE_5_3\\nFig. 6. Activations induced by the Excitation operator at different depths in the SE-ResNet-50 on ImageNet. Each set of activations is named\\naccording to the following scheme: SE_stageID_blockID. With the exception of the unusual behaviour at SE_5_2, the activations become\\nincreasingly class-speci\\ufb01c with increasing depth.\\nEffect of Squeeze operator on ImageNet (error rates %).\\ntop-1 err.\\ntop-5 err.\\nreceptive \\ufb01eld, global embeddings are no longer directly\\naccessible throughout the network in the NoSqueeze variant.\\nThe accuracy and computational complexity of both models\\nare compared to a standard ResNet-50 model in Table 16. We\\nobserve that the use of global information has a signi\\ufb01cant\\nin\\ufb02uence on the model performance, underlining the importance of the squeeze operation. Moreover, in comparison\\nto the NoSqueeze design, the SE block allows this global\\ninformation to be used in a computationally parsimonious\\nRole of Excitation\\nTo provide a clearer picture of the function of the excitation\\noperator in SE blocks, in this section we study example\\nactivations from the SE-ResNet-50 model and examine their\\ndistribution with respect to different classes and different\\ninput images at various depths in the network. In particular,\\nwe would like to understand how excitations vary across\\nimages of different classes, and across images within a class.\\nWe \\ufb01rst consider the distribution of excitations for different classes. Speci\\ufb01cally, we sample four classes from the\\nImageNet dataset that exhibit semantic and appearance diversity, namely gold\\ufb01sh, pug, plane and cliff (example images\\nfrom these classes are shown in Appendix). We then draw\\n\\ufb01fty samples for each class from the validation set and\\ncompute the average activations for \\ufb01fty uniformly sampled\\nchannels in the last SE block of each stage (immediately\\nprior to downsampling) and plot their distribution in Fig. 6.\\nFor reference, we also plot the distribution of the mean\\nactivations across all of the 1000 classes.\\nWe make the following three observations about the\\nrole of the excitation operation. First, the distribution across\\ndifferent classes is very similar at the earlier layers of the\\nnetwork, e.g. SE 2 3. This suggests that the importance of\\nfeature channels is likely to be shared by different classes in\\nthe early stages. The second observation is that at greater\\n(a) SE_2_3\\n(b) SE_3_4\\n(c) SE_4_6\\n(d) SE_5_1\\n(e) SE_5_2\\n(f) SE_5_3\\nFig. 7. Activations induced by Excitation in the different modules of SE-ResNet-50 on image samples from the gold\\ufb01sh and plane classes of\\nImageNet. The module is named \\u201cSE_stageID_blockID\\u201d.\\ndepth, the value of each channel becomes much more\\nclass-speci\\ufb01c as different classes exhibit different preferences to the discriminative value of features, e.g. SE 4 6 and\\nSE 5 1. These observations are consistent with \\ufb01ndings in\\nprevious work , , namely that earlier layer features\\nare typically more general (e.g. class agnostic in the context\\nof the classi\\ufb01cation task) while later layer features exhibit\\ngreater levels of speci\\ufb01city .\\nNext, we observe a somewhat different phenomena in\\nthe last stage of the network. SE 5 2 exhibits an interesting\\ntendency towards a saturated state in which most of the\\nactivations are close to one. At the point at which all\\nactivations take the value one, an SE block reduces to the\\nidentity operator. At the end of the network in the SE 5 3\\n(which is immediately followed by global pooling prior\\nbefore classi\\ufb01ers), a similar pattern emerges over different\\nclasses, up to a modest change in scale (which could be\\ntuned by the classi\\ufb01ers). This suggests that SE 5 2 and\\nSE 5 3 are less important than previous blocks in providing\\nrecalibration to the network. This \\ufb01nding is consistent with\\nthe result of the empirical investigation in Section 4 which\\ndemonstrated that the additional parameter count could be\\nsigni\\ufb01cantly reduced by removing the SE blocks for the last\\nstage with only a marginal loss of performance.\\nFinally, we show the mean and standard deviations of\\nthe activations for image instances within the same class\\nfor two sample classes (gold\\ufb01sh and plane) in Fig. 7. We\\nobserve a trend consistent with the inter-class visualisation,\\nindicating that the dynamic behaviour of SE blocks varies\\nover both classes and instances within a class. Particularly\\nin the later layers of the network where there is considerable diversity of representation within a single class, the\\nnetwork learns to take advantage of feature recalibration to\\nimprove its discriminative performance . In summary,\\nSE blocks produce instance-speci\\ufb01c responses which nevertheless function to support the increasingly class-speci\\ufb01c\\nneeds of the model at different layers in the architecture.\\nCONCLUSION\\nIn this paper we proposed the SE block, an architectural\\nunit designed to improve the representational power of a\\nnetwork by enabling it to perform dynamic channel-wise\\nfeature recalibration. A wide range of experiments show\\nthe effectiveness of SENets, which achieve state-of-the-art\\nperformance across multiple datasets and tasks. In addition,\\nSE blocks shed some light on the inability of previous\\narchitectures to adequately model channel-wise feature dependencies. We hope this insight may prove useful for other\\ntasks requiring strong discriminative features. Finally, the\\nfeature importance values produced by SE blocks may be\\nof use for other tasks such as network pruning for model\\ncompression.\\nACKNOWLEDGMENTS\\nThe authors would like to thank Chao Li and Guangyuan\\nWang from Momenta for their contributions in the training\\nsystem optimisation and experiments on CIFAR dataset. We\\nwould also like to thank Andrew Zisserman, Aravindh Mahendran and Andrea Vedaldi for many helpful discussions.\\nThe work is supported in part by NSFC Grants (61632003,\\n61620106003, 61672502, 61571439), National Key R&D Program of China (2017YFB1002701), and Macao FDCT Grant\\n . Samuel Albanie is supported by EPSRC\\nAIMS CDT EP/L015897/1.\\nAPPENDIX: DETAILS OF SENET-154\\nSENet-154 is constructed by incorporating SE blocks into a\\nmodi\\ufb01ed version of the 64\\u00d74d ResNeXt-152 which extends\\n(a) gold\\ufb01sh\\nFig. 8. Sample images from the four classes of ImageNet used in the\\nexperiments described in Sec. 7.2.\\nthe original ResNeXt-101 by adopting the block stacking strategy of ResNet-152 . Further differences to the\\ndesign and training of this model (beyond the use of SE\\nblocks) are as follows: (a) The number of the \\ufb01rst 1 \\u00d7 1\\nconvolutional channels for each bottleneck building block\\nwas halved to reduce the computational cost of the model\\nwith a minimal decrease in performance. (b) The \\ufb01rst 7 \\u00d7 7\\nconvolutional layer was replaced with three consecutive\\n3 \\u00d7 3 convolutional layers. (c) The 1 \\u00d7 1 down-sampling\\nprojection with stride-2 convolution was replaced with a\\n3 \\u00d7 3 stride-2 convolution to preserve information. (d) A\\ndropout layer (with a dropout ratio of 0.2) was inserted\\nbefore the classi\\ufb01cation layer to reduce over\\ufb01tting. (e) Labelsmoothing regularisation (as introduced in ) was used\\nduring training. (f) The parameters of all BN layers were\\nfrozen for the last few training epochs to ensure consistency\\nbetween training and testing. (g) Training was performed\\nwith 8 servers (64 GPUs) in parallel to enable large batch\\nsizes . The initial learning rate was set to 1.0.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"referenced_paper_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"p1129\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 95
        }
      ],
      "source": [
        "df_cln = (\n",
        "    df_precln[df_precln[\"is_referenced\"] == 1]\n",
        "      .sample(n=1, random_state=1)\n",
        ")\n",
        "df_ans = df_cln[\"is_referenced\"]\n",
        "df_ans = df_cln[\"is_referenced\"].rename(\"citation_true\")\n",
        "df_cln = df_cln.drop(\"is_referenced\", axis=1)\n",
        "df_cln.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = {\n",
        "    \"top_k\": 1,\n",
        "    \"top_p\": 1.0,\n",
        "    \"max_tokens\": 1,\n",
        "    \"min_tokens\": 1,\n",
        "    \"random_seed\": 42,\n",
        "    \"repetition_penalty\": 1.2,\n",
        "    \"stopping_criteria\": \"sequence\",\n",
        "    \"stopping_sequence\": [\"\\n\"]\n",
        "}\n",
        "\n",
        "def predict_citation(paper_text, ref_text, df_ex):\n",
        "    example_pos = df_ex.iloc[0]\n",
        "    example_neg = df_ex.iloc[1]\n",
        "    prompt = f\"\"\"\n",
        "    Task: Determine if the paper_task cites the referenced_paper_task.\n",
        "    Definition: \"Cite\" means the paper includes the referenced paper in its bibliography or reference list.\n",
        "    It is not enough to just mention similar topics; the referenced paper must appear as a formal citation.\n",
        "\n",
        "    Output rules:\n",
        "    - Return only '1' if cited.\n",
        "    - Return only '0' if not cited.\n",
        "    - Do not write anything else (no words, no explanation, no sentences).\n",
        "    - The answer must be exactly one character: '1' or '0'.\n",
        "\n",
        "    Example:\n",
        "    Paper: {example_pos['paper']}\n",
        "    Referenced Paper: {example_pos['referenced_paper']}\n",
        "    Answer: paper cites referenced_paper\n",
        "\n",
        "    Example:\n",
        "    Paper: {example_neg['paper']}\n",
        "    Referenced Paper: {example_neg['referenced_paper']}\n",
        "    Answer: paper does not cites referenced_paper\n",
        "\n",
        "    paper_task: {paper_text}\n",
        "    referenced_paper_task: {ref_text}\n",
        "    Final Answer (strictly '1' or '0' only):\n",
        "    \"\"\"\n",
        "    response = output.invoke(prompt, parameters=parameters)\n",
        "    response = response.strip().replace(\"'\", \"\").replace('\"', \"\")\n",
        "    print(\"RAW:\", response)\n",
        "\n",
        "    if response.startswith(\"1\"):\n",
        "        return \"1\"\n",
        "    elif response.startswith(\"0\"):\n",
        "        return \"0\"\n",
        "    else:\n",
        "        return \"0\"\n",
        "\n",
        "df_cln[\"citation_pred\"] = df_cln.apply(\n",
        "    lambda row: predict_citation(row[\"paper\"], row[\"referenced_paper\"], df_ex),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "df_cln_pred = df_cln[\"citation_pred\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P366o2PZHeqZ",
        "outputId": "9a5e17a9-2935-45a5-ba32-bd7137523995"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAW: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_cln_pred.head(), df_ans.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-zWa74SbnP6",
        "outputId": "f1c61dc5-1497-4ba2-a68b-e6a9c6a150e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(237922    0\n",
              " Name: citation_pred, dtype: object,\n",
              " 237922    1\n",
              " Name: citation_true, dtype: int64)"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#METRIK EVALUASI"
      ],
      "metadata": {
        "id": "esIF5bBMNAQ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hasil prediksi sitasi menggunakan IBM Granite 3.3 8b Instruct"
      ],
      "metadata": {
        "id": "UEMPgyUpNI-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "k0cHNOpVNDTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = [0,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1]\n",
        "y_true = [0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1]"
      ],
      "metadata": {
        "id": "Y3dqDs3ENAlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Pred 0\",\"Pred 1\"], yticklabels=[\"Actual 0\",\"Actual 1\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "sBMZ3jYdORR9",
        "outputId": "4e3df574-c8c5-4d4d-e1f4-ccfe3bc16929"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy :\", accuracy_score(y_true, y_pred))\n",
        "print(\"Precision:\", precision_score(y_true, y_pred))\n",
        "print(\"Recall   :\", recall_score(y_true, y_pred))\n",
        "print(\"F1-score :\", f1_score(y_true, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred))"
      ],
      "metadata": {
        "id": "ikdZD0cCOiDN",
        "outputId": "e4194b1e-e496-4012-a3c6-0ac770e8de0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 0.5\n",
            "Precision: 0.5\n",
            "Recall   : 0.2\n",
            "F1-score : 0.2857142857142857\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.80      0.62        10\n",
            "           1       0.50      0.20      0.29        10\n",
            "\n",
            "    accuracy                           0.50        20\n",
            "   macro avg       0.50      0.50      0.45        20\n",
            "weighted avg       0.50      0.50      0.45        20\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}